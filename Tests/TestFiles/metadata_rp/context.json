{"2410.18402v1.Low_Rank_Tensor_Learning_by_Generalized_Nonconvex_Regularization.pdf": "Low-Rank Tensor Learning by Generalized Nonconvex Regularization. Sijia Xia*, Michael K. Ng, and Xiongjun Zhang. October 25, 2024. Abstract. In this paper, we study the problem of low-rank tensor learning, where only a few of train-ing samples are observed and the underlying tensor has a low-rank structure. The existingmethods are based on the sum of nuclear norms of unfolding matrices of a tensor, whichmay be suboptimal. In order to explore the low-rankness of the underlying tensor effec-tively, we propose a nonconvex model based on transformed tensor nuclear norm for low-rank tensor learning. Specifically, a family of nonconvex functions are employed onto thesingular values of all frontal slices of a tensor in the transformed domain to characterizethe low-rankness of the underlying tensor. An error bound between the stationary point ofthe nonconvex model and the underlying tensor is established under restricted strong con-vexity on the loss function (such as least squares loss and logistic regression) and suitableregularity conditions on the nonconvex penalty function. By reformulating the nonconvexfunction into the difference of two convex functions, a proximal majorization-minimization(PMM) algorithm is designed to solve the resulting model. Then the global convergenceand convergence rate of PMM are established under very mild conditions. Numerical ex-periments are conducted on tensor completion and binary classification to demonstrate theeffectiveness of the proposed method over other state-of-the-art methods.. Key Words: Low-rank tensor learning, nonconvex regularization, transformed tensorSVD, proximal majorization-minimization, error bound. 2020 Mathematics Subject Classification: 15A69, 90C25. 1Introduction. Tensors, which are higher-order generalization of vectors and matrices, have attracted much attentionin the past decades and have a broad range of applications in various fields such as image processing[38], computer vision [48], machine learning [32, 65], and bioinformation [8]. These tensor dataare generally lied in low-dimensional subspaces in realistic scenes, which substantially have low-rank. *School of Mathematics and Statistics, Central China Normal University, Wuhan 430079, China (e-mail:si-jiax@mails.ccnu.edu.cn).Department of Mathematics, Hong Kong Baptist University, Kowloon Tong, Hong Kong (e-mail:michael-ng@hkbu.edu.hk). The research of this author was supported in part by the Hong Kong Research Grant Council GRF17300021, C7004-21GF and Joint NSFC-RGC N-HKU76921.School of Mathematics and Statistics, and Key Laboratory of Nonlinear Analysis & Applications (Ministry of Education),Central China Normal University, Wuhan 430079, China (e-mail: xjzhang@ccnu.edu.cn). The research of this author wassupported in part by the National Natural Science Foundation of China under Grant No. 12171189 and the FundamentalResearch Funds for the Central Universities under Grant No. CCNU24AI002.. 1. arXiv:2410.18402v1  [cs.LG]  24 Oct 2024. structure. In turn, the low-rank tensor data lead to efficient estimation and prediction. As a result, thelow-rank tensor based approaches are utilized to exploit the internal structure of a tensor efficiently. Inthis paper, we focus on the problem of low-rank tensor learning, where only a few of samples are givento learn the underlying tensor. Specifically, the general model of low-rank tensor learning is formulatedas follows:minXD fn,Y(X) +   rank(X),(1). where fn,Y() is the loss function related to the number of samples n and the observed tensor Y, D isa given constraint set,  > 0 is the regularization parameter, and rank(X) denotes the rank function ofthe underlying parameter tensor X  Rn1n2n3.Different tasks result in different loss functions in (1). Under the least squares loss, the model in(1) is suitable for low-rank tensor completion with noises. For example, Gandy et al. [19] proposed amodel composed of least squares loss and nuclear norms of unfolding matrices of a tensor for tensorcompletion, where the nuclear norms of unfolding matrices were utilized to approximate the Tuckerrank. Besides, Qiu et al. [53] and Zhang et al. [77] also employed the least squares loss with differ-ent low-rank approximations for noisy tensor completion, respectively. Under logistic regression loss,the model in (1) can be used for binary classification, where the low-rankness with respect to learningcoefficients represents that only a subspace of feature space is utilized for classification. For high-dimensional tensor regression to classification problems, Lian [37] proposed a support tensor machinemodel with hinge loss and low-rank regularization, and also established the convergence rate of the esti-mator. Tan et al. [61] proposed a logistic tensor regression model for classification of high-dimensionaldata with structural information, which employed the CANDECOMP/PARAFAC (CP) decomposition[12] to measure the low-rankness of a tensor. Wimalawarne et al. [69] proposed a regularization methodcombined the logistic regression loss function and tensor norms based on the unfolding matrices of atensor along each mode for binary classification. Furthermore, when the order of the tensor is second,low-rank tensor learning reduces to low-rank matrix learning, which has attracted much attention in thepast few decades. For instance, when the loss function is the least squares loss, it is low-rank matrixcompletion with noisy observations. There exist many work for matrix completion in the literature, see[9, 10, 54] and references therein. Besides, for logistic regression, Yin et al. [73] proposed a low-rankplus sparse method to detect the intra-sample outliers via training data.. 1.1Low-Rank Tensor Learning. For low-rank tensor learning, the key issue is the definition of the rank of a tensor, which is acquiredby tensor decomposition. Some widely used tensor decompositions include CP decomposition [12],Tucker decomposition [63], tensor singular value decomposition (SVD) [31], tensor train decompo-sition [47], tensor ring decomposition [80], and fully-connected tensor network decomposition [82].However, computing the CP rank of a tensor is NP-hard in general [26]. Although the Tucker rank of atensor can be computed via the SVD of the unfolding matrices easily, the Tucker rank minimization isalso NP-hard due to the difficulty of matrix rank minimization [9]. Some convex approximation meth-ods were proposed and studied via the sum of nuclear norms (SNN) of unfolding matrices of a tensorfor tensor completion [19, 38]. However, the SNN is not the convex envelope of the sum of entries ofTucker rank of a tensor [57].For tensor train rank minimization, which constitutes of ranks of matrices formed by a well-balanced matricization scheme, Bengua et al. [6] proposed two approaches for low-rank tensor comple-tion via tensor train nuclear norm and its parallel matrix factorization, which were capable of capturingthe global correlation of the tensor entries. However, the above work may destroy the intrinsic structureof a tensor since a low-order tensor should be represented into a higher-order tensor by the ket aug-. 2. mentation technique. Based on tensor ring rank minimization, Yuan et al. [74] proposed a tensor ringnuclear norm method for noisy tensor completion to reduce the computational complexity by using thelow-rank assumption on tensor factors instead of on the original tensor, where the tensor ring nuclearnorm was defined as the nuclear norms of the unfolding matrices of all factor tensors in tensor ringdecomposition. While the tensor train and tensor ring methods only established a connection betweenadjacent two factors, rather than any two factors. In order to overcome the limitation of tensor trainand tensor ring decomposition, Zheng et al. [82] proposed a fully-connected tensor network decom-position and presented a novel method for tensor completion via this kind of decomposition, whichestablished an operation between any two factor tensors. However, the fully-connected tensor networkdecomposition may be resulted in large computational burden due to the multiple connections of factortensors.. 1.2Tensor SVD. The tensor SVD was proposed and studied based on tensor-tensor product for third-order tensors in[31], which was further extended to higher-order tensors [42, 49]. Moreover, the optimal represen-tation and compression about tensor SVD were studied in [30]. For the problem of low-rank tensorminimization, the tensor nuclear norm was proposed to approximate the tensor tubal rank in [58]. Andthe tensor nuclear norm based methods were further presented to low-rank tensor learning problems,see [41, 66] and references therein. However, the tensor nuclear norm is based on Fourier transform,which may be challenged since the periodicity is assumed. Recently, Song et al. [59] proposed atransformed tensor SVD via any unitary transform instead of Fourier transform. Then the transformedtensor nuclear norm (TTNN) was proposed to approximate the transformed multi-rank of a tensor forrobust tensor completion, which was capable of acquiring a lower rank tensor under suitable unitarytransformations. Moreover, the TTNN was applied to other low-rank tensor optimization problems suc-cessfully, see [50, 51, 60, 78] and references therein. However, the TTNN is just the convex envelopeof sum of each entry of transformed multi-rank of a tensor under the unit ball of tensor spectral norm,which may be suboptimal. Furthermore, the tensor nuclear norms based on non-invertible transforma-tions [28] and nonlinear transformations [35] were proposed and studied for tensor completion.On the other hand, some nonconvex approximations were also proposed and studied for low-ranktensor learning, which can generally approximate the tensor rank better than the convex relaxationmethods. For example, based on SNN for tensor completion, Zhang proposed a nonconvex methodby folding a tenor into a square matrix to approximate the Tucker rank minimization [76]. Xu et al.[71] proposed a low-rank factorization model for tensor completion, where the unfolding matrices ofa tensor along each mode were factorized into the product of two smaller matrices. Besides, Yao etal. [72] proposed a nonconvex model based on SNN for low-rank tensor learning and established thestatistical performance on the tensor completion problem, where the nonconvex regularization was em-ployed onto the singular values of unfolding matrices of a tensor along each mode. However, the errorbound between the stationary point of the nonconvex model and the underlying tensor was only estab-lished for low-rank tensor learning with least squares loss. Moreover, the nonconvex approximationbased on SNN may be suboptimal since the SNN is not the tightest convex relaxation of Tucker rankminimization [44].Based on tensor SVD in low-rank tensor learning, Wang et al. [66] proposed a nonconvex modelvia using the nonconvex function onto the singular values of all frontal slices in the Fourier domain fortensor completion without noise. Furthermore, Qiu et al. [50, 52] proposed a nonconvex approach viausing nonconvex regularization for robust tensor completion, where a family of nonconvex functionswere employed onto the low-rank and sparse components, respectively. Moreover, Zhao et al. [81]proposed a nonconvex tensor surrogate function for robust tensor completion via equivalent nonconvex. 3. surrogates with difference of convex functions structures. While they only established the recoveryerror bound between the estimator of an approximate convex model and the underlying tensor, and didnot analyzed the error bound of the nonconvex model. In addition, Gao et al. [20] proposed an p (0 <p < 1) method by employing the p function to each singular value of the low-rank tensor in the Fourierdomain and each entry of the sparse tensor for tensor robust principle component analysis, where thep norm was used to approximate tensor fibered rank and measure sparsity, respectively. Recently,Zhang et al. [79] proposed a sparse tensor factorization method based on tensor-tensor product undergeneral observations. In the previous two work, the error bounds between the global minimizer of thenonconvex model and the underlying tensor were established under some conditions. However, there isa significant gap between the theory and practice computation since it is difficult to achieve the globalminimizers of the these nonconvex models in numerical algorithms.. 1.3The Contribution. In this paper, we propose a nonconvex approach for low-rank tensor learning. Specifically, a gen-eral loss function is utilized between given samples and the underlying tensor to fit the observed data.Moreover, in order to explore the global low-rankness of the underlying tensor, a family of noncon-vex functions are employed onto the singular values of all frontal slices of a tensor in the transformeddomain. Compared with TTNN, the nonconvex method is capable of acquiring a lower rank tensor,which is preferred for low-rank tensor learning. Besides, an error bound between any stationary pointof the proposed nonconvex model and the underlying tensor is established under the restricted strongconvexity (RSC) condition on the loss function and suitable regularity conditions on the nonconvexpenalty, which is smaller than that of the Tucker based method in [72]. In particular, our model coversthe least squares loss for tensor completion and logistic regression for binary classification. Moreover,by reformulating the nonconvex regularization into a difference of convex (DC) functions, a proxi-mal majorization-minimization (PMM) algorithm is designed to solve the proposed nonconvex model,where the loss function and one convex function in the DC structure are linearized at the current pointof iterations. Moreover, we show that the PMM algorithm globally converges to a stationary pointof the proposed nonconvex model under the Kurdyka-ojasiewicz (KL) assumption on the nonconvexpenalty, where the convergence rate of PMM is also established. And an alternating direction methodof multipliers (ADMM) is presented to solve the resulting subproblem in PMM. Numerical exampleson tensor completion and binary classification are conducted to demonstrate the superiority of the pro-posed method compared with several state-of-the-art methods.The remaining parts of this paper are organized as follows. Next, we give some notations andnotions about tensors and transformed tensor SVD. In Section 2, we propose a nonconvex model forlow-rank tensor learning based on TTNN. The error bound of any stationary point of the proposedmodel is established under some conditions. A PMM algorithm is designed to solve the proposednonconvex model and the ADMM is applied to solve the resulting subproblem in Section 3, where theglobal convergence and convergence rate of PMM are also established. In Section 4, some numericalexperiments are conducted on tensor completion and logistical regression to illustrate the advantageof our method over other existing approaches. We conclude this paper in Section 5. Finally, all thetechnical proofs are deferred to the Appendix.. 1.4Preliminaries. Some notations used throughout this paper are summarized in Table 1, where the size of a tensor isn1  n2  n3.Now we give the definition of subdifferential of a function.. 4. Table 1: Notations. NotationsDescription. a/a/A/AScalars/Vectors/Matrices/TensorsAijkThe (i, j, k)-th element of ATThe conjugate transpose operatorTr()The trace of a matrixAiThe i-th frontal slice of AA(i)The mode-i unfolding of AFoldi()The inverse operator of mode-i unfolding, i.e., Foldi(A(i)) = Aa2The 2 norm of aDiag(a)A diagonal matrix with the i-th diagonal element being the i-th component of aj(A)The j-th largest singular value of AAThe nuclear norm of A defined as A := min{n1,n2}j=1j(A)AThe spectral norm of A defined as A := 1(A)AFFrobenius norm of A defined as AF :=. Tr(AT A)UA unitary matrix satisfying UUT = UT U = In3, where In3 is the n3  n3 identity matrixXU or U[X]XU = U[X] := Fold3(UX(3))A, BThe inner product of two tensors defined as A, B := n3i=1 Tr((Ai)T Bi)vec(A)Vectorizing a tensor A into a vectorATensor  norm of A defined as A := max |Aijk|AFTensor Frobenius norm of A defined as AF :=. A, AD()The indicator function of a set D with D(a) = 0 if a  D, otherwise +dist(a, D)The distance from a to D and is defined as dist(a, D) := inf{y  a2, y  D}. Definition 1 [56, Definition 8.3] Consider a function f : Rn  (, +] and a point x  Rn withthe finite f(x). The regular subdifferential of f at x is defined by. f(x) :=y  Rn : lim infzx,z=xf(z)  f(x)  y, z  x. z  x2 0.. The (limiting) subdifferential of the function f at x is defined by. f(x) :=y  Rn : xkf x, yk  y with yk  f(xk) for each k,. where xkf x means xk  x with f(xk)  f(x).. The KL function plays a vital role for the convergence analysis in our algorithm, and we list thedefinition of KL function in the following definition.. Definition 2 [7, Definition 3] Let f : Rn  R  {+} be a proper lower semicontinuous function.We say that f has the Kurdyka-ojasiewicz (KL) property at point x  dom(f), if there exist aneighborhood U of x,   (0, +] and a continuous concave function  : [0, )  R+ such that:(i) (0) = 0,(ii)  is C1 on (0, ),(iii) for all s  (0, ), (s) > 0,(iv) for all x in U  [f(x) < f < f(x) + ], the KL inequality holds:. (f(x)  f(x)) dist(0, f(x))  1.(2). If f satisfy the KL property at each point of dom(f) := {v  Rn : f(v) = }, then f is called a KLfunction.. 5. A function is said to have the KL property at x with an exponent  if the function  in Definition. 2 takes the form of (s) = 1s1 with 1 > 0 and   [0, 1). The proper closed semi-algebraicfunctions are KL functions with exponent   [0, 1) [36].Next, we review the definition of transformed tensor SVD for third-order tensors, see [59] for moredetails. We denote X (also denoted by bdiag( XU)) as a block diagonal matrix, where the i-th block isthe matrix X iU , i = 1, . . . , n3, i.e.,. X = bdiag( XU) :=. . . X 1U...X n3U. . .. The corresponding inverse operator, denoted by fold3(), is defined as. fold3(bdiag( XU)) = XU.(3). Definition 3 [59, Definition 1] The U-product of arbitrary two tensors A  Cn1n2n3 and B Cn2ln3 is defined as A U B = UT [fold3(bdiag( AU)  bdiag( BU))]  Cn1ln3.. Definition 4 [59, Definition 7] The transformed tensor nuclear norm of X  Cn1n2n3 is defined asXTTNN = n3i=1  X iU .. It can be easily verified that XTTNN = X.. Definition 5 [59] The tensor spectral norm with respect to U, denoted by XU, is defined as XU =X.. Now we recall the definitions of the diagonal tensor, conjugate transpose, and unitary tensor [31,. 59]. A third-order tensor is called to be diagonal if each frontal slice is a diagonal matrix. Theconjugate transpose of X  Cn1n2n3 with respect to U, denoted by X T , is defined as X T =UT [fold3((bdiag( XU))T )]  Cn2n1n3. A tensor U is called to be unitary if U U UT = UT U U =IU, where IU denotes the identity tensor and is defined as each frontal slice of U[IU] being the identitymatrix. Next we give the definition of transformed tensor SVD of a third-order tensor.. Theorem 1 [29, Theorem 5.1] The transformed tensor singular value decomposition of X  Cn1n2n3. is given by X = U U  U VT , where   Cn1n2n3 is a diagonal tensor, U  Cn1n1n3 andV  Cn2n2n3 are unitary tensors with respect to U-product.. Definition 6 [59, Definition 6] The transformed multi-rank of X  Cn1n2n3 is a vector r =(r1, r2, ..., rn3) with ri = rank( X iU ), i = 1, 2, . . . , n3, where rank( X (i)U ) denotes the rank of the matrixX iU .. 2Nonconvex Model for Low-Rank Tensor Learning. In this section, we present the framework of our approach for the problem of low-rank tensor learning.Given n samples, we consider a general loss function fn,Y(X), which is used to fit the parameter tensorX  Rn1n2n3 and the observed data Y. Suppose that the parameter tensor X is low-rank, which we. 6. aim to estimate. Now a nonconvex model based on TTNN with a general loss function is presented forlow-rank tensor learning:minXfn,Y(X) + G(X). s.t. X  c,(4). where fn,Y(X) represents a differentiable loss function with n samples, G(X) represents the noncon-vex regularization defined as. G(X) :=. n3. i=1. min{n1,n2}. j=1g(j( X iU )),(5). > 0 is a penalty parameter, and c > 0 is a given constant. Here g() is a nonconvex function withrespect to the parameter . In addition, the tensor  norm constraint is effective to exclude over-spikytensors.Throughout this paper, the nonconvex function g() should satisfy the following assumptions.. Assumption 1 The function g (x) : R  R+ is symmetric and has the following properties:(i) g(x) is concave and non-decreasing for x  0 with g (0) = 0.(ii) The function g(x). xis non-increasing for x > 0.(iii) g(x) is differentiable for any x = 0 and limx0+ g(x) = k0, where k0 > 0 is a constant and. k0 is an upper bound of g() on (0, +).(iv) There exists a parameter  > 0 such that g(x) +. 2x2 is convex on (0, +).. A lot of nonconvex functions satisfy Assumption 1 in the literature, such as smoothly clippedabsolute deviation (SCAD) [16], minimax concave penalty (MCP) [75], logarithmic function [11],which will be given in Table 2. These nonconvex function can approximate the 0 norm better thanthe 1 norm, which can yield a sparser solution in statistical learning [22]. In model (4), we proposeto employ a family of nonconvex functions based on TTNN to explore the global low-rankness ofthe underlying tensor in low-rank tensor learning. Although the TTNN can get a lower rank tensorcompared with tensor nuclear norm under suitable unitary transformations [59], the TTNN is just the1 norm of all singular values vectors of the frontal slices of a tensor in the transformed domain, whichalso suffers from the drawback of 1 norm. Recently, some nonconvex surrogate functions have beendemonstrated more efficiently than the convex relaxation in various fields such as statistical learning[16, 75], compressed sensing [11], and tensor completion [50, 76]. Moreover, the nonconvex relaxationbased on TTNN can help to achieve a lower rank tensor than TTNN for robust tensor completion [50].In low-rank tensor learning, we employ a family of nonconvex functions onto each singular value ofthe frontal slices of the underlying tensor in the transformed domain. Compared with the TTNN, themain advantage of the proposed method is that the nonconvex functions can approximate the sum ofentries of the transformed multi-rank of a tensor better.For the general loss function fn,Y in model (4), we just need it to be differentiable. In various real-world applications, we can specify the loss function fn,Y. In particular, when fn,Y is the least squaresloss function, model (4) can be used for noisy tensor completion. When fn,Y is the logistic regressionloss function, model (4) can be utilized for binary classification in machine learning.. Remark 1 When fn,Y(X) = 1. 2P(Y)  P(X)2F , model (4) can be used for tensor completionwith noisy observations. Recently, some nonconvex surrogates were proposed and studied for tensorcompletion [50, 66, 76], where a family of nonconvex functions were employed onto the low-rank com-ponent of a tensor. For example, Wang et al. [66] proposed to utilize a general nonconvex surrogate of. 7. the tensor tubal rank for tensor completion and a least squares loss function for the data-fitting term,where the tensor nuclear norm was used in the Fourier domain. And Zhang [76] used a family of non-convex function onto the singular values of the square matrice via matricizing a tensor to approximatethe Tucker rank of a tensor. However, they do not analyze the statistical performance of their proposedmodels.. Remark 2 Based on the overlapped nuclear norm of a tensor, Yao et al. [72] proposed a nonconvexapproach for low-rank tensor learning, where the nonconvex functions were employed onto each sin-gular value of unfolding matrices of a tensor. However, the unfolding based methods are challengeddue to its suboptimality, where the overlapped nuclear norm was not the convex envelope of the sum ofTucker rank of a tensor [57]. Moreover, they only analyzed the statistical performance of their modelfor the least squares loss function. We will establish the error bound between any stationary point ofmodel (4) and the underlying tenor for a general loss function in the next subsection.. 2.1Statistical Guarantee. In this subsection, we first introduce the definition of the RSC condition for a general loss function andthen establish the error bound between any stationary point of the proposed model and the underlyingtensor.The RSC condition of a differentiable function in the tensor case plays a vital role in establishing theerror bound of the proposed model. The foundational works on the RSC condition in the vector case aredue to [40, 46]. For any tensor V  Rn1n2n3, we say that the function fn,Y(X) : Rn1n2n3  Rsatisfies the RSC condition if. fn,Y(X  + V)  fn,Y(X ), V. 1V2F  1log d. n V2TTNN,if VF  1,. 2VF  2. log d. n VTTNN,otherwise,(6). where X  denotes the ground-truth tensor, d := n1n2n3, n is the number of samples, and 1, 2 > 0,1, 2  0 are given constants.The RSC condition involves a lower bound on the remainder in the first-order Taylor expansionof fn,Y, where we only require fn,Y to be differentiable. If fn,Y is convex, the left hand in (6) is. always nonnegative, and then (6) holds trivially for VTTNN. VF. 1n. 1 log d and VTTNN. VF22. n. log d.. As a result, the inequality in (6) only enforces a type of strong convexity condition over the coneVTTNN. VF c. n. log d, where c > 0 is a constant. In particular, we will show the least squares loss and. logistic regression loss satisfy the RSC condition in Appendix A and B. More discussions about theRSC condition in the vector case can be referred to [40].. Remark 3 The RSC condition has been widely studied for statistical learning in the literature. Forexample, the least squares loss for linear regression in the matrix case satisfies the RSC condition [23].Moreover, the loss functions of the generalized linear model and corrected linear model satisfy theRSC condition by selecting appropriate parameters [39, 40, 46], where the RSC condition of these lossfunctions is employed in the vector case.. Let X be a stationary point of problem (4). In the following analysis of the statistical performanceguarantee of the proposed model, we enforce the additional constraint XTTNN  t for any X Rn1n2n3 in model (4), where t > 0 is a given constant. The main result is described in the followingtheorem.. 8. Theorem 2 Suppose that the regularizer g() satisfies Assumption 1 and the loss function fn,Y sat-isfies the RSC condition (6) with 3. 4 < 1, where  is defined in Assumption 1(iv). Consider theparameter  with4. k0max. . fn,Y(X )U, 2. . log d. n. . 2. 6tk0,(7). and. n  16t2 max{ 21 ,  22 }. 22log d,(8). where d := n1n2n3. Then any stationary point X of model (4) satisfies. X  X F  6k0n3i=1 ri. 41  3,. where ri denotes the rank of matrix (X )iU , i = 1, . . . , n3.. The proof of Theorem 2 is left to Appendix E. From Theorem 2, we know that the upper boundis related to the sum of each entry of the transformed multi-rank of the underlying tensor, which willbe small under suitable unitary transformations [59]. More details about the choice of unitary trans-formations in TTNN can be referred to [59, 60]. Note that the parameters k0, ,  are related to thenonconvex function g, which are given in Assumption 1. Moreover, we need the loss function fn,Y tosatisfy the RSC condition (6) in Theorem 2. We will show the least squares loss for tensor completionand logistic regression for binary classification in low-rank tensor learning satisfy the RSC conditionin Appendix A and B, where the two loss functions are widely used in practice.In particular, the error bound in Theorem 2 can reduce to that of the TTNN model, where thenonconvex regularization term of model (4) is replaced by TTNN. Notice that the error bound in The-orem 2 is just the tensor Frobenius norm of the difference between any stationary point of model(4) and the underlying tensor, which is the worst case for the error bound of the nonconvex modelin (4). In numerical computation, we can design an efficient algorithm to obtain a stationary pointof the nonconvex model, which will be shown in the next section. Now we compare with the errorbound of the model in [72], which utilized the least squares loss and the nonconvex regularizationbased on the nuclear norms of all unfolding matrices of a tensor for low-rank tensor learning. Notethat r1++rn3. n3 max{r1, . . . , rn3}  rank(X (1)). This demonstrates that a tensor with low Tuckerrank has low average transformed multi-rank. Compared with the error bound in [72], which used theTucker rank in the model and was on the order of O(di=1. rank(X (i))), the error bound in Theorem. 2 is smaller when n3 is not too large or the rank of unfolding matrices of the underlying tensor is large.. 3Optimization Algorithm. In this section, we first design a proximal majorization-minimization (PMM) algorithm [62, 81] tosolve problem (4), and then establish the global convergence and convergence rate of PMM under verymild conditions. Finally, an ADMM based algorithm is utilized to solve the resulting subproblem inPMM.. 3.1PMM Algorithm. Note that problem (4) is nonconvex and nonsmooth since G(X) is nonconvex and nonsmooth. Thenonconvexity of the objective function results in great challenges to numerical computation and the-oretical analysis. By the special structure of some nonconvex functions, they can be written as the. 9. difference of two convex functions, which has a variety of applications in statistical and machine learn-ing [1, 22, 24, 33]. In particular, we assume that the nonconvex function g(x) in our model can bewritten asg(x) = s1(x)  s2(x),(9). where s1 is convex and s2 satisfies Assumption 2.. Assumption 2 The function s2 : R  R satisfies the following assumptions.(a) s2 is convex and symmetric, i.e., s2(x) = s2(x).(b) s2 is differentiable and its derivative s2 is locally Lipschitz continuous.. A lot of nonconvex functions satisfying Assumption 1 can be expressed in the decomposition for-mulation (9), which are summarized in Table 2. Besides, s2 in Table 2 also satisfies Assumption 2.. Table 2: Examples of the nonconvex function g(x) and its corresponding difference of two convexfunctions (i.e., g(x) = s1(x)  s2(x)), where  > 0 for Logarithm and MCP, and  > 1 for SCAD.. Nonconvex functiong(x)(x  0,  > 0)s1(x)s2(x). Logarithm [11] log( x. + 1)xx   log( x. + 1), x > 0. MCP [75]. x  x2. 2 ,x  ,. 122,x > .. x. x22 ,x  ,. x  2. 2 ,x > .. SCAD [16]. . . x,x < .x2+2x2. 2(1),  x < ,. 2(+1). 2,x  .. x. . . 0,x < ,. x22x+2. 2(1),  x < ,. x  (+1)2. 2,x  .. Based on (9), we can rewritten G(X) in (4) as follows:. G(X) = S1(X)  S2(X),(10). where S1(X) = n3i=1min{n1,n2}j=1s1(j( X iU )) and. S2(X) =. n3. i=1. min{n1,n2}. j=1s2(j( X iU )).(11). Consequently, problem (4) can be reformulated equivalently as follows:. minX fn,Y(X) + S1(X)  S2(X) + D(X),(12). where D(X) is the indicator function of the set D with D := {X : X  c}.We adopt the PMM algorithm to solve problem (12), whose main idea is to linearize the concavefunction S2(X) and the smooth loss function fn,Y(X) of the objective function in (12) at the currentiteration point X t. Specifically, given X t  Rn1n2n3, we consider to solve the following problem:. minXfn,Y(X t) + fn,Y(X t), X  X t +. 2X  X t2F + S1(X)  S2(X t). S2(X t), X  X t + D(X),(13). 10. where  > 0 is a given constant. Notice that problem (13) is equivalent to. minX S1(X) + fn,Y(X t)  S2(X t), X  X t +. 2X  X t2F + D(X).(14). Denote. H(X) := fn,Y(X) + G(X) + D(X) = fn,Y(X) + S1(X)  S2(X) + D(X),(15). and. Q(X, X t) := fn,Y(X t) + fn,Y(X t), X  X t +. 2X  X t2F + S1(X)  S2(X t). S2(X t), X  X t + D(X).(16). It can be easily verified that Q(X t, X t) = H(X t).However, it is difficult to compute the exact solution of problem (14) in practice. We considerto solve an inexact solution at each iteration of PMM. In particular, we propose an inexact versionof PMM algorithm for solving problem (14). The error criteria of each iteration should satisfy thefollowing condition: Find Wt+1  Rn1n2n3 such that. Wt+1  Q(X t+1, X t) and Wt+1F  X t+1  X tF ,(17). where   (0, 1. 2) is a constant.Now, we state the PMM in Algorithm 1.. Algorithm 1 A PMM Algorithm for Solving Problem (12). 1: Initialization: Given parameter , , ,  > 0 and W0. For t = 0, 1, 2, . . .. 2: repeatFind Wt+1 such that Wt+1  Q(X t+1, X t) and Wt+1F  X t+1  X tF .. 3: until A stopping condition is satisfied.. Remark 4 In Algorithm 1, we propose an inexact PMM algorithm for solving problem (12), where thecondition in (17) should be satisfied. In particular, when X t+1 is the optimal solution of (14), we justchoose Wt+1 to be a zero tensor.. 3.2Convergence Analysis. In this subsection, the global convergence and convergence rate of Algorithm 1 are established. First,the convergent result of Algorithm 1 is presented in the following theorem.. Theorem 3 Let {X t} be the sequence generated by Algorithm 1. Suppose that Assumption 2 hold,where the locally Lipschitz constant of s2 is set as L0. Assume that fn,Y is Lipschitz continuous withLipschitz constant L, and fn,Y, g(x) are KL functions. Then for any  >L. 12 with   (0, 1. 2), thesequence {X t} converges to a stationary point X of (4) as t goes to infinity.. The proof of Theorem 3 is left to Appendix G. The assumptions in Theorem 3 are very mild. Thenonconvex functions in Table 2 are KL functions [50, 68]. Moreover, when fn,Y is the logistic lossfunction (e.g., see (27)) or the least squares loss function (e.g., see (25)), it is a KL function [7, 68].Besides, when fn,Y is the least squares loss, the Lipschitz constant of fn,Y is 1. And when fn,Yis the logistic loss function in (27), it follows from Lemma 4 that the Lipschitz constant of fn,Y is14nni=1 Zi2F .. 11. Remark 5 The assumption about the locally Lipschitz continuity of s2 is very mild. In fact, the deriva-tives of these functions in Table 2 are Lipschitz continuous. In particular, it is evident from [1, 68] thatfor Logarithm function, the Lipschitz constant of s2 is. 2 . For SCAD and MCP, the Lipschitz constantsof s2 are1. 1 and 1. , respectively.. Theorem 3 shows that the sequence {X t} generated by Algorithm 1 converges to X globally as ttends to infinity, i.e. limt X t = X. Now we also give the convergence rate of Algorithm 1, whichis stated in the following theorem.. Theorem 4 Let {X t} be the sequence generated by Algorithm 1. Suppose that the assumptions inTheorem 3 hold and H(X) defined in (15) satisfies the KL property at X with an exponent   [0, 1).Then, we have the following results:(i) If  = 0, then the sequenceX tconverges in a finite number of steps.(ii) If 0 <  12, then the sequenceX tconverges R-linearly, i.e., there exist w > 0 and  [0, 1) such that X t  XF  wt.(iii) If 1. 2 <  < 1, then the sequenceX tconverges R-sublinearly, i.e., there exists w > 0 such. that X t  XF  wt 1. 21 .. The proof of Theorem 4 is left to Appendix H. In particular, if we know the KL exponent of H(X),the detailed convergence rate of PMM can be determined.. 3.3ADMM for Solving the Subproblem. Now we consider to solve the subproblem (14) by applying ADMM [17, 21]. Let X = M, thenproblem (14) is equivalent to. minX,M S1(M) + fn,Y(X t)  S2(X t), X  X t +. 2X  X t2F + D(X). s.t. X = M.(18). The augmented Lagrangian function associated with (18) is given by. L(X, M, Z) = S1(M) + fn,Y(X t)  S2(X t), X  X t +. 2X  X t2F + D(X). + Z, X  M +. 2X  M2F ,. where  > 0 is the penalty parameter and Z  Rn1n2n3 is the Lagrangian multiplier. Then theiteration of ADMM is given as follows:. Mk+1 = arg minML(X k, M, Zk),(19). X k+1 = arg minXL(X, Mk+1, Zk),(20). Zk+1 = Zk + (X k+1  Mk+1),(21). where   (0, 1+. 5. 2) is the step size.Now we consider to solve the subproblems in (19) and (20) in detail. Note that problem (19) canbe equivalently written as. Mk+1 = arg minMS1(M) +. 2. M X k + 1. Zk. 2. F. = Prox. S1. X k + 1. Zk.(22). 12. Problem (20) can be rewritten as. X k+1 = arg minXD(X) + fn,Y(X t)  S2(X t), X  X t + Zk, X  Mk+1. +. 2X  Mk+12F +. 2X  X t2F. = arg minXD(X) +  +. 2. X 1. + Hk+1. 2. F,. where Hk+1 = X t  fn,Y(X t) + S2(X t) + Mk+1  Zk. A simple computation leads to. X k+1 = PD. 1. + Hk+1,(23). where PD() is the projection operator onto the set D given by PD(Y) = max{min{Yijk, c}, c}.Then the ADMM for solving problem (18) is stated in Algorithm 2.. Algorithm 2 An ADMM Algorithm for Solving Problem (18). 1: Initialization: Given initial value X 0, M0, Z0, X t, parameters  > 0,   (0, 1+. 5. 2).. 2: repeat. 3:Step 1. Compute Mk+1 by (22).. 4:Step 2. Compute X k+1 by (23).. 5:Step 3. Update Zk+1 by (21).. 6: until A stopping condition is satisfied.. Remark 6 In Algorithm 2, we need to compute the proximal mapping of S1. In the experiments, thefunction s1(x) is taken as s1(x) = x and then S1(X) = XTTNN. As a result, problem (22) can beequivalently written as. Mk+1 = Prox. TTNN. X k + 1. Zk.. It follows from [59, Theorem 3] that. Mk+1 = U U  U VT ,. where X k + 1. Zk = U U  U VT ,  = UT [] and  = max{U. , 0}.. Remark 7 For (23), we need to compute S2(X t) in order to get Hk+1. Note that s2 is differentiable.By utilizing the differentiable case in Lemma 12, we obtain that. S2(X t) = Ut U Dt U (Vt)T ,. where X t = Ut U t U (Vt)T and (Dt)iU = Diag(s2((tiU )11), . . . , s2((tiU )mm)), i = 1, . . . , n3.. Here (tiU )jj represents the (j, j)-th entry of a matrix and m = min{n1, n2}.. Note that problem (18) is convex, and Algorithm 2 is just the classical two-block ADMM, whoseconvergence has been established in [17, 21]. For brevity, we omit the details of convergence of Algo-rithm 2 here.. 13. Now we give the computational cost of ADMM for solving the subproblem (14) based on thenonconvex functions in Table 2 and the least squares loss or logistic regression, which is the mainiteration of PMM. Note that S1(X) = XTTNN in Table 2. In this case, the computational costof Mk+1 is O(n(1)n2(2)n3 + n1n2n23) [59, Section 4.1], where n(1) = max{n1, n2} and n(2) =min{n1, n2}. The main cost of X k+1 is to compute S2(X t), which is O(n(1)n2(2)n3 + n1n2n23)and only computes one time in ADMM. Therefore, the computational cost of ADMM in each iterationis O(n(1)n2(2)n3 + n1n2n23). Furthermore, the computational complexity of PMM in each iteration isO((n(1)n2(2)n3 + n1n2n23)km), where km represents the number of iterations of ADMM.. 4Numerical Experiments. In this section, some experiments are conducted to demonstrate the effectiveness of the proposedmethod. Our model combines the loss function and nonconvex regularization (called LFNR for short),where the least squares loss and logistic regression loss are used for the loss function in model (4).In this case, the corresponding problems are tensor completion and binary classification, respectively.For the nonconvex function g() in model (4), we use the MCP in all experiments for simplicity. Weremark that the performance of other nonconvex functions is similar to that of MCP. All experimentsare conducted in MATLAB R2020b with an Intel Core i7-10750H 2.6GHz and 16GB RAM.For the unitary matrix in TTNN, the choice is given as follows: First, an initial estimator X1 isobtained by discrete cosine transform in (12). Afterwards, we unfold X1 into a matrix (X1)(3) alongthe third-dimension and take the SVD of (X1)(3) as (X1)(3) = UVT . Then UT is the desirableunitary matrix in TTNN. More details about the choice of unitary transform can be referred to [59].. 4.1Stopping Criterion. Algorithm 1 will be terminated if X t+1X tF. X tF 5  104 or the number of iterations reaches 100.The Karush-Kuhn-Tucker (KKT) condition of (18) is given as follows:. Z  (S1(M)), M = X, (X t  X)  fn,Y(X t) + S2(X t)  Z  D(X).. The relative KKT residual is employed to evaluate the accuracy of Algorithm 2:. res := max{e, d, p},(24). where. e :=M  XF. 1 + MF + XF, d := M  ProxS1(M + Z)F. 1 + MF + ZF,. p :=X  Prox1D()(X t  1(fn,Y(X t)  S2(X t) + Z))F. 1 + 1ZF + X tF + 1fn,Y(X t)F + 1S2(X t)F.. Then Algorithm 2 will be terminated if res  3  103 or the number of iterations exceeds 100.. 4.2Tensor Completion. In this subsection, we present numerical experiments for tensor completion, where the least squares lossis utilized for fn,Y. In this case, model (4) reduces to the least squares loss function with nonconvexregularization given by. minX12pP(X  Y)2F + G(X). s.t. X  c,(25). 14. Table 3: PSNR and SSIM values of different methods for the Balloons dataset with different  andSRs.. SRSNNTMacNORTTTNNLFNR. PSNR. 0.005. 0.0524.5035.2332.7933.9135.650.1031.1337.7437.2238.7540.010.1534.4139.0539.4441.7642.800.2036.7640.2040.3843.7244.570.2538.5741.2541.5945.4646.070.3039.8241.8842.4846.4347.21. 0.01. 0.0523.9633.4133.6033.2435.150.1030.5035.9637.2237.5638.780.1533.3837.6538.7939.2441.080.2035.3639.1739.2641.1042.470.2536.6940.2639.9642.3543.510.3037.7340.8340.9643.3244.34. SSIM. 0.005. 0.050.83420.90530.89890.90570.92810.100.92110.94470.94170.96150.96930.150.94930.96040.96550.97680.98190.200.96400.96920.97150.98380.98680.250.97200.97470.97760.98750.98990.300.97630.97770.98100.98980.9913. 0.01. 0.050.82180.83850.90150.88540.90580.100.90290.90640.94170.94210.95410.150.93000.93840.95770.95930.96610.200.94170.95450.96120.96400.97560.250.94690.96210.96150.97520.97900.300.94930.96670.97210.97870.9828. where Y  Rn1n2n3 is the observed tensor only known its entries in ,  is the index set, p :=n. n1n2n3 denotes the probability of each element to be observed, and P is the projection operator onto such that the entry maintaining the same for the index in  and zero outside . By choosing theregularization term G(X) as TTNN, model (25) is convex and called TTNN for short. We also com-pare with the following three methods: sum of nuclear norms of unfolding matrices of a tensor (SNN)[19], parallel matrix factorization for tensor completion (TMac)1 [71], nonconvex regularized tensoralgorithm (NORT)2 [72].The observed tensor is constructed as follows: For an n1  n2  n3 tensor X, we first add thezero mean Gaussian noise with standard deviation , which is denoted by Y. Then the index set is uniformly generated at random and we get P(Y), where the sample ratio (SR) is defined asSR :=||. n1n2n3 . Here || denotes the cardinality of .The peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) index [67] are adopted to. 1https://xu-yangyang.github.io/TMac/2https://github.com/quanmingyao/FasTer. 15. Table 4: PSNR and SSIM values of different methods for the Lemons dataset with different  and SRs.. SRSNNTMacNORTTTNNLFNR. PSNR. 0.005. 0.0530.1737.7636.3837.8139.980.1035.6040.4639.5342.2444.250.1538.3442.2442.6844.6846.690.2040.1943.5644.2446.3747.900.2541.6744.6044.9247.6849.150.3042.8745.4645.6748.7650.14. 0.01. 0.0530.0436.6237.0737.3338.990.1034.5439.9340.5741.2542.640.1536.8141.4441.6943.2744.440.2038.3442.5842.2144.5345.610.2539.2943.0842.8744.7046.400.3040.0243.4543.6644.9047.00. SSIM. 0.005. 0.050.87750.91940.91370.92650.95540.100.94100.95410.95290.96970.98040.150.96080.97030.97420.98190.98770.200.96990.97830.98060.98690.99030.250.97560.98200.98300.98980.99250.300.97870.98450.98520.99160.9937. 0.01. 0.050.87600.90040.92100.92360.93830.100.92310.95010.95910.96000.96740.150.93900.96260.96570.97260.97660.200.94640.96910.96870.97730.98100.250.94900.97170.97100.97570.98450.300.94980.97350.97380.98000.9867. 051015202530Index of band. 30. 32. 34. 36. 38. 40. PSNR. SNNTMacNORT. TTNNLFNR. (a) Balloons. 051015202530Index of band. 17. 21. 25. 29. 33. 37. 41. PSNR. SNNTMacNORT. TTNNLFNR. (b) Lemons. Figure 1: PSNR values versus index of band of different methods for the Balloons and Lemons datasets.(a) Balloons dataset, where SR = 0.5 and  = 0.05. (b) Lemons dataset, where SR = 0.4 and  = 0.1.. 16. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (a) Original. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (b) Observation. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (c) SNN: 30.48. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (d) TMac: 34.24. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (e) NORT: 34.51. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (f) TTNN: 35.57. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (g) LFNR: 37.24. Figure 2: The recovered images (with PSNR values) and zoomed regions of different methods for the30th band of the Balloons dataset, where SR = 0.4 and  = 0.05.. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (a) Original. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (b) Observation. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (c) SNN: 29.86. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (d) TMac: 36.36. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (e) NORT: 36.29. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (f) TTNN: 37.58. <image: DeviceRGB, width: 1000, height: 1001, bpc: 8>. (g) LFNR: 38.61. Figure 3: The recovered images (with PSNR values) and zoomed regions of different methods for the30th band of the Lemons dataset, where SR = 0.3 and  = 0.05.. measure the recovery performance for real-world data. Specifically, the PSNR is defined as. PSNR = 10 log10n1n2n3(Xmax  Xmin)2. X  X2F,. 17. where Xmax and Xmin denote the maximum and minimum entries of X, respectively, and X and X arethe recovered and underlying tensors, respectively. The SSIM is defined as. SSIM =(2xy + c1) (2xy + c2)2x + 2y + c1+2x + 2y + c2,. where x and x represent the mean intensity and standard deviation of the original image, respectively,y and y represent the mean intensity and standard deviation of the recovered image, respectively, xydenotes the covariance between the original and recovered images, and c1, c2 > 0 are constants. Formulti-dimensional images, the SSIM denotes the mean value of SSIM of all images.. 4.2.1Parameter Settings. For the parameters  and  in Algorithm 2, we set  = 10 and  = 1.618 in the experiments forsimplicity. For the parameter  in (14), it is set to 10.  is sensitive to the results for different cases andwe select it from the set {0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9, 1.2, 1.3, 1.4, 1.5, 2} to obtain the best recoveryperformance for tensor completion. For the two parameters  and  in MCP, we simply set  = 2.7and choose  from the set {0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2, 1.5, 1.6, 1.8, 2, 2.5} to achievethe best performance in the testing cases.. 4.2.2Multispectral Images. For tensor completion, we test two multispectral images to demonstrate the effectiveness of the pro-posed method, including Balloons and Lemons3, whose sizes are both 256  256  31. In Tables 3 and4, we show the PSNR and SSIM values of different methods for the Balloons and Lemons datasets,respectively, where  = 0.005, 0.01 and SR = 0.05, 0.10, 0.15, 0.20, 0.25, 0.30. The best results arehighlighted in bold and the second best results are highlighted in underline. We can see that the PSNRand SSIM obtained by LFNR are higher than those obtained by other methods. In particular, the LFNRoutperforms NORT in terms of PSNR and SSIM values, which demonstrates that the nonconvex regu-larization based on TTNN is better than that based on the sum of nuclear norms of unfolding matricesof a tensor. Besides, the TTNN performs better than SNN, TMac, and NORT for most cases, especiallyfor  = 0.01.Figure 1 displays the PSNR values versus index of band of different methods for the Balloons andLemons datasets, where SR = 0.5,  = 0.05 for the Balloons dataset, and SR = 0.4,  = 0.1 forthe Lemons dataset. As can be seen from the two figures that the LFNR performs best compared withSNN, TMac, NORT, and TTNN for almost all bands. And the TTNN outperforms SNN, TMac, andNORT in terms of PSNR values for most bands.Figures 2 and 3 present the recovered images and zoomed regions of the 30th band of differentmethods for the Balloons and Lemons datasets, where SR = 0.4,  = 0.05 for the Balloons datasets,and SR = 0.3,  = 0.05 for the Lemons dataset. We can see that the images recovered by LFNR arebetter than other comparison methods in term of visual quality, especially from the zoomed regions.In fact, the LFNR can preserve the details and edges of images better than SNN, TMac, NORT, andTTNN.. 4.2.3MRI Dataset. In this subsection, we test a magnetic resonance imaging (MRI) dataset from Brainweb4 to show thegood performance of LFNR, where the size of MRI is 181  217  100 and the noise level  is set to. 3https://www.cs.columbia.edu/CAVE/databases/multispectral/4https://brainweb.bic.mni.mcgill.ca/brainweb/. 18. Table 5: PSNR and SSIM values of different methods for the MRI dataset with different SRs.. IndexSRSNNTMacNORTTTNNLFNR. PSNR. 0.0514.2217.4117.3017.2817.880.1015.5819.8519.5119.1320.020.1516.8120.8120.8020.5921.770.2017.9921.422.0321.8423.240.2519.1221.8823.5623.0924.500.3020.2122.3124.2624.1725.600.3521.3122.7024.7625.3326.710.4022.4423.0825.2326.3627.750.4523.5624.8025.6727.3128.720.5024.7125.3126.1128.2929.68. SSIM. 0.050.24850.34470.31040.31070.35260.100.33460.51430.49550.45670.51360.150.42820.58050.57860.62180.62680.200.51740.61930.65720.61890.69870.250.59660.65270.73980.69130.75210.300.66510.68030.76440.74550.79140.350.72390.70180.78380.78410.82610.400.77400.72290.80190.81890.86010.450.81550.80250.81920.83780.88230.500.85000.82140.83590.86070.9002. 0102030405060708090100Index of frame. 21. 23. 25. 27. 29. 31. 33. PSNR. SNNTMacNORT. TTNNLFNR. (a). 0102030405060708090100Index of frame. 24. 26. 28. 30. 32. 34. PSNR. SNNTMacNORT. TTNNLFNR. (b). Figure 4: PSNR values versus index of frame of different methods for the MRI dataset. (a) SR = 0.45and  = 0.005. (b) SR = 0.6 and  = 0.02.. be 0.01. Table 5 lists the PSNR and SSIM values of different methods for the MRI dataset at differentSRs. where the best and second results are highlighted in bold and in underline, respectively. It canbe observed from Table 5 that the LFNR, TMac, NORT and TTNN always reach higher PSNR andSSIM values than the SNN model. In particular, the proposed LFNR method obtains the best recoveryperformance on both PSNR and SSIM values compared with other approaches. And the LFNR method. 19. <image: DeviceRGB, width: 1600, height: 1600, bpc: 8>. (a) Original. <image: DeviceRGB, width: 1600, height: 1600, bpc: 8>. (b) Observation. <image: DeviceRGB, width: 1600, height: 1600, bpc: 8>. (c) SNN: 25.77. <image: DeviceRGB, width: 1600, height: 1600, bpc: 8>. (d) TMac: 27.73. <image: DeviceRGB, width: 1600, height: 1600, bpc: 8>. (e) NORT: 28.55. <image: DeviceRGB, width: 1600, height: 1600, bpc: 8>. (f) TTNN: 30.27. <image: DeviceRGB, width: 1600, height: 1600, bpc: 8>. (g) LFNR: 31.51. Figure 5: The recovered images (with PSNR values) and zoomed regions of different methods for the74th frontal slice of the MRI dataset, where SR = 0.5 and  = 0.005.. has at least 1dB improvement in terms of PSNR values in comparison with the TTNN model.In Figure 4, we show the PSNR values of each frame of different methods for the MRI dataset,where SR = 0.45 and  = 0.005 in Figure 4(a), and SR = 0.6,  = 0.02 in Figure 4(b). Wecan see that LFNR achieves higher PSNR values than other methods for all frames. Besides, theTTNN outperforms SNN, TMac, NORT in most frames. Figure 5 exhibits the visual results of differentmethods on the 74th frontal slice of the MRI dataset, where SR = 0.5,  = 0.005. Compared withSNN, TMac, NORT and TTNN, the images obtained by LFNR is visually closest to the original image,especially for the zoomed regions. In fact, the LFNR can preserve the details and texture better thanother methods.. 4.3Logistic Regression for Binary Classification. In this subsection, we investigate the logistic regression loss for binary classification in problem (4).Specifically, we consider the case in which the observation pairs {(Zi, yi)}ni=1 are drawn independentand identically distributed (i.i.d.) from a distribution of the form. P(yi|Zi, X) = exp{yiZi, X  log (1 + exp(Zi, X))}.(26). Here the response yi takes binary values {0, 1}. In addition, X is an unknown parameter tensor, whichneed to be estimated. By maximum likelihood estimate, the loss function based on the distribution ofthe observations in (26) can be written as. fn,y(X) := 1. n. n. i=1[log(1 + exp(Zi, X))  yiZi, X].(27). 20. Then problem (4) reduces to. X = argminXc. 1n. n. i=1[log (1 + exp(Zi, X))  yiZi, X] + G(X). . .(28). Model (28) combines the logistic regression loss with nonconvex regularization. In particular, whenG is the TTNN, model (28) is also called TTNN for brevity.We compare our methods with the support vector machine (SVM), which comes from the fitcsvmfunction in MATLAB, the dual structure preserving kernels approach (DuSK)5 [25], support tensortrain machine by employing tensor train as the parameter model (STTM)6 [13], logistic loss with over-lapped trace norm (LOTN) [69]. The CIFAR-10 dataset7 is tested in the experiments for binary classi-fication, where there are 60000 color images (32  32  3) from 10 classes, including 50000 trainingimages and 10000 testing images. In the following experiments, each case run 10 times to reduce theimpact of randomness, and the final results are reported by the average value of all results of ten times.In particular, Zi (with size 32  32  3) are the training images and yi are the labels of the correspond-ing training images in our experiments for image classification, i = 1, . . . , n. Here n is the number oftraining samples.Let m be the number of the testing images. Then the response of the testing images is defined as. ytestpj=1. 1 + exp(Ztestj, X),j = 1, 2, . . . , m,. where Ztestjdenotes the j-th testing image and X is the trained parameter in (28). If ytestpj> 0.5, setytestpj= 1, otherwise, ytestpj= 0. The testing accuracy (TAc) of classification is defined as. TAc = 1  1. m. m. j=1|ytestpj ytestj |,. where ytestjdenotes the true category of the j-th testing image.. 4.3.1Parameter Settings. For the experiments of image classification, we set  = 100, and choose  from the set {10, 100} to getthe best classification results. For the parameters  and  in the MCP function, we choose  from the set{0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 0.9, 1.5, 2.5, 3.4, 5, 6} and  from the set {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8} toobtain the best classification accuracy. In addition, the penalty parameter  is chosen from the set{0.1, 0.2, 0.3, 0.4, 0.6, 0.7} to achieve the best results.. 4.3.2Image Classification. In this subsection, we demonstrate the effectiveness of the LFNR for binary image classification. First,we select two out of the ten classes in CIFAR-10. Then the first 250 images of the two classes are usedin our experiments. More specifically, we randomly choose 200 images from each class for training,and employ the remaining ones as the testing images. Table 6 displays the classification accuracyof different methods for the CIFAR-10 dataset. It can be observed from this table that the LFNR. 5https://github.com/LifangHe/SDM14_DuSK6https://github.com/git2cchen/STTM7https://www.cs.toronto.edu/kriz/cifar.html. 21. Table 6: Classification accuracy of different methods for the CIFAR-10 dataset.. Class 1Class 2SVMDuSKSTTMLOTNTTNNLFNR. dogfrog0.6750.7380.7210.6960.7470.753automobilebird0.7870.8390.8260.8200.8570.861birddeer0.6250.7190.6760.6490.7150.737catbird0.6370.7120.7160.7000.7380.742birddog0.6320.7190.7030.6470.7150.730truckautomobile0.5980.6680.6660.6240.6800.686birdhorse0.6850.7270.6950.7140.7230.750deerfrog0.6650.7350.7050.7120.7470.752frogautomobile0.8360.8790.8550.8530.8680.882birdairplane0.6230.7800.7520.6890.7960.812. 100200300400500600700800900Number of training samples. 65. 67. 69. 71. 73. 75. 77. 79. TAc(%). SVMDuskSTTM. LOTNTTNNLFNR. Figure 6: Classification accuracy versus number of training samples of different methods for theCIFAR-10 dataset.. performs better than SVM, DuSK, STTM, LOTN, and TTNN in terms of classification accuracy forthe testing cases. Besides, for the classes bird and deer, bird and dog, bird and house, frogand automobile, the DuSK outperforms TTNN in terms of classification accuracy. However, theTTNN performs better than other methods for other testing cases. In particular, the TTNN can achievehigher classification accuracy than LOTN for these images, which implies that the TTNN can explorethe low-rankness better than overlapped trace norm.Now we test the influence of different number of training samples of different methods for imageclassification. Two categories of deer and horse are used in our experiments, where 100 images arerandomly selected from each class as the testing samples and the number of training samples variesfrom 100 to 900 with step size 100. In Figure 6, we show the classification accuracy of differentmethods versus number of training samples for the deer and horse classes. We can see that theclassification accuracy of LFNR is higher than those of SVM, DuSK, STTM, LOTN, and TTNN.Furthermore, the classification accuracy of these methods increases as the number of training samplesincreases.. 22. 5Concluding Remarks. In this paper, we have studied the problem of low-rank tensor learning and proposed the LFNR modelbased on TTNN. Specifically, in order to explore the low-rankness of the underlying tensor, a familyof nonconvex functions were employed onto the singular values of all frontal slices of a tensor in thetransformed domain. Besides, a general loss function was considered in the proposed model, whichcould be specified to the least squares loss and logistic regression loss, respectively. And the corre-sponding models were suitable for tensor completion and binary classification. Then we establishedthe error bound between the stationary point of the LFNR model and the underlying tensor under theRSC condition on the loss function and some regularity conditions on the nonconvex penalty function.Furthermore, based on the DC structure of the nonconvex regularization, a PMM algorithm was de-signed to solve the proposed model, where the loss function and one convex function in DC structurewere linearized with a proximal term. Besides, we showed that the sequence generated by PMM con-verges globally to a stationary point of LFNR under very mild conditions. Numerical experiments ontensor completion and binary classification were reported to demonstrate the effectiveness of LFNRcompared with other methods.In the future work, we are going to extend our model to more general loss functions, where the RSCconditions are utilized to more general loss functions. Another possible extension is to incorporate therandom method instead of using SVD directly to accelerate the PMM. It would be also of great interestto analyze the KL exponent of the objective function of the LFNR model.. Acknowledgments. The authors would like to thank Dr. Silvia Gandy for providing the code of SNN in [19].. Appendix A. Least Squares Loss for Tensor Completion. In the following, we show that the loss function in (25) satisfies the RSC condition. The least squaresloss function in (25) can be also rewritten as. fn,Y(X) = 1. 2pP(X  Y)2F = 1. 2p. . (i,j,k)(Aijk, X  Yijk)2,. where Aijk  Rn1n2n3 denotes a basic tensor whose (i, j, k)-th element is 1 and other elements are0. Note that the gradient of the loss function fn,Y(X) is given by. fn,Y(X) = 1. p. . (i,j,k)(Aijk, X  Yijk)Aijk.(29). Now we show that fn,Y(X) satisfies the RSC condition in (6) with high probability, which is statedin the following lemma.. Lemma 1 For any V  Rn1n2n3, there exists a constant 1  (0, 1) such that. fn,Y(X  + V)  fn,Y(X ), V  (1  1)V2F ,(30). with probability at least 1  exp( 221n2. d3 ).. 23. Proof. For any V  Rn1n2n3, we let b := V. By (29), one can get. fn,Y(X  + V)  fn,Y(X ), V. =. 1p. . (i,j,k)(Aijk, X  + V  Yijk)Aijk  (Aijk, X   Yijk)Aijk, V. . =. 1p. . (i,j,k)Aijk, VAijk, V. . = 1. p. . (i,j,k)(Aijk, V)2.. (31). Denote ijk = (Aijk, V)2 if (i, j, k)  , 0 otherwise, 1  i  n1, 1  j  n2, 1  k  n3. Noticethat the observations are sampled at random, we know that. ijk =. V2ijk,with probability p,. 0,with probability 1  p.(32). Note that {ijk} are independent random variables and the expectation of ijk is E(ijk) = pV2ijk. Let. =. n1. i=1. n2. j=1. n3. k=1ijk,. then E() = p n1i=1n2j=1n3k=1 V2ijk = pV2F . Notice that 0  ijk  b2. For any  > 0, it followsfrom the Hoeffdings inequality [64, Proposition 2.5] that. P(  E() < ) < exp22. db4. .. By taking  := 1pV2F , where 1  (0, 1) is a constant, we deduce that. P < E()  1pV2F= P1. p < (1  1)V2F. . < exp. . 221p2V4F. db4. . ,(33). which implies. P1. p  (1  1)V2F. 1  exp. . 221p2V4F. db4. . .(34). Since 1. d V. VF. d, we obtain that 1. d2 V4F. d2V4  1, which implies that. 0 < 1. d  V4F. dV4= V4F. db4 d.. As a consequence, (34) further implies that. P1. p  (1  1)V2F. = P. . 1. p. n1. i=1. n2. j=1. n3. k=1ijk  (1  1)V2F. . . 1  exp221n2. d3. ,. (35). 24. Combining (35) with (31) yields. Pfn,Y(X  + V)  fn,Y (X ) , V  (1  1)V2F 1  exp221n2. d3. .. This concludes the proof.2From Lemma 1, we know that fn,Y(X) defined in (25) satisfies the RSC condition in (6) by taking1 = 2 = 1  1, 1 = 2 = 0.. Appendix B. Logistic Regression Loss. First, we give the definition and property of a sub-Gaussian random variable, which plays a vital rolein the proof of Lemma 2.. Definition 7 [18] A random variable x is called sub-Gaussian if there exist constants a1,  > 0 suchthat P(|x|  t)  a1et2 for any t > 0.. Proposition 1 [18, Proposition 7.24] If x is sub-Gaussian with E(x) = 0, then there exists a constantb (depending only on a1 and ) such that. E[exp(x)]  expb2for all   R,(36). where the constant b is called a sub-Gaussian parameter of x.. Now we show that the logistic loss function fn,y in (27) satisfies the RSC condition (6) with highprobability in the following lemma.. Lemma 2 Assume that {(Zi, yi)}ni=1 are drawn i.i.d., where Zi, i = 1, 2, . . . , n, are sub-Gaussianwith independent, mean-zero, and sub-Gaussian entries with the same sub-Gaussian parameter b in(36), and the response variables yi  {0, 1}. Suppose that the number of samples n  4c42t2 log d,where c2 > 0 is a given constant and t is defined in Section 2.1. Then there exists some positiveconstant c3 such that the loss function fn,y in (27) satisfies the RSC condition (6) with probability atleast 1  exp (c3 log d).. Proof. Note that the gradient of fn,y(X) in (27) is. fn,y(X) = 1. n. n. i=1. exp(Zi, X). 1 + exp (Zi, X)  yi. Zi. .(37). For any third-order tensor V  Rn1n2n3, we have. fn,y(X  + V)  fn,y(X ). = 1. n. n. i=1. exp(Zi, X  + V). 1 + exp(Zi, X  + V) yi. . Zi. . 1. n. n. i=1. exp(Zi, X ). 1 + exp (Zi, X )  yi. Zi. . = 1. n. n. i=1. exp(Zi, X  + V). 1 + exp(Zi, X  + V)exp(Zi, X ). 1 + exp (Zi, X ). . Zi. . .. 25. Therefore, one can deduce that. fn,y(X  + V)  fn,y(X ), V. = 1. n. n. i=1. exp(Zi, X  + V). 1 + exp(Zi, X  + V)exp(Zi, X ). 1 + exp (Zi, X ). . Zi, V.(38). Define the function (u) := log (1 + exp(u)). Note that the first-order derivative of (u) is (u) =exp(u). 1+exp(u). Then (38) can be rewritten as. fn,y(X  + V)  fn,y(X ), V = 1. n. n. i=1. (Zi, X  + V)  (Zi, X )Zi, V. = 1. n. n. i=1(Zi, X  + tiZi, V)Zi, V2,. (39). where the second equality holds by the mean value theorem and ti  [0, 1]. Now we consider two casesVF  1 and VF > 1.Case I. Suppose that VF =   (0, 1]. First, similar to [64, Theorem 9.36], we introduce thefollowing assumptions. E(Zi, V2)  u and E(Zi, V4)  l,for any V  Rn1n2n3 with VF = 1,(40). where u, l > 0 are given constants. In (40), the first inequality is used to control the lower boundof covariance of the entries of Zi and the second inequality is used to simplify the upper bound ofE(Zi, V4). Indeed, by simple calculations, we can get E(Zi, V4)  (4b)2 2 = 64b2 [55, Proposi-tion 3.2]. Similar to [4, 40], we use a truncated version to the right-hand side of (39). Denote a trunca-tion level  := K with some constant K > 0. For any tensor V  Rn1n2n3 with VF =   (0, 1],we define. I[|Zi, V|  2] :=. 1,if |Zi, V|  2,. 0,otherwise,I[|Zi, V| > 2] :=. 1,if |Zi, V| > 2,. 0,otherwise.. In addition, for any given truncation level T > 0, define the functions. I[|Zi, X |  T] :=. 1,if |Zi, X |  T,. 0,otherwise,I[|Zi, X | > T] :=. 1,if |Zi, X | > T,. 0,otherwise.. Now we can represent (Zi, X  + tiZi, V)Zi, V2 in (39) via above functions as follows. fn,y(X  + V)  fn,y(X ), V. = 1. n. n. i=1(Zi, X  + tiZi, V)Zi, V2I[|Zi, V|  2]I [|Zi, X |  T]. + 1. n. n. i=1(Zi, X  + tiZi, V)Zi, V2I[|Zi, V|  2]I [|Zi, X | > T]. + 1. n. n. i=1(Zi, X  + tiZi, V)Zi, V2I[|Zi, V| > 2]I [|Zi, X |  T]. + 1. n. n. i=1(Zi, X  + tiZi, V)Zi, V2I[|Zi, V| > 2]I [|Zi, X | > T]. 1. n. n. i=1(Zi, X  + tiZi, V)Zi, V2I[|Zi, V|  2]I [|Zi, X |  T] ,. (41). 26. where the inequality holds by the fact that the second derivative  is always positive.Note that 0 <  = K  K. Define (T) := min|u|(T+2K) (u). An immediate conse-quence is that (Zi, X  + tiZi, V)I[|Zi, V|  2]I [|Zi, X |  T]  (T)I[|Zi, V| 2]I [|Zi, X |  T]. This together with (41) leads to. fn,y(X  + V)  fn,y(X ), V  (T) 1. n. n. i=1Zi, V2I[|Zi, V|  2]I [|Zi, X |  T] . (42). Next, we will demonstrate that, for 0  0,. 1n. n. i=1Zi, V2I[|Zi, V|  2]I [|Zi, X |  T]  u. 2 V2F  0(43). with high probability, where u is defined in (40). Note that  = K. For notational convenience, wedefine()(Zi, V) := Zi, V2I[|Zi, V|  2].(44). In particular, if  = 1, then (1)(Zi, V) = Zi, V2I[|Zi, V|  2K].By using the notation in (44), (43) can be rewritten as. 1n. n. i=1()(Zi, V)I [|Zi, X |  T]  u. 2 V2F  0,for any 0  0.(45). Furthermore, we claim that the inequality in (45) can be reduced to the case for  = 1. Let V1 =V. VF = V. , then V1F = 1. Note that. ()(Zi, V) = Zi, V2I[|Zi, V|  2K] = 2Zi, V12I[|Zi, V1|  2K] = 2(1)(Zi, V1).(46)Then (45) is equivalent to the following form. 1n. n. i=1(1)(Zi, V1)I [|Zi, X |  T]  u. 2  0. 2 ,for any 0  0.(47). In the following, we begin to prove that (47). Define xi := ()(Zi, V)I [|Zi, X |  T]. For anyi, one can easily check the random variable xi  [0, 4K2]. Note that the bounded random variables aresub-Gaussian [64, Example 2.4], which implies that xi is sub-Gaussian. Furthermore, we can obtainthat xi is also sub-exponential [64, Definition 2.7]. For any 0  0,. P. 1n. n. i=1(1)(Zi, V1)I [|Zi, X |  T]  1. n. n. i=1E(1)(Zi, V1)I [|Zi, X |  T] 0. 2. . = P. 1n. n. i=1()(Zi, V)I [|Zi, X |  T]  1. n. n. i=1E()(Zi, V)I [|Zi, X |  T] 0. . = P. 1n. n. i=1xi  1. n. n. i=1E(xi)  0. . 1  exp. . n20. 2nni=1 E(x2i ). . ,. (48). 27. where the first equality holds by (46), the second equality holds by the definition of xi, and the inequal-ity holds by the Bernsteins inequality [64, Propostion 2.14]. Suppose that we can prove that. E (xi) = E()(Zi, V)I [|Zi, X |  T] u. 2 V2F ,(49). which taken collectively with (44), (45), (48) yields the desired claim (43).Note that (49) can be written equivalently in the following form. E(1)(Zi, V1)I [|Zi, X |  T] u. 2 .(50). Observe that. E(1)(Zi, V1)I [|Zi, X |  T]. = EZi, V12I[|Zi, V1|  2K]I [|Zi, X |  T]. = EZi, V12I[|Zi, V1|  2K] EZi, V12I[|Zi, V1|  2K]I [|Zi, X | > T]. = EZi, V12 EZi, V12I[|Zi, V1| > 2K]. EZi, V12I[|Zi, V1|  2K]I [|Zi, X | > T]. u  EZi, V12I[|Zi, V1| > 2K]. EZi, V12I[|Zi, V1|  2K]I [|Zi, X | > T],. (51). where the first equality follows from (46) and the inequality holds by the condition EZi, V12 u.By Cauchy-Schwarz inequality, we get that. EZi, V12I[|Zi, V1| > 2K]. . EZi, V14. P|Zi, V1| > 2K. l. . P|Zi, V1| > 2K,. (52). where the last inequality follows from (40). On the other hand, we can deduce. P|Zi, V1| > 2K= P|Zi, V1|4 > 16K4 E(Z, V14). 16K4l. 16K4 ,(53). where the first inequality follows from Markov inequality [18, Theorem 7.3] and the second inequalityfollows from (40). Set K2  l. u . Combining (53) with (52) yields. EZi, V12I[|Zi, V1| > 2K]l. 4K2  u. 4 .(54). By a similar discussion, we obtain. P (|Zi, X | > T)  EZi, X 4. T 4=X 4F EZi,X. X F 4. T 4 X 4F l. T 4.(55). 28. By Cauchy-Schwarz inequality, one can easily get. EZi, V12I[|Zi, V1|  2K]I [|Zi, X | > T]. . . EZi, V12I[|Zi, V1|  2K]2. P (|Zi, X | > T). . . EZi, V14. P (|Zi, X | > T)  lX 2F. T 2,. where the last inequality follows from (55) and (40). Taking T 2  4X 2F l. uyields. EZi, V12I[|Zi, V1|  2K]I [|Zi, X | > T] u. 4 .(56). Combining (51), (54) and (56) then gives. E(1)(Zi, V1)I [|Zi, X |  T] u  u. 4  u. 4 = u. 2 .(57). With the above analysis, we can establish the desired result (50). From (48), (49) and (50), for anyVF  (0, 1], we get. P. 1n. n. i=1()(Zi, V)I [|Zi, X |  T]  u. 2 V2F  0. . 1  exp. . n20. 2nni=1 E(x2i ). . .. (58)Combining this with (44) yields the claim (43). Taking this collectively with (42), we arrive at. Pfn,y(X  + V)  fn,y(X ), V  (T)u. 2V2F  (T)0. . 1  exp. . n20. 2nni=1 E(x2i ). . .(59). Note thatx2i = Zi, V4I[|Zi, V|  2]I [|Zi, X |  T] .. Denote zi := Zi, V = vec(Zi), vec(V), i = 1, 2, . . . , n. Note that Zi are sub-Gaussian with i.i.d.zero-mean entries with same sub-Gaussian parameter b in (36), which implies that the random variablezi = vec(Zi), vec(V) is sub-Gaussian with parameter bV2F [18, Theorem 7.27]. Then we have. E(x2i ) = EZi, V4I[|Zi, V|  2]I [|Zi, X |  T]). . . EZi, V4I[|Zi, V|  2]2. P(|Zi, X |  T). . . EZi, V4I[|Zi, V|  2]2. . E(Zi, V8)  256b2V4F ,. where the first inequality follows from the Cauchy-Schwarz inequality and the last inequality followsfrom [55, Proposition 3.2]. Consequently,. n20. 2nni=1 E(x2i )  n20. 2nni=1 256b2V4F= n20. 512b2V4F= n20. V4F,(60). 29. where  := 512b2. Let. 0 = c2. . log d. n VTTNNVF ,. where c2 > 0 is a given constant. By using the arithmetic mean-geometric mean inequality, we have. 0 = c2. . log d. n VTTNNVF =. . 2c22. u. log d. n V2TTNN  u. 2 V2F  c22u. log d. n V2TTNN + u. 4 V2F .. Consequently, we deduce. (T)u. 2V2F  (T)0  (T)u. 2V2F  c22(T). u. log d. n V2TTNN  (T)u. 4V2F. = (T)u. 4V2F  c22(T). u. log d. n V2TTNN.(61). Combining (61), (60), and (59) yields. Pfn,y(X  + V)  fn,y(X ), V  (T)u. 4V2F  c22(T). u. log d. n V2TTNN. . 1  exp. . n20. V4F. . = 1  exp. . c22 log dV2TTNN. V2F. . 1  expc22 log d. . = 1  exp (c3 log d) ,. (62). where the second inequality holds by V2TTNN  V2F and the last equality holds by letting c3 := c22 .Case II. Suppose that VF > 1. Similar to [40, Lemma 8], we assume VTTNN  2t. DefineL() : [0, 1]  R as L() := f(X  +  V). Notice that L() is convex by the convexity of f,which implies that L is monotonously non-decreasing. Consequently, for any   [0, 1], one hasL(1)  L(0)  L()  L(0). Then we have. fn,y(X  + V)  fn,y(X ), V  1. fn,y(X  +  V)  fn,y(X ),  V.. Taking  =1. VF  (0, 1], we conclude that. fn,y(X  + V)  fn,y(X ), V  VF. (T)u. 4 c22(T). u. log d. nV2TTNN. V2F. . VF. (T)u. 4 2tc22(T)u. log d. nVTTNN. V2F. . VF. (T)u. 4 (T). u. . log d. nVTTNN. V2F. . VF. (T)u. 4 (T). u. . log d. nVTTNN. VF. . = (T)u. 4VF  (T). u. . log d. n VTTNN,. (63). 30. where the second inequality holds since the assumption VTTNN  2t, the third inequality followsfrom n  4t2c42 log d, and the last inequality holds since V2F  VF > 1.. Therefore, by combining (62) with (63), and taking 1 = 2 =(T)u. 4, 1 =c22(T). u, 2 =(T). u , we get that the loss function (27) satisfies the RSC condition (6) with probability at least 1 exp(c3 log d) .2. Appendix C. Auxiliary Lemmas. Lemma 3 Denote the function h(x) :=1. 1+exp(x), where x  R. Then. |h(x1)  h(x2)|  1. 4|x1  x2|, x1, x2  R.. Proof. For simplicity, we assume x1 < x2. Then there exists x3  (x1, x2) such that. |h(x1)  h(x2)| = |h(x3)(x1  x2)|.(64). Note that for any x  R, we have. h(x) =exp(x). (1 + exp(x))2 =1. 2 + exp(x) + exp(x)  1. 4,(65). where the inequality follows from the fact that exp(x) + exp(x)  2. exp(x)  exp(x) = 2. Inaddition, it is worth noting that h(x) > 0. Taking (64) collectively with (65) shows that. |h(x1)  h(x2)|  1. 4|x1  x2|.. This completes the proof.2Note that the gradient of fn,y(X) in (27) is given by. fn,y(X) = 1. n. n. i=1. exp(Zi, X). 1 + exp (Zi, X)  yi. Zi. .(66). Now we show that fn,y is Lipschitz continuous in the following lemma.. Lemma 4 The gradient fn,y in (66) is Lipschitz continuous with Lipschitz constant 1. 4nni=1 Zi2F ,i.e.,. fn,y(X1)  fn,y(X2)F  1. 4n. n. i=1Zi2F X1  X2F ,  X1, X2  Rn1n2n3.. Proof. For any tensor X1, X2  Rn1n2n3, we obtain. fn,y(X1)  fn,y(X2) = 1. n. n. i=1. exp(Zi, X1). 1 + exp(Zi, X1) exp(Zi, X2). 1 + exp (Zi, X2). Zi. .(67). Let h(Zi, X) =1. 1+exp(Zi,X). Then (67) can be equivalently expressed as. fn,y(X1)  fn,y(X2) = 1. n. n. i=1[(h(Zi, X1)  h(Zi, X2)) Zi] .. 31. Thus, we get. fn,y(X1)  fn,y(X2)F = 1. n. . n. i=1[(h(Zi, X1)  h(Zi, X2)) Zi]F. 1. n. n. i=1. (h(Zi, X1)  h(Zi, X2)) Zi. F. = 1. n. n. i=1|h(Zi, X1)  h(Zi, X2)|ZiF ,. (68). where the inequality holds by the Minkowskis inequality. Using Lemma 3 further leads to. |h(Zi, X1)  h(Zi, X2)|ZiF  1. 4|Zi, X1  Zi, X2|ZiF. = 1. 4|Zi, X1  X2|ZiF. 1. 4ZiF X1  X2F ZiF. = 1. 4Zi2F X1  X2F ,. (69). where the second inequality follows from the Cauchy-Schwarz inequality. Plugging (69) into (68)immediately yields. fn,y(X1)  fn,y(X2)F  1. 4n. n. i=1Zi2F X1  X2F .. This concludes the proof.2. Lemma 5 For any matrix X  Rn1n2, define h(X) := min{n1,n2}j=1g(j(X)), where g() satisfiesAssumption 1. Then. X  h(X). k0+. 2k0X2F .. Proof. Firstly, we provek0x  g(x) +. 2 x2, for any x  0,(70). where k0,  are the parameters in Assumption 1. The above inequality is trivial for x = 0. For x > 0,it follows from Assumption 1 that there exists  > 0 such that g (x) +. 2x2 is convex, which yields. g (x) +. 2 x2  g(x) +. 2 (x)2 + g(x) + x, x  x, for any x, x > 0.(71). Here g() represents the derivative of g(). By taking x  0+ in (71), under Assumption 1, we canget that (70) holds.Secondly, one has. k0X =. min{n1,n2}. j=1k0j(X). min{n1,n2}. j=1(g(j(X)) +. 2 (j(X))2). = h(X) +. 2 X2F ,. (72). where the last equation follows from the fact that X2F = min{n1,n2}j=1(j(X))2. This completes theproof.2. 32. Lemma 6 For any X  Rn1n2n3, define (X) := G(X) +. 2 X2F , where G(X) is defined in(4) and the parameter  is the same in Assumption 1(iv). Then (X) is convex.. Proof. Let (x) := g(x) +. 2x2 and m := min{n1, n2}. Under Assumption 1(iv), for any X, Y Rn1n2n3, i = 1, . . . , n3, j = 1, . . . , m, the convexity of (x) leads to. gj( X iU ) + (1  )j( YiU )+. 2. j( X iU ) + (1  )j( YiU )2. gj( X iU )+. 2. j( X iU )2+ (1  )gj( YiU )+ (1  ). 2. j( YiU )2,   [0, 1].. Summing up the above inequalities from i = 1, . . . , n3, j = 1, . . . , m yields. n3. i=1. m. j=1gj( X iU ) + (1  )j( YiU )+. 2. j( X iU ) + (1  )j( YiU )2. . n3. i=1. m. j=1gj( X iU )+. 2. j( X iU )2+ (1  )gj( YiU )+ (1  ). 2. j( YiU )2.. (73)Moreover, by [27, Corollary 3.4.3], we have. m. j=1j X iU + (1  ) YiU. m. j=1j X iU+ j(1  ) YiU, i = 1, 2, . . . , n3.. Note that (x) = g(x) +. 2x2 is a real-valued convex function on [0, +). Then by [27, Lemma3.3.8], we obtain. n3. i=1. m. j=1j X iU + (1  ) YiU. n3. i=1. m. j=1j X iU+ j(1  ) YiU,. which yields. n3. i=1. m. j=1gj X iU + (1  ) YiU+. 2. j X iU + (1  ) YiU2. . n3. i=1. m. j=1gj X iU+ j(1  ) YiU+. 2. j X iU+ j(1  ) YiU2. =. n3. i=1. m. j=1gj( X iU ) + (1  )j( YiU )+. 2. j( X iU ) + (1  )j( YiU )2.. (74). Plugging (74) into (73) yields. n3. i=1. m. j=1gj X iU + (1  ) YiU+. 2. j X iU + (1  ) YiU2. . n3. i=1. m. j=1gj( X iU )+. 2. j( X iU )2+ (1  )gj( YiU )+ (1  ). 2. j( YiU )2.. (75). 33. By the definition of G and the fact that X2F = n3i=1mj=1(j( X iU ))2, (75) implies that. G(X + (1  )Y) +. 2 X + (1  )Y2F. G(X) +. 2 X2F + (1  )G(Y) + (1  ). 2Y2F ,. which is equivalent to. (X + (1  )Y)  (X) + (1  )(Y),  [0, 1].. Therefore, (X) is convex.2. Lemma 7 For any tensors X1, X2  Rn1n2n3, the following inequality holds:. Z, X1  X2  G(X1)  G(X2) +. 2 X1  X22F ,  Z  (G(X2)),. where  is defined in (4) and  is defined in Assumption 1(iv).. Proof. As a consequence of Lemma 6, the function G(X) +. 2 X2F is convex with  > 0, whichyields. G(X1) +. 2 X12F  G(X2) +. 2 X22F +  Z + X2, X1  X2,. for any Z  (G(X2)). The above inequality can be rewritten as. Z, X1  X2  G(X1)  G(X2) +. 2 X12F. 2 X22F  X2, X1 + X22F. = G(X1)  G(X2) +. 2 X1  X22F .. The proof is completed.2For any matrix A  Rn1n2, we define r(A) to be the best rank r approximation of A. Denote. r(A) = A  r(A)(76). and. (A) =. m. j=1g(j(A)),(77). where g() satisfies Assumption 1.. Lemma 8 For any X, Y  Rn1n2 with rank(X) = r, one has. (r(Y))  (r(X  Y)),(78). where r() and () are defined as in (76) and (77).. Proof. For any X, Y  Rn1n2, it follows from [27, Theorem 3.3.16] that. j(Y)  1(X) + j(Y  X),j = r + 1, . . . , m,. 34. where m = min{n1, n2}. Summing up the above inequality over j = r + 1, . . . , m, we obtain. m. j=r+1j(Y). m. j=r+1(j(Y  X) + 1(X)).. Note that g is convex and non-increasing on [0, +) by Assumption 1(i), which together with [27,Lemma 3.3.8] leads to. m. j=r+1g(j(Y)). m. j=r+1g(j(Y  X) + 1(X)). . m. j=r+1g(j(Y  X)) =. m. j=r+1g(j(X  Y)),. (79). where the second inequality holds since j(Y  X) + 1(X)  j(Y  X)  0 for any j = r +1, . . . , m and the fact g is non-increasing. By definitions of r() and () in (76) and (77), weknow that (79) is equivalent to(r(Y))  (r(X  Y)).. This completes the proof.2. Lemma 9 For any X, Y  Rn1n2 with rank(X) = r, the following inequality holds. (X)  (Y)  (r(E))  (r(E)),. where E := X  Y, r(), r(), and () are defined as in (76) and (77).. Proof. By the definitions of , r and r in (76) and (77), we can obtain that. (X)  (Y) = (r(X)) + (r(X))  (r(Y))  (r(Y)). = (r(X))  (r(Y))  (r(Y)). =. r. j=1g(j(X)). r. j=1g(j(Y))  (r(Y)). =. r. j=1g(j(X  Y + Y)). r. j=1g(j(Y))  (r(Y)). . r. j=1g(j(X  Y))  (r(Y)). = (r(E))  (r(Y)). (r(E))  (r(E)),. (80). where the first inequality follows from [43, Theorem 2.6] and the second inequality follows fromLemma 8.2. Appendix D. Subdifferential of G (X) in (5). Lemma 10 For any matrix X  Rn1n2 and any tensor X  Rn1n2n3, define (g  )(X) :=min{n1,n2}j=1g(j(X)) and H( XU) := n3i=1min{n1,n2}j=1g(j( X iU )) . Then. Z  (g  )(X) ZU  H( XU).. 35. Proof. By the definition of the subdifferential of a function, we know that for any Z  (g  )(X),there exist a sequence {Xk} such that Xk converges to X with (g  )(Xk)  (g  )(X) and asequence of regular subdifferential Zk  (g)(Xk) such that Zk  Z. Notice that (g)(Xk) =H( X kU). For any YkU  Cn1n2n3, where  YkUF  0 as k  +, the following inequality holds. H( X kU + YkU) = (g  )(Xk + Yk). (g  )(Xk) + Zk, Yk + o(YkF ). = H( X kU) +  ZkU, YkU + o( YkUF ),. where the first inequality follows from the definition of the regular subdifferential of g   at Xk andthe second equation holds since Zk, Yk =  ZkU, YkU and YkF =  YkUF [59, Definition 5]. Hereo() denotes a higher-order infinitesimal. As a consequence, we can deduce that ZkU  H( X kU) foreach k.Since Zk  Z, we get that ZkU ZU.It can be easily shown that the sequence { X kU}converges to XU and {H( X kU)} converges to H( XU). Since g() is continuous, we know thatH( X kU) and H( X kU) are closed with H( X kU)  H( X kU) [56, Theorem 8.6], which yieldsZU  H( XU)  H( XU).By using similar arguments to the proof of the opposite inclusion, we can easily obtain that for anyZU  H( XU), Z  (g  )(X) holds.2. Lemma 11 For any tensor X  Rn1n2n3, one has. H( XU) = fold3((g  )(X)),. where H and g   are defined as the same in Lemma 10, and fold3() operating on a set means tooperate each element of the set.. Proof. For any YU  H( XU), it follows from Lemma 10 that bdiag( YU) = Y  (g  )(X). Bythe definition of fold3() in (3), we get that YU = fold3(Y)  fold3((g  )(X)), which implies thatH( XU)  fold3((g  )(X)).A similar argument leads to the opposite direction, i.e., fold3((g  )(X))  H( XU). Thiscompletes the proof.2. Lemma 12 For any X  Rn1n2n3, the transformed tensor SVD of X is denoted by X = U U  UVT . Let G(X) be defined in (4). Then the subdifferential of G (X) at X is given by. G(X) =U U D U VT | DiU  Bi, i = 1, 2, . . . , n3,. where Bi is defined as. Bi =Diag(di) | di = (di1, di2, . . . , dim)T  Rm, dij  g(j( X iU )), j = 1, 2, . . . , m(81). and m := min{n1, n2}.. Proof. Note that (g  )(X) := mj=1 g(j(X)) for any matrix X  Rn1n2. For any X Rn1n2n3, by [34, Theorem 7.1], we get that. (g  )(X) =bdiag( UU)  bdiag( DU)  bdiag(VU) | DiU  Bi, i = 1, 2, . . . , n3,(82). 36. where Bi is defined in (81). It follows from Lemma 11 that. (g  )(X) = bdiag(H( XU)),(83). where H is defined as the same in Lemma 10 and bdiag() operates a set means that bdiag() operateseach element of the corresponding set. In light of the definitions of G and H, it is obvious that. G(X) = H( XU) = H(U[X]).(84). Under Assumptions 1, one can easily get that H() is a proper and lower semicontinuous function. Onthe other hand, the tensor U[X] is defined as multiplying by a unitary matrix U onto each tube of X,which is an invertible linear transformation. Applying [56, Exercise 10.7] to (84) yields. G(X) = UT [H(U[X])] = UT [H( XU)].(85). Here for abuse of notation, we use U[E] to denote a set with each element of E being operated by U.Taking (85) with (83), we obtain that. bdiag(U[G(X)]) = bdiag(H( XU)) = (g  )(X),. which together with (82) and the definition of U-product further yields. G(X) = UT [fold3((g  )(X))]. =U U D U VT | DiU  Bi, i = 1, 2, . . . , n3,. where Bi is defined in (81). The proof is completed.2. Appendix E. Proof of Theorem 2. Step 1. Let  = X  X . Suppose that F > 1, it can be easily seen from (6) that. 2F  2. log(d)/nTTNN  fn,Y (X  + )  fn,Y(X ),. = fn,Y( X)  fn,Y (X ) , X  X. = fn,Y( X) + Z  Z  fn,Y(X ), X  X. = fn,Y( X) + Z, X  X  + Z  fn,Y(X ), X  X ,(86)where Z  (G( X)) will be given in detail later. Since X is a stationary point of (4), we get that. 0  fn,Y( X) + (G( X)) + D( X),. where D(X) is the indicator function of the set D := {X : X  c}. Then there exists a tensorZ  (G( X)) such that fn,Y( X)Z  D( X) = ND( X), where ND( X) denotes the normalcone of D at X and the equality holds by [5, Example 3.5]. Since X   D is feasible, by the definitionof normal cone (e.g., see [5]), we have. fn,Y( X) + Z, X  X   0.(87). Taking (87) together with (86) yields. 2F  2. . log d. n TTNN  Z  fn,Y(X ), X  X. = Z, X  X I1. + fn,Y(X ), X  X I2. .(88). 37. Upper bound of I1. By virtue of the definition of the inner product of two tensors [59, Definition5], we immediately obtain. Z, X  X  = Z,  Z  = ZU TTNN ,(89). where the first inequality holds by the Holders inequality. In addition, by Lemma 12 and Assumption1(iii), we obtain ZU  k0, which together with (89) yields. Z, X  X   k0 TTNN .(90). Upper bound of I2. Similar to the analysis of I1, we have. fn,Y(X ), X  X  = fn,Y(X ),. fn,Y (X )= fn,Y (X )U TTNN. k0. 4TTNN ,. (91). where the last inequality follows from (7).Substituting (91) and (90) into (88) yields. 2F  2. . log d. n TTNN k0 + k0. 4. TTNN = 5k0. 4TTNN .(92). Since XTTNN  t, we can deduce that. TTNN =  X  X TTNN   XTTNN + X TTNN  2t,(93). where the first inequality follows from [51, Remark 2]. This taken together with (92) indicates that. 2F  2. . log d. n TTNN  5k0t. 2.(94). Note that. 2. . log d. n TTNN  2. . log d22. 16t2 22 log dTTNN. = 2. 4t TTNN. 2. 4t  2t = 2. 2 .. (95). where the first inequality holds by (8) and the second inequality holds by (93). By taking (95) with(94), we obtain that. 2F  2. 2 + 5k0t. 2. 2. 2 + 52. 12 = 112. 12 ,(96). where the second inequality holds by the assumption  2. 6tk0 in (7). Then we can derive thatF  11. 12 < 1, which leads to a contradiction with the assumption F >1.Step 2. We know that F  1 from Step 1. Invoking the RSC condition in (6) gives. 12F  1log d. n 2TTNN  fn,Y(X  + )  fn,Y(X ), ,(97). 38. which together with (86) and (87) yields that. 12F  1log d. n 2TTNN  Z  fn,Y(X ), X  X. = Z, X  X  + fn,Y(X ), X  X .(98). It follows from Lemma 7 that. Z, X  X  = Z, X   X  G(X )  G( X) +. 2 X   X2F ,(99). where G is defined in (4). This together with (91) and (98) yields. 12F  1log d. n 2TTNN  G(X )  G( X) +. 2 X   X2F + k0. 4TTNN. = G(X )  G( X) +. 2 2F + k0. 4TTNN.. Rearranging the terms of the above inequality yields1. 2. 2F  G (X )  G( X) + k0. 4TTNN + 1log d. n 2TTNN.(100). Notice that. 1log d. n 2TTNN = 1. 2. . log d. n 2. . log d. n TTNN  TTNN. 1. 2. . log d. n 2. . log d. n 2t  TTNN,. (101). where the inequality follows from (93). In view of (7) and (8), one has. 12. . log d. n 1. 2. . log d22. 16t2 21 log d = 1. 4t,2. . log d. n k0. 4.(102). Plugging (102) into (101) then gives. 1log d. n 2TTNN  1. 4t  k0. 4 2t  TTNN = k0. 8TTNN,. which taken together with (100) gives1. 2. 2F  G(X )  G( X) + k0. 4TTNN + k0. 8TTNN. G(X )  G( X) + k0. 2TTNN.(103). It follows from Lemma 5 and Definition 4 that. k0. 2TTNN = k0. 2. n3. i=1. iU  k0. 2. n3. i=1. (iU ). k0+. 2k0. iU2. F. . =. n3. i=1. (iU ). 2+. 4. iU2. F. . ,. (104). 39. where (iU ) = mj=1 g(j(iU )).For any matrix A  Rn1n2, we define r(A) to be the best rank r approximation of A andr(A) = A  r(A). Thus, one can rewrite (104) equivalently as follows:. k0. 2TTNN. n3. i=1. (iU ). 2+. n3. i=1. 4. iU2. F. =. n3. i=1. 12. riiU+ riiU+. 4 U2F. =. n3. i=1. 12. riiU+ riiU+. 4 2F ,. (105). where the last equation follows from the fact that UF = F . Recalling the definition of G in(4), we know that G(X) = n3i=1 ( X iU ) for any X  Rn1n2n3. Suppose ri is the rank of matrix(X )iU , i = 1, . . . , n3. Consequently, we obtain. G(X )  G( X) =. n3. i=1(X )iU  Xi. U. . n3. i=1riiU riiU,. (106). where the inequality holds arises from Lemma 9. Taking this together with (103) and (105), one has1. 2. 2F. . n3. i=1. riiU riiU+. n3. i=1. 12. riiU+ riiU+. 4 2F. =. n3. i=1. 32 riiU. n3. i=1. 2 riiU+. 4 2F .. (107)Rearranging the above terms yields. 1  3. 4. 2F. n3. i=1. 32 riiU. n3. i=1. 2 riiU. . n3. i=1. 32 riiU.. (108). Step 3. It follows from [40, Lemma 4] that. n3. i=1riiU=. n3. i=1. ri. j=1g(j(iU )). n3. i=1. ri. j=1k0j(iU ) =. n3. i=1k0riiU . (109). 40. Combining (108) and (109), one gets that. 21  3. 4. 2F. n3. i=13k0riiU. n3. i=13k0ririiUF. . n3. i=13k0riiUF. 3k0. . n3. i=1riUF = 3k0. . n3. i=1ri F .. (110). where the second inequality holds by the fact that X  rXF for any X with rank at mostr, the third inequality holds since ri(iU )2F = rij=1(j(iU ))2  min{n1,n2}j=1(j(iU ))2 =. iU 2F , and the fourth inequality holds by the Holders inequality [45]. Consequently, one can deducethat. 21  3. 4. F  3k0. . n3. i=1ri,(111). which together with 1 > 3. 4implies that. X  X F  6k0n3i=1 ri. 41  3.(112). This completes the proof.. Appendix F. Locally Lipschitz Continuity of S2 in (11). First we give the locally Lipschitz continuity for the composition function between the singular valuevector and a differentiable function.. Lemma 13 Suppose that s2 satisfies Assumption 2. Define f : Rm  R as f(x) := mi=1 s2(xi). Forany matrix X  Cn1n2 with the singular value vector (X) := (1(X), 2(X), . . . , m(X))T  Rm,where m = min{n1, n2}, define (f  )(X) : Cn1n2  R as (f  )(X) := f((X)). Thenf   is differentiable. Moreover, (f  ) is locally Lipschitz continuous, i.e., for any given matrixX  Rn1n2, there exist constants  > 0, L1 > 0 such that. (f  )(X1)  (f  )(X2)F  L1X1  X2F ,  X1, X2  B( X, /(2m)).. Proof. Since s2 is convex, we know that f(x) is convex. Recall that a matrix is a signed permutationmatrix if there is just only one nonzero entry taken 1 or 1 in each row and column. Since s2 issymmetric, we get that f(Px) = f(x) for any x  R, which implies that f is absolutely symmetric.Since s2 is differentiable and f(x) = mi=1 s2(xi), we can deduce that f is differentiable. As aconsequence, we get that f   is differentiable at X [34, Proposition 6.2]. Let the singular valuedecomposition of X  Cn1n2 be X = UVT , where U  Cn1n1, V  Cn2n2 are unitarymatrices. It follows from [34, Proposition 6.2] that. (f  )(X) = U Diag(f((X)))VT .. 41. By the definition of f, we get that. g(x) := f(x) = (s2(x1), s2(x2), . . . , s2(xm))T  Rm,(113). where xi is the i-th component of x. Denote. M(X) := (f  )(X) = U Diag(g((X)))VT .. Since s2 is symmetric and differentiable, we know that s2(x) = s2(x). Therefore, for any signedpermutation matrix P  Rmm, one has g(Px) = Pg(x), which shows that g is mixed symmetric atx [15, Definition 2.1].Since the derivative s2 of s2 is locally Lipschitz continuous, we obtain that for any given xi, thereexist i, L0 > 0 such that |s2(yi)  s2(zi)|  L0|yi  zi|, yi, zi  B(xi, i) := {w : |w  xi|  i}.Therefore, for any given x = (x1, x2, . . . , xm)T  Rm,. g(y)  g(z)  L0y  z, y, z  B(x, ) := {w : w  x  },. where  =mi=1 2i . Consequently, g is locally Lipschitz continuous near x. For any given ma-. trix X  Cn1n2, it follows from [15, Theorem 3.3] that M() is locally Lipschitz continuous onB( X, /(2m)) with modulus. L1 = max{(2L0 + 0)/,. 2L0},(114). where 0 := maxi,j{|s2(i( X))  s2(j( X))|, |s2(i( X)) + s2(j( X))|}.2Next we show the locally Lipschitz continuity of S2 in (11), which is stated in the following lemma.. Lemma 14 Define S2(X) := n3i=1min{n1,n2}j=1s2(j( X iU )). Let X  Rn1n2n3 be a given tensor.Then there exists a constant  > 0 such that. S2(Y)  S2(Z)F  L1Y  ZF ,  Y, Z  B(X, /(2mn3)). where L1 is the same constant defined in (114).. Proof. Define f1 : Rmn3  R as f1(x) = mn3i=1 s2(xi), where m = min{n1, n2}. For any giventensor X  Rn1n2n3. it follows from Lemma 13 that. (f1  )(Y)  (f1  )(Z)F  L1Y  ZF ,  Y, Z  B(X, /(2mn3)).(115). Define P : Cn1n2n3  R as P( XU) := n3i=1min{n1,n2}j=1s2(j( X iU )). By a similar argument. as that in Lemma 10, we get that A = (f1  )(X) is equivalent to AU = P( XU), which im-plies that P( XU) = fold3((f1  )(X)). Note that Y  XF =  YU  XUF , which yieldsYU  B( XU, /(2mn3)). Similarly, we get that ZU  B( XU, /(2mn3)). Consequently, for anytensors YU, ZU  B( XU, /(2mn3)), we have. P( YU)  P( ZU)F = fold3((f1  )(Y))  fold3((f1  )(Z))F= (f1  )(Y)  (f1  )(Z)F L1Y  ZF. = L1 YU  ZUF ,. (116). 42. where the first inequality holds by (115).By the definitions of S2 and P, we get S2(X) = P( XU) = P(U[X]). By using a similar discussionin [56, Exercise 10.7], we can easily obtain that. S2(X) = UT [P(U[X])] = UT [P( XU)].(117). Additionally, we can easily obtain from YU, ZU  B( XU, /(2mn3)) that Y, Z  B(X, /(2mn3)).By (116) and (117), we obtain. S2(Y)  S2(Z)F = UT [P( YU)]  UT [P( ZU)]F. = UT [P( YU)  P( ZU)]F. = P( YU)  P( ZU)F. L1 YU  ZUF= L1Y  ZF ,. (118). where the third equality holds since the tensor Frobenius norm is unitarily invariant [14, Definition2.1]. This concludes the proof.2. Appendix G. Proof of Theorem 3. First, we give the sufficiently descent property of the objective function H(X) in (15).. Lemma 15 Let {X t} be the sequence generated by Algorithm 1. Suppose that fn,Y is Lipschitzcontinuous with Lipschitz constant L. Then for any  >L. 12 with   (0, 1. 2),. H(X t+1) + aX t+1  X t2F  H(X t),(119). where a := 12. 2   L. 2 > 0.. Proof. By virtue of the definitions (15) and (16), we immediately obtain. H(X t+1)  Q(X t+1, X t). = fn,Y(X t+1) + S1(X t+1)  S2(X t+1) + D(X t+1)  fn,Y(X t)  fn,Y(X t), X t+1  X t. . 2X t+1  X t2F  S1(X t+1) + S2(X t) + S2(X t), X t+1  X t  D(X t+1). = fn,Y(X t+1)  fn,Yf(X t)  fn,Y(X t), X t+1  X t. 2X t+1  X t2F. (S2(X t+1)  S2(X t)  S2(X t), X t+1  X t).(120)Since fn,Y is Lipschitz continuous with Lipschitz constant L, we can deduce from [5, Lemma 5.7]thatfn,Y(X t+1)  fn,Y(X t)  fn,Y(X t), X t+1  X t  L. 2 X t+1  X t2F .(121). Additionally, it follows from the convexity of S2 that. S2(X t+1)  S2(X t) + S2(X t), X t+1  X t.(122). 43. Combining (120), (121) and (122), we get that. H(X t+1)  Q(X t+1, X t)  L. 2 X t+1  X t2F. 2X t+1  X t2F. = L. 2X t+1  X t2F .(123). Recalling the definition of Q(X, X t) in (16), it can be easily verified that Q(X, X t) is convex. Thistogether with the definition of Wt+1 in (17) leads to. Q(X t, X t)  Q(X t+1, X t) + Wt+1, X t  X t+1. Q(X t+1, X t)  Wt+1F X t+1  X tF Q(X t+1, X t)  X t+1  X t2F .. (124). Taking (124) together with (123) yields. H(X t+1)  Q(X t+1, X t) + L. 2X t+1  X t2F. Q(X t, X t) + X t+1  X t2F + L. 2X t+1  X t2F. = Q(X t, X t) +L. 2  1  2. 2X t+1  X t2F .. (125). Therefore, we get that. H(X t+1) +1  2. 2  L. 2. X t+1  X t2F  Q(X t, X t) = H(X t),(126). where 12. 2   L. 2 > 0. This completes the proof.2Next, we give the relative error property of the iterations in Algorithm 1, which is stated in thefollowing lemma.. Lemma 16 Let {X t} be the sequence generated by Algorithm 1. Suppose that fn,Y and s2 areLipschitz continuous and locally Lipschitz continuous with Lipschitz constants L and L0, respectively.Then there exist N t+1  H(X t+1) and a positive integer K such that. N t+1F  X t+1  X tF ,  t  K,. where  := L + L1 + ( + 1) > 0 and L1 is defined in (114).. Proof. By (17), we obtain that there exists Yt+1  [S1(X t+1) + D(X t+1)] such that. Wt+1 = fn,Y(X t) + (X t+1  X t)  S2(X t) + Yt+1 and Wt+1F  X t+1  X tF .. DenoteN t+1 := fn,Y(X t+1)  S2(X t+1) + Yt+1.. Note thatH(X) = fn,Y(X)  S2(X) + [S1(X) + D(X)].. Consequently, we deduce that N t+1  H(X t+1).By (126), we know that X t+1  X tF tends to 0 as t tends to infinity, which implies that thereexists a constant  > 0 and positive integer K such that X t+1  X tF  , for any t  K. Note that. 44. the sequence {X t} is an approximate solution of Q(X, X t) in (16) at each iteration and D() is theindicator function of D. Therefore, X t  D, which implies that {X t} is bounded. Assume that X is acluster point of {X t}, there exists 0 > 0 such that X t  B( X, 0) holds for t  K. We further obtainthatX t+1  XF = X t+1  X t + X t  XF. X t+1  X tF + X t  XF  + 0.. Denote  :=  + 0. Then we have X t+1  B( X, ). Note that X t  XF  0 < , whichimmediately yields X t, X t+1  B( X, ). It follows from Lemma 14 that. S2(X t+1)  S2(X t)F  L1X t+1  X tF .(127). for any t  K.Notice that. N t+1 = fn,Y(X t+1)  S2(X t+1) + Wt+1  fn,Y(X t)  (X t+1  X t) + S2(X t).. Then we get that. N t+1F  fn,Y(X t+1)  fn,Y(X t)F + Wt+1F + X t+1  X tF+ S2(X t+1)  S2(X t)F LX t+1  X tF + ( + 1)X t+1  X tF + L1X t+1  X tF= (L + L1 + ( + 1))X t+1  X tF ,. (128). where the second inequality holds by (127) and the Lipschitz continuity of fn,Y. This completes theproof.2Now we return to prove Theorem 3 in detail. By the proof of Lemma 16, we know that {X t} isbounded. Consequently, there exists a subsequence {X ti} such that X ti converges to X as i tends to, where X is a cluster point of the sequence {X t}. Note that D is a closed set and X ti  D, thenX  D. Therefore, D(X ti) converges to D( X) as i tends to . Since fn,Y and G are continuous,we deduce that fn,Y(X ti) + G(X ti) converges to fn,Y( X) + G( X) as i tends to infinity, whichimplies that H(X ti) converges to H( X) as i tends to .Since D is a closed convex set, we know that D() is closed [5, Proposition 2.3], which impliesthat D() is lower semicontinouns [5, Theorem 2.6]. Since fn,Y and G are continuous, we obtain thatH(X) is lower semicontinouns. It is known that D(X) is semialgebraic since D is a semialgebraic set[7]. Then D(X) is a KL function [7, Theorem 3]. Since g is a KL function, G(X) is a KL function[7, 50]. Note that fn,Y is a KL function. As a result, H(X) = fn,Y(X) + G(X) + D(X) is a KLfunction. According to [3, Theorem 2.9], we conclude that the sequence {X t} converges to X as t goesto infinity, and X is a stationary point of H.. Appendix H. Proof of Theorem 4. First, we give a lemma about the sequence {X t} generated by Algorithm 1.. Lemma 17 Suppose that the assumptions in Theorem 3 hold and H(X) defined in (15) satisfies theKL property at X with an exponent   [0, 1). Then the following statements hold:. 45. (i) There exist constants 1, 1 > 0 and a KL exponent   (0, 1), such that. . j=tX j+1  X jF  X t  X t1F +1. 1(1  )(H(X t)  H( X))1.(129). (ii) For any positive integer t, the following inequality holds:. X t  XF. . j=tX j+1  X jF .(130). Proof. (i) Define a function p : R  R+ as. p(s) :=1. 1  (s  H( X))1, s  H( X),. where 1 > 0 is a constant and   (0, 1). It can be easily seen that p is a concave function, whosederivative function is given by p(s) =1. (sH( X)) for any s > H( X). From the concavity of p(s), onehasp(H(X t))  p(H(X t+1))  p(H(X t))(H(X t)  H(X t+1)). =1. (H(X t)  H( X)) (H(X t)  H(X t+1)).(131). Note that H(X) satisfies the KL property at X with an exponent   [0, 1). Then the KL inequality(2) yields. (H(X t)  H( X))  1(1  ) dist(0, H(X t))  1X t  X t1F ,(132). where the last inequality follows from   (0, 1) and Lemma 16. Here  is defined in Lemma 16. Thistaken together with (131) and Lemma 15 gives. p(H(X t))  p(H(X t+1))  aX t+1  X t2F. X t  X t1F,. where a is defined in Lemma 15. Furthermore, we can obtain that. 2X t+1  X tF  2. a. 1. 2 p(H(X t))  p(H(X t+1)) 1. 2 X t  X t1. 12F. . a. p(H(X t))  p(H(X t+1))+ X t  X t1F ,. where the second inequality follows from the fact that 2xy  x2 + y2. Summing the aforementionedinequality from t to infinity yields. 2. . j=tX j+1  X jF. . j=t. a. p(H(X j))  p(H(X j+1))+. . j=tX j  X j1F .. Consequently,. . j=t. X j+1  X jF  X t  X t1F +. a. . j=t(p(H(X j))  p(H(X j+1))). = X t  X t1F +. a. p(H(X t))  limj p(H(X j)). X t  X t1F +. ap(H(X t)). = X t  X t1F +1. a(1  )(H(X t)  H( X))1. = X t  X t1F +1. 1(1  )(H(X t)  H( X))1,. 46. where 1 := a. .(ii) Define Mt(X) := X t+1  X t. It follows from Theorem 3 that {X t+1}tN is bounded andMt(X)F = X t+1X tF <  for any t = 1, 2, . . . Additionally, we denote a bounded measurableset by G, which obeys {X t+1}tN  G. One can obtain that Mt(X) is a measurable function on G. Notethat, the sequence {X t}tN converges to X, which implies that nt=1 Mt(X) = nt=1X t+1  X t=X n+1  X 1 converges to X  X 1 as n tends to infinity. Similar to the argument in [70, Lemma 1(iv)],we can easily prove X t  XF  j=t X j+1  X jF . This completes the proof.2We now provide the details of the proof of Theorem 4, which follows a similar argument of [2, 70].(i) We must have H(X t0) = H( X) for some t0 when  = 0. Otherwise, for sufficiently large t,one has H(X t) > H( X). Apply the KL inequality to obtain 1 dist(0, H(X t))  1 for all t, it isimpossible since X t  X and 0  H( X). Then there exists some t0 such that H(X t0) = H( X).Note that H decreases monotonically, then X t = X t0 = X holds true for all t > t0.(ii) Let t := j=t X j+1  X jF . According to (130), it can be directly seen that t X t  XF . Then, (129) implies that. t =. . j=tX j+1  X jF  X t  X t1F +1. 1(1  )(H(X t)  H( X))1. X t  X t1F +1. 1(1  ). 1X t  X t1F 1. . = (t1  t) + a1(t1  t)1. ,. (133). where the second inequality follows from (132) and the last equality holds according to letting a1 :=1. 1(1)(1)1. .If 0 <   1. 2, then 1. 1. Similar to the proof in [70, Theorem 2(ii)], by (133), we can easilyshow that t a2. a2+1t1 holds, where a2 := a1 + 1. Denote w := 0 and  :=a2. a2+1  (0, 1). Itfollows from Lemma 17(ii) that. X t  XF  t  t1      t0 = wt.. (iii) If 1. 2 <  < 1, then 0 < 1. < 1. Let bt := tj=0 X j+1  X jF . By setting t = 1 in (129),we can obtain that. . j=0X j+1  X jF =. . j=1X j+1  X jF + X 1  X 0F. 2X 1  X 0F +1. 1(1  )(H(X 1)  H( X))1 < +,. (134). which implies that limt bt = j=0 X j+1  X jF exists. Observe that. t =. . j=tX j+1  X jF =. . j=0X j+1  X jF  bt1.. Hence, we have. limt t =. . j=0X j+1  X jF  limt bt1 = 0.(135). Consequently, there exists some positive integer T0 such that for any t > T0,. (t1  t)  (t1  t)1. .. 47. This together with (133) leads to. . . 1t a3(t1  t),(136). where a3 = (1 + a1). 1 .Given a constant c with c  (1, ). Define (s) := s. 1 , s > 0.Case I. Assume that (t)  c(t1), by (136), we obtain. a13. 1t(t1  t) = (t)(t1  t). t1. tc(t1)ds  t1. tc(s)ds. = c(1  ). 1  2. . 121t1. 121t. .. (137). Let  := 12. 1 and e1 := a1321c(1). Note that  < 0. Observe from (137) that. t  t1  e1 > 0.(138). Case II. Assume that (t) > c(t1), i.e.,. 1t> c. 1t1, which is equivalent to t <. c 1. t1. By the definition of  = 12. 1 < 0, we get that. t > (c 1. )t1,. which implies thatt  t1 > ((c 1. )  1)t1.(139). Notice that c 1. (0, 1). Therefore, we obtain (c 1. )  1 > 0.It follows from (135) that there exist a constant a4 > 0 and a positive integer T1 such that t1 a4 for any t > T1. Consequently, we can deduce that ((c 1. )  1)t1 > ((c 1. )  1)a4. Thistaken together with (139) leads tot  t1 > e2 > 0,(140). where e2 := ((c 1. )  1)a4.Let e := min{e1, e2}. Combining (138) with (140), we obtain t  t1  e. By a similarargument as [70, Theorem 2(iii)], we get that t et2 , where t > 2 max{T0, T1}. Then we canconclude thatX t  XF  t  wt1 = wt 1. 21 ,. where w := ( e. 2) 1. 21 .. References. [1] M. Ahn, J.-S. Pang, and J. Xin. Difference-of-convex learning: Directional stationarity, optimal-ity, and sparsity. SIAM J. Optim., 27(3):16371665, 2017.. [2] H. Attouch and J. Bolte. On the convergence of the proximal algorithm for nonsmooth functionsinvolving analytic features. Math. Program., 116(1-2):516, 2009.. [3] H. Attouch, J. Bolte, and B. F. Svaiter. Convergence of descent methods for semi-algebraic andtame problems: proximal algorithms, forwardbackward splitting, and regularized Gauss-Seidelmethods. Math. Program., 137(1-2):91129, 2013.. 48. [4] A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. InProceedings of the 27th International Conference on Neural Information Processing Systems,pages 15561564, Cambridge, MA, USA, 2014.. [5] A. Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics,Philadelphia, 2017.. [6] J. A. Bengua, H. N. Phien, H. D. Tuan, and M. N. Do. Efficient tensor completion for color imageand video recovery: Low-rank tensor train. IEEE Trans. Image Process., 26(5):24662479, 2017.. [7] J. Bolte, S. Sabach, and M. Teboulle. Proximal alternating linearized minimization for nonconvexand nonsmooth problems. Math. Program., 146(1-2):459494, 2014.. [8] C. Broadbent, T. Song, and R. Kuang. Deciphering high-order structures in spatial transcriptomeswith graph-guided Tucker decomposition. Bioinformatics, 40:i529i538, 2024.. [9] E. Cand`es and B. Recht. Exact matrix completion via convex optimization. Found. Comput.Math., 9(6):717772, 2009.. [10] E. J. Cand`es and Y. Plan. Matrix completion with noise. Proc. IEEE, 98(6):925936, 2010.[11] E. J. Cand`es, M. B. Wakin, and S. P. Boyd. Enhancing sparsity by reweighted 1 minimization. J.Fourier Anal. Appl., 14:877905, 2008.. [12] J. D. Carroll and J.-J. Chang. Analysis of individual differences in multidimensional scaling via ann-way generalization of Eckart-Young decomposition. Psychometrika, 35(3):283319, 1970.. [13] C. Chen, K. Batselier, C.-Y. Ko, and N. Wong. A support tensor train machine. In 2019 Interna-tional Joint Conference on Neural Networks, pages 18, 2019.. [14] Y. Chen, Y. Chi, J. Fan, and C. Ma. Spectral methods for data science: A statistical perspective.Found. Trends Mach. Learn., 14(5):566806, 2021.. [15] C. Ding, D. Sun, J. Sun, and K.-C. Toh. Spectral operators of matrices: Semismoothness andcharacterizations of the generalized Jacobian. SIAM J. Optim., 30(1):630659, 2020.. [16] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties.J. Am. Stat. Assoc., 96(456):13481360, 2001.. [17] M. Fazel, T. K. Pong, D. Sun, and P. Tseng. Hankel matrix rank minimization with applicationsto system identification and realization. SIAM J. Matrix Anal. Appl., 34(3):946977, 2013.. [18] S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing. Springer, NewYork, 2013.. [19] S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n-rank tensor recovery via convexoptimization. Inverse Probl., 27(2):025010, 2011.. [20] K. Gao and Z.-H. Huang. Tensor robust principal component analysis via tensor fibered rank andp minimization. SIAM J. Imaging Sci., 16(1):423460, 2023.. [21] R. Glowinski and A. Marroco.Sur lapproximation, par elements finis dordre un, et laresolution, par penalisation-dualite dune classe de probl`emes de dirichlet non lineaires.Re-vue Francaise Dautomatique, Informatique, Recherche Operationnelle. Analyse Numerique,9(R2):4176, 1975.. [22] P. Gong, C. Zhang, Z. Lu, J. Z. Huang, and J. Ye. A general iterative shrinkage and thresholdingalgorithm for non-convex regularized optimization problems. In Proceedings of the 30th Interna-tional Conference on International Conference on Machine Learning, volume 28, pages 3745.JMLR, 2013.. [23] H. Gui, J. Han, and Q. Gu. Towards faster rates and oracle property for low-rank matrix estima-tion. In Proceedings of the 33rd International Conference on Machine Learning, pages 23002309. PMLR, 2016.. [24] P. Hartman.On functions representable as a difference of convex functions.Pac. J. Math.,9(3):707713, 1959.. 49. [25] L. He, X. Kong, P. S. Yu, X. Yang, A. B. Ragin, and Z. Hao. Dusk: A dual structure-preservingkernel for supervised tensor learning with applications to neuroimages. In Proceedings of the2014 SIAM International Conference on Data Mining, pages 127135, 2014.. [26] C. J. Hillar and L.-H. Lim. Most tensor problems are NP-hard. J. ACM, 60(6):45, 2013.[27] R. A. Horn and C. R. Johnson. Topics in Matrix Analysis. Cambridge University Press, Cam-bridge, 1991.. [28] T.-X. Jiang, M. K. Ng, X.-L. Zhao, and T.-Z. Huang. Framelet representation of tensor nuclearnorm for third-order tensor completion. IEEE Trans. Image Process., 29:72337244, 2020.. [29] E. Kernfeld, M. Kilmer, and S. Aeron. Tensortensor products with invertible linear transforms.Linear Algebra Appl., 485:545570, 2015.. [30] M. E. Kilmer, L. Horesh, H. Avron, and E. Newman. Tensor-tensor algebra for optimal represen-tation and compression of multiway data. Proc. Natl. Acad. Sci., 118(28):e2015851118, 2021.. [31] M. E. Kilmer and C. D. Martin. Factorization strategies for third-order tensors. Linear AlgebraAppl., 435(3):641658, 2011.. [32] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Rev., 51(3):455500,2009.. [33] H. A. Le Thi, T. Pham Dinh, H. M. Le, and X. T. Vo. DC approximation approaches for sparseoptimization. Eur. J. Oper. Res., 244(1):2646, 2015.. [34] A. S. Lewis and H. S. Sendov. Nonsmooth analysis of singular values. part I: Theory. Set-ValuedAnal., 13(3):213241, 2005.. [35] B.-Z. Li, X.-L. Zhao, T.-Y. Ji, X.-J. Zhang, and T.-Z. Huang. Nonlinear transform induced tensornuclear norm for tensor completion. J. Sci. Comput., 92(3):83, 2022.. [36] G. Li and T. Pong. Calculus of the exponent of kurdyka-ojasiewicz inequality and its applicationsto linear convergence of first-order methods. Found. Comput. Math., 18(5):11991232, 2018.. [37] H. Lian. Learning rate for convex support tensor machines. IEEE Trans. Neural Netw. Learn.Syst., 32(8):37553760, 2021.. [38] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion for estimating missing values invisual data. IEEE Trans. Pattern Anal. Mach. Intell., 35(1):208220, 2013.. [39] P.-L. Loh and M. J. Wainwright. High-dimensional regression with noisy and missing data: Prov-able guarantees with nonconvexity. Ann. Stat., 40(3):16371664, 2012.. [40] P.-L. Loh and M. J. Wainwright. Regularized M-estimators with nonconvexity: Statistical andalgorithmic theory for local optima. J. Mach. Learn. Res., 16(19):559616, 2015.. [41] C. Lu, J. Feng, Y. Chen, W. Liu, Z. Lin, and S. Yan. Tensor robust principal component analysiswith a new tensor nuclear norm. IEEE Trans. Pattern Anal. Mach. Intell., 42(4):925938, 2020.. [42] C. D. Martin, R. Shafer, and B. LaRue. An order-p tensor factorization with applications inimaging. SIAM J. Sci. Comput., 35(1):A474A490, 2013.. [43] W. Miao. Matrix Completion Models with Fixed Basis Coefficients and Rank Regularized Prob-lems with Hard Constraints. PhD thesis, National University of Singapore, 2013.. [44] C. Mu, B. Huang, J. Wright, and D. Goldfarb. Square deal: Lower bounds and improved re-laxations for tensor recovery. In Proceedings of the 31st International Conference on MachineLearning, pages 7381. PMLR, 2014.. [45] G. S. Mudholkar, M. Freimer, and P. Subbaiah. An extension of Holders inequality. J. Math.Anal. Appl., 102(2):435441, 1984.. [46] S. N. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. Stat. Sci., 27(4):538557,2012.. [47] I. V. Oseledets. Tensor-train decomposition. SIAM J. Sci. Comput., 33(5):22952317, 2011.. 50. [48] Y. Panagakis, J. Kossaifi, G. G. Chrysos, J. Oldfield, M. A. Nicolaou, A. Anandkumar, andS. Zafeiriou. Tensor methods in computer vision and deep learning. Proc. IEEE, 109(5):863890, 2021.. [49] W. Qin, H. Wang, F. Zhang, J. Wang, X. Luo, and T. Huang. Low-rank high-order tensor comple-tion with applications in visual data. IEEE Trans. Image Process., 31:24332448, 2022.. [50] D. Qiu, M. Bai, M. K. Ng, and X. Zhang. Nonlocal robust tensor recovery with nonconvexregularization. Inverse Probl., 37(3):035001, 2021.. [51] D. Qiu, M. Bai, M. K. Ng, and X. Zhang. Robust low transformed multi-rank tensor methods forimage alignment. J. Sci. Comput., 87(1):24, 2021.. [52] D. Qiu, B. Yang, and X. Zhang. Robust tensor completion via dictionary learning and generalizednonconvex regularization for visual data recovery. IEEE Trans. Circuits Syst. Video Technol.,2024.. [53] Y. Qiu, G. Zhou, Q. Zhao, and S. Xie. Noisy tensor completion via low-rank tensor ring. IEEETrans. Neural Netw. Learning Syst., 35(1):11271141, 2024.. [54] B. Recht, M. Fazel, and P. A. Parrilo.Guaranteed minimum-rank solutions of linear matrixequations via nuclear norm minimization. SIAM Rev., 52(3):471501, 2010.. [55] O. Rivasplata. Subgaussian random variables: An expository note. Technical report, https://www.homepages.ucl.ac.uk/ucabriv/pubs/note/subgaussians.pdf, 2012.. [56] R. T. Rockafellar and R. J. B. Wets. Variational Analysis. Springer, Berlin, 2009.[57] B. Romera-Paredes and M. Pontil. A new convex relaxation for tensor completion. In Procedingsof Neural Information Processing Systems, volume 26, pages 29672975, 2013.. [58] O. Semerci, N. Hao, M. E. Kilmer, and E. L. Miller. Tensor-based formulation and nuclear normregularization for multienergy computed tomography. IEEE Trans. Image Process., 23(4):16781693, 2014.. [59] G. Song, M. K. Ng, and X. Zhang. Robust tensor completion using transformed tensor singularvalue decomposition. Numer. Linear Algebra Appl., 27(3):e2299, 2020.. [60] G.-J. Song, M. K. Ng, and X. Zhang. Tensor completion by multi-rank via unitary transformation.Appl. Comput. Harmon. Anal., 65:348373, 2023.. [61] X. Tan, Y. Zhang, S. Tang, J. Shao, F. Wu, and Y. Zhuang. Logistic tensor regression for clas-sification. In International Conference on Intelligent Science and Intelligent Data Engineering,pages 573581. Springer, 2013.. [62] P. Tang, C. Wang, D. Sun, and K.-C. Toh.A sparse semismooth newton based proximalmajorization-minimization algorithm for nonconvex square-root-loss regression problems.J.Mach. Learn. Res., 21(226):138, 2020.. [63] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279311, 1966.. [64] M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Uni-versity Press, Cambridge, 2019.. [65] H. Wang and N. Ahuja. A tensor approximation approach to dimensionality reduction. Inter. J.Comput. Vis., 76:217229, 2008.. [66] H. Wang, F. Zhang, J. Wang, T. Huang, J. Huang, and X. Liu. Generalized nonconvex approachfor low-tubal-rank tensor recovery. IEEE Trans. Neural Netw. Learn. Syst., 33(8):33053319,2022.. [67] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli. Image quality assessment: From error visibilityto structural similarity. IEEE Trans. Image Process., 13(4):600612, Apr. 2004.. [68] B. Wen, X. Chen, and T. K. Pong. A proximal difference-of-convex algorithm with extrapolation.Comput. Optim. Appl., 69(2):297324, 2017.. 51. [69] K. Wimalawarne, R. Tomioka, and M. Sugiyama.Theoretical and experimental analyses oftensor-based regression and classification. Neural Comput., 28(4):686715, 2016.. [70] S. Xia, D. Qiu, and X. Zhang. Tensor factorization via transformed tensor-tensor product forimage alignment. Numer. Algorithms, 95(3):12511289, 2024.. [71] Y. Xu, R. Hao, W. Yin, and Z. Su. Parallel matrix factorization for low-rank tensor completion.Inverse Probl. Imaging, 9(2):601624, 2015.. [72] Q. Yao, Y. Wang, B. Han, and J. T. Kwok. Low-rank tensor learning with nonconvex overlappednuclear norm regularization. J. Mach. Learn. Res., 23(136):160, 2022.. [73] M. Yin, D. Zeng, J. Gao, Z. Wu, and S. Xie. Robust multinomial logistic regression based onRPCA. IEEE J. Sel. Topics Signal Process., 12(6):11441154, 2018.. [74] L. Yuan, C. Li, D. Mandic, J. Cao, and Q. Zhao. Tensor ring decomposition with rank minimiza-tion on latent space: An efficient approach for tensor completion. In Proceedings of the AAAIConference on Artificial Intelligence, volume 33, pages 91519158, 2019.. [75] C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Stat.,38(2):894942, 2010.. [76] X. Zhang. A nonconvex relaxation approach to low-rank tensor completion. IEEE Trans. NeuralNetw. Learn. Syst., 30(6):16591671, 2019.. [77] X. Zhang and M. K. Ng. A corrected tensor nuclear norm minimization method for noisy low-ranktensor completion. SIAM J. Imaging Sci., 12(2):12311273, 2019.. [78] X. Zhang and M. K. Ng. Low rank tensor completion with Poisson observations. IEEE Trans.Pattern Anal. Mach. Intell., 44(8):42394251, 2022.. [79] X. Zhang and M. K. Ng. Sparse nonnegative tensor factorization and completion with noisyobservations. IEEE Trans. Inf. Theory, 68(4):25512572, 2022.. [80] Q. Zhao, G. Zhou, S. Xie, L. Zhang, and A. Cichocki.Tensor ring decomposition.arXiv:1606.05535, 2016.. [81] X. Zhao, M. Bai, D. Sun, and L. Zheng. Robust tensor completion: Equivalent surrogates, errorbounds, and algorithms. SIAM J. Imaging Sci., 15(2):625669, 2022.. [82] Y.-B. Zheng, T.-Z. Huang, X.-L. Zhao, Q. Zhao, and T.-X. Jiang. Fully-connected tensor networkdecomposition and its application to higher-order tensor completion. In Proceedings of the AAAIConference on Artificial Intelligence, volume 35, pages 1107111078, 2021.. 52", "2410.18404v1.Enhancing_Feature_Specific_Data_Protection_via_Bayesian_Coordinate_Differential_Privacy.pdf": "Enhancing Feature-Specific Data Protection via. Bayesian Coordinate Differential Privacy *. Maryam Aliakbarpour Syomantak Chaudhuri Thomas A. Courtade. Alireza Fallah Michael I. Jordan. October 23, 2024. Abstract. Local Differential Privacy (LDP) offers strong privacy guarantees without requiring users totrust external parties. However, LDP applies uniform protection to all data features, includingless sensitive ones, which degrades performance of downstream tasks. To overcome thislimitation, we propose a Bayesian framework, Bayesian Coordinate Differential Privacy (BCDP),that enables feature-specific privacy quantification. This more nuanced approach complementsLDP by adjusting privacy protection according to the sensitivity of each feature, enablingimproved performance of downstream tasks without compromising privacy. We characterizethe properties of BCDP and articulate its connections with standard non-Bayesian privacyframeworks. We further apply our BCDP framework to the problems of private mean estimationand ordinary least-squares regression. The BCDP-based approach obtains improved accuracycompared to a purely LDP-based approach, without compromising on privacy.. 1Introduction. The rapid expansion of machine learning applications across various sectors has fueled an un-precedented demand for data collection. With the increase in data accumulation, user privacyconcerns also intensify. Differential privacy (DP) has emerged as a prominent framework thatenables algorithms to utilize data while preserving the privacy of individuals who provide itWarner [1965], Evfimievski et al. [2003], Dwork et al. [2006, 2014], Vadhan [2017], Desfontaines[2020]. A variant of DP, local differential privacy (LDP), suitable for distributed settings whereusers do not trust any central authority has been extensively studied [Warner, 1965, Evfimievski. et al., 2003, Beimel et al., 2008, Kasiviswanathan et al., 2011, Chan et al., 2012]. In this framework,. *The authors are listed in alphabetical order.Department of Computer Science & Ken Kennedy Institute, Rice UniversityDepartment of Electrical Engineering and Computer Sciences, University of California, BerkeleyDepartment of Electrical Engineering and Computer Sciences & Department of Statistics, University of California,Berkeley; INRIA, Paris. 1. arXiv:2410.18404v1  [cs.LG]  24 Oct 2024. each user shares an obfuscated version of their datapoint, meaningful in aggregate to the centralserver yet concealing individual details. Leading companies, including Google [Erlingsson et al.,2014], Microsoft [Ding et al., 2017], and Apple [Differential Privacy Team, Apple, 2017, Thakurta. et al., 2017], have employed this approach for data collection.. In the LDP framework, a privacy parameter must be settypically by the data collectorthat. determines the level of protection for user information. This decision is particularly challenging in. high-dimensional settings, where user data includes multiple attributes such as name, date of birth,and social security number, each with differing levels of sensitivity. One common strategy is to. allow the most sensitive data feature to dictate the level of privacy, providing the highest level of. protection. Alternatively, separate private mechanisms can be applied to each feature according to. its required privacy level. However, this approach fails when there is correlation between features.. For instance, setting a lower level of privacy for a feature that is consistently aligned with a more. sensitive one risks compromising the privacy of the sensitive feature.This complexity raises a compelling question: Can we tailor privacy protection to specificdata features if the correlation among data coordinates is controlled, without defaulting to theprivacy level of the most sensitive coordinate? In other words, can we leverage users lower. sensitivity preferences for certain features to reduce the error in our inference algorithms, provided. the correlation with sensitive coordinates is weak?. Our contributions:Our main contribution is addressing the challenge of feature-specific dataprotection from a Bayesian perspective. We present a novel Bayesian privacy framework, which. we refer to as Bayesian Coordinate Differential Privacy (BCDP). BCDP complements LDP and allows a. tailored privacy demand per feature. In essence, BCDP ensures that the odds of inferring a specific. feature (also referred to as a coordinate) remain nearly unchanged before and after observing theoutcome of the private mechanism. The frameworks parameters control the extent to which the. odds remain unchanged, enabling varying levels of privacy per feature. As part of developing thisnew framework, we explore its formal relationship with other related DP notions and examine. standard DP properties, such as post-processing and composition.. To demonstrate the efficacy of our framework, we study two fundamental problems in machine. learning under a baseline level of LDP privacy, coupled with more stringent per-coordinate BCDP. constraints. First, we address multivariate mean estimation. Our proposed algorithm satisfies BCDP. constraints while outperforming a standard LDP algorithm, where the privacy level is determinedby the most sensitive coordinate. Our experimental results confirm this advantage. Second, we. present an algorithm for ordinary least squares regression under both LDP and BCDP constraints.. A key technical component of our algorithm for these two problems is a mechanism that buildson any off-the-shelf LDP mechanism and can be applied to various problems. Our algorithm. involves making parallel queries to the LDP mechanism with a series of carefully selected privacy. parameters where the i-th query omits the i  1 most sensitive coordinates. The outputs are then. aggregated to produce the most accurate estimator for each coordinate.. 2. Bayesian Feature-Specific Protection:We now give a high-level discussion of privacy in the. Bayesian framework to motivate our approach. Formal definitions are in Section 2. Consider a set. of users, each possessing a high-dimensional data point x with varying privacy protection needsacross different features. For instance, x may contain genetic markers for diseases, and the i-thcoordinate xi could reveal Alzheimers markers that the user wishes to keep strictly confidential.In a local privacy setting, users apply a locally private mechanism M to obtain obfuscated data. M(x), for public aggregation and analysis. Our focus is on understanding what can be inferred. about x, especially the sensitive feature xi, from the obfuscated data.Now, imagine an adversary placing a bet on a probabilistic event related to x. Without extrainformation, their chances of guessing correctly are based on prior knowledge of the population.. However, access to M(x) could prove advantageous for inference. Privacy in the Bayesian modelis measured by how much M(x) improves the adversarys chances. In other words, the privacy. parameter limits how much their odds can shift.. The standard Bayesian interpretation of LDP ensures uniform privacy across all data features.In contrast, our framework BCDP enhances LDP by enabling feature-specific privacy controls.That is, BCDP restricts adversarial inference of events concerning individual coordinates. For. example, BCDP ensures that the advantage an adversary gains from observing M(x) when betting. on whether a person has the Alzheimers trait (i.e., xi = 1) is smaller than what would be achieved. under a less stringent LDP constraint.We remark that BCDP does not replace LDP, since it lacks global data protection. Also, corre-lation plays a crucial role in BCDPs effectiveness: if a sensitive feature is highly correlated with. less sensitive features, increasing protection for the sensitive feature imposes stricter protection for. other features as well. As the correlation between features increases, BCDPs guarantees converge. to the uniform guarantees of LDP.. Related Work:Statistical inference with LDP guarantees has been extensively studied. For. comprehensive surveys see Xiong et al. [2020], Yang et al. [2023]. Below, we discuss the results that. are most relevant to our setup.Several papers consider the uneven protection of privacy across features. For instance, Ghaziet al. [2022] and Acharya et al. [2020] focus on settings where the privacy parameters vary acrossdifferent neighboring datasets. Other models arise from altering the concept of neighboring. datasets [Kifer and Machanavajjhala, 2014, He et al., 2014], or they emphasize indistinguishabilityguarantees within a redefined metric space for the datasets or data points Chatzikokolakis et al.[2013], Alvim et al. [2018], Imola et al. [2022], Andres et al. [2013]. Another relevant notion forclassification tasks is Label-DP which considers labels of training data as sensitive, but not the. features themselves [Chaudhuri and Hsu, 2011, Beimel et al., 2013, Wang and Xu, 2019, Ghazi et al.,2021]. A recent generalization of Label-DP, proposed by Mahloujifar et al. [2023], is Feature-DP,which allows specific features of the data to be disclosed by the algorithm while ensuring theprivacy of the remaining information. A line of work, e.g., Kifer and Machanavajjhala [2011],. 3. Kenthapadi et al. [2013], studies attribute DP, which limits changes in the output distribution of the. privacy mechanism when a single coordinate of the data is modified.A common shortcoming in the aforementioned approaches is that the heterogeneous privacyprotection across features, without accounting for their correlations, can lead to unaddressedprivacy leakagean issue our framework is specifically designed to mitigate. Another key dis-. tinction of our work is the integration of the BCDP framework with the LDP framework, enabling. simultaneous protection at both the feature level and the global level.A Bayesian approach to Central-DP is considered in Triastcyn and Faltings [2020] where thenotation of neighboring dataset are taken over pairs that are drawn from the same distribution.. This is similar in spirit to our framework but we operate in the Local-DP regime so we work with a. prior instead. Xiao and Devadas [2023] consider a Bayesian approach to privacy where the goal isto prevent an observer from being able to nearly guess the input to a mechanism under a givennotion of distance and error probability. Another Bayesian interpretation of DP is presented in. Kasiviswanathan and Smith [2014] and show that DP ensures that the posterior distribution afterobserving the output of a DP algorithm is close to the prior in Total-Variation distance. None of. these results explicitly focus on varying privacy features across coordinates.Private mean estimation and private least-squares regression are well studied problems inliterature in both central [Biswas et al., 2020, Karwa and Vadhan, 2017, Kifer et al., 2012, Vu andSlavkovic, 2009] and local [Asi et al., 2022, Feldman and Talwar, 2021, Zheng et al., 2017] DPregimes. We depart from these works as we consider a BCDP constraint in addition to an LDP. constraint.. Organization:We present a formal treatment of our framework in Section 2. Some key properties. of the framework are presented in Section 3. We consider the problem of mean estimation in Section 4. and ordinary least-squares regression in Section 5.. 2Problem Formulation. Let X denote the data domain represented as the Cartesian product X :=dj=1 Xj. One can consider. this as a general decomposition where each of d canonical data coordinates can be categorical or areal value, or a combination of both, as the setting dictates. For a vector x  X, we let xi denotethe i-th coordinate, and xi the vector with (d  1) coordinates obtained by removing the i-th. coordinate xi from x. A private mechanism M is a randomized algorithm that maps x to M(x) in. an output space Y.. Let (X, FX ) and (Y, FY) be measurable spaces. Let each coordinate space Xi be equipped with. -algebra FXi, and take FX equal to the product -algebradi=1 FXi. A randomized mechanism. M from X to Y can be represented by a Markov kernel  : (X, FX )  (Y, FY) where for an input. x  X, the output M(x) is a Y-valued random variable with law x().. 4. Local differential privacyWe first revisit the definition of Local Differential Privacy (LDP).. Definition 1 (Local DP [Kasiviswanathan et al., 2011]). A mechanism M is -LDP if, for all R  FYand all x, x  X, we have:. x(R)  ex(R).(1). With the understanding that M(x)  x(), Definition 1 can be rewritten in the more familiar. form Pr{M(x)  R}  ePr{M(x)  R} x, x  X, R  FY.. Next, we focus on a Bayesian interpretation of this setting. Suppose we have an underlying datadistribution . That is, we equip (X, FX ) with the probability measure  which we call the prior.. This induces a probability space (X  Y, FX  FY, P) where the measure P is characterized via. P(S  R) =. Sx(R)d(x), S  FX , R  FY.(2). Note that the mechanism M defining  is precisely represented on this space by the random variable. M : (x, y)  X  Y  y. By a slight abuse of notation, we continue to denote the (randomized)output of the mechanism by M(x). We first define the sets on X which have positive probability. under. F+X = {S  FX |(S) > 0}.(3). For S  F+X and R  FY, note that. P{M(x)  R|x  S} = P(S  R). (S).(4). Bayesian differential privacy:We now introduce a Bayesian formulation of the LDP definitionwhich involves both the mechanism M and the prior . This can be seen as the local version ofthe definitions proposed in earlier works, e.g., Kifer and Machanavajjhala [2014], Triastcyn and. Faltings [2020].. Definition 2 (Bayesian DP). The pair (, M) is -BDP if for all R  FY and all S, S  F+X , we have:. P{M(x)  R|x  S}  eP{M(x)  R|x  S}.(5). We show in Proposition 1 that -LDP and -BDP can be viewed as equivalent, modulo some. technicalities.. Proposition 1. If M is -LDP then (, M) is -BDP. Conversely, if (, M) is -BDP, then (assuming Y is. a Polish space) there exists a mechanism M which is -LDP such that P{M(x) = M(x)} = 1.. The reverse direction of the equivalence above relies on Y being a Polish space, which should. capture all settings of practical interest. The proof can be found in Appendix A.1.. 5. As discussed in Kifer and Machanavajjhala [2014], we can interpret the BDP definition in terms. of odds ratios. The prior odds of two disjoint events S and S is given by (S)/(S). Now, for a set. R  FY with positive measure, the posterior odds, after observing mechanism output M(x)  R, is. equal toP{x  S|M(x)  R}P{x  S|M(x)  R}.(6). One can easily verify that the BDP definition implies that the ratio of posterior and prior odds, alsoknown as the odds ratio, is in the interval [e, e]. In other words, the BDP definition limits how. much the output of the mechanism changes the odds, which in turn limits our ability to distinguish. whether the data x was in S or S.. Coordinate differential privacy:We wish to measure the privacy loss of each coordinate incurred. by a mechanism M. Initially, one might hypothesize that by altering the definition of LDP to reflectchanges in specific coordinates, we could achieve privacy protection for certain features of theusers data. More precisely, consider the following definition of Coordinate DP (CDP)1 where c. represents a vector of the desired levels of privacy for each coordinate.. Definition 3 (Coordinate DP). Let c  Rd0. A mechanism M is c-CDP if, for all R  FY and all. x, x  X, we have the implication:. xi = xi  x(R)  ecix(R).(7). A special case of CDP, where all cis are equal, is referred to as attribute DP in the literature. Kifer and Machanavajjhala [2011], Kenthapadi et al. [2013], Ghazi et al. [2022]. Additionally, CDP. itself can be viewed as a special case of Partial DP, introduced in Ghazi et al. [2022].. The drawback of this definition is that it does not account for potential correlation among thedata coordinates and, therefore, does not provide the type of guarantee we hope for regarding. sensitive parts of the data. In other words, in c-CDP, stating that the privacy level of coordinate i is. ci could be misleading. For instance, having ci = 0 does not necessarily ensure that no information. is leaked about xi, as the other (potentially less-well protected) coordinates xi may be correlated. with it.. Our formulation: Bayesian coordinate differential privacy:In order to capture the correlationamong the data coordinates, we adopt a Bayesian interpretation of differential privacy. As seenin Proposition 1, BDP and LDP are closely related and this serves as the main motivation for our. definition.. For i  [d], define. F+Xi = {Si  FXi|{xi  Si} > 0}.(8). 1The acronym CDP is not to be confused with Concentrated DP, which is not considered in this work.. 6. For Si  F+Xi, we have. P{M(x)  R|xi  Si} = P((Si  Xi)  R). (Si  Xi),(9). where Xi =j[d],j=i Xj. Thus, similar to Definition 2, we have a natural version of privacy for. each coordinate which we shall refer to as Bayesian Coordinate DP (BCDP).. Definition 4 (Bayesian Coordinate DP). Let   Rd0. A pair (, M) is -BCDP if, for each i  [d],. and for all R  FY, Si, Si  F+Xi, we have:. P{M(x)  R|xi  Si}  eiP{M(x)  R|xi  Si}.(10). Unlike the Coordinate-DP definition, this definition accounts for potential correlation between. coordinates through incorporation of the prior .It is worth noting that, similar to the BDP definition, we can interpret the BCDP definitionin terms of the odds ratio, with the difference that we consider the two events {xi  Si} and. {xi  Si}. In other words, having a small i guarantees that the mechanisms output does not. provide much useful information for discriminating between the possibilities that the underlyingdata has coordinate xi in Si or Si. This aligns with our goal of proposing a new formulation thatcharacterizes how much information is revealed about each particular coordinate. We further. discuss this interpretation in the next section.. 3Properties of BCDP. In this section, we discuss a number of properties following from the BCDP definition. Figure 1compiles some of the results from Section 2 and Section 3 on how the different notions of LocalDP (LDP), Coordinate-DP (CDP), Bayesian DP (BDP), and Bayesian Coordinate-DP (BCDP) are. interrelated.. 3.1BCDP through the Lens of Hypothesis Testing. In this subsection, we present an interpretation of what the privacy vector  quantifies in -BCDP.. We begin by revisiting the connection between hypothesis testing and the definition of differentialprivacy, and demonstrate how this connection extends to the -BCDP definition. For an -LDPmechanism M represented by Markov kernel , suppose an adversary observes a realization of. the output Y and is tasked with determining whether the input was x or x. This scenario can be. formulated as the following binary hypothesis problem:. H0 : Y  x(),. H1 : Y  x().. Let the rejection region be R  Y. Denote the type I error (false alarm) rate as (R, M, x, x). 7. and the type II error (missed detection) rate as (R, M, x, x). We restate a known result below. [Kairouz et al., 2015, Wasserman and Zhou, 2010].. Proposition 2. A mechanism M() is -LDP iff the following condition holds for any x, x  X and any. R  FY:. e(R, M, x, x) + (R, M, x, x)  1.(11). We note that the criterion (11) can be equivalently replaced by the criterion. (R, M, x, x) + e(R, M, x, x)  1.. As noted in Kairouz et al. [2015], this operational perspective shows that it is impossible toforce type I and type II error rates to simultaneously vanish for a DP algorithm, and the reverse. implication also holds.. For BCDP:Now, consider a -BCDP mechanism M. Suppose an adversary has knowledge ofthe prior  and again observes the algorithms output Y . For any i  [d], the adversary aims to. distinguish whether the output was generated from data where the i-th coordinate belongs to the. set Si or Si. In other words, we consider the following binary hypothesis testing problem:. Hi0 : Y  P{  |xi  Si},. Hi1 : Y  P{  |xi  Si}.. Let the rejection region for the i-th hypothesis test be Ri  FY. For the i-th test, denote the type I. and type II error rates as (Ri, M, Si, Si) and (Ri, M, Si, Si). Then, the following result holds:. Theorem 1. A pair (, M) is -BCDP iff the following condition holds for every coordinate i  [d]:. ei(Ri, M, Si, Si) + (Ri, M, Si, Si)  1,(12). for any Si, Si  F+Xi and Ri  FY.. We note that the criteria (12) can be equivalently replaced by the criteria. (Ri, M, Si, Si) + ei(Ri, M, Si, Si)  1.. The proof of Theorem 1 is presented in Appendix A.2. Thus, the theorem shows that the vector  in. the -BCDP formulation captures the privacy loss for each of the coordinates in the same way  in. -DP captures the privacy loss of the entire data. In contrast, the vector c in c-CDP does not have. such an interpretation of privacy loss across the coordinates.. 8. L-LDPB-BDP. c-CDP-BCDP. L = B. B1c  L1. func. of (c, ). L  i ci. Figure 1: General relation of LDP (Definition 1), BDP (Definition 2), CDP (Definition 3) and BCDP(Definition 4). The condition a  b is to be read as ai  bi i. The implication arrows andthe described transformation of the parameters are to be interpreted as sufficient condition. Forexample, an L-DP mechanism is guaranteed to be c-CDP for c  L1. The function that translatesc-CDP under the prior  to BCDP is presented in the Proposition 4.. 3.2BCDP can be achieved via LDP. We begin by a simple result that follows directly from -BDP definition. If a mechanism is -LDP,. then it is also -BDP. Noticing that -BDP ensures a ratio of e for each coordinate as well in (10), we. see that each coordinate has a privacy level of  as well. Proposition 3 formalizes this observation. and the proof can be found in Appendix A.3.. Proposition 3. (LDP to BCDP) An -LDP mechanism satisfies 1-BCDP.. Thus, a baseline approach to implement -BCDP is to use a (mini i)-LDP mechanism. Never-theless, this would mean that we provide the most conservative privacy guarantee required for. one coordinate to all coordinates. However, as we will show, we can maintain an  > min  for the. overall privacy guarantee while providing the -BCDP guarantee, therefore achieving lower error. for downstream tasks.. 3.3BCDP does not imply LDP. It should be noted that BCDP does not ensure LDP, BDP, or CDP. Example 1 below demonstrates a. mechanism that is BCDP, but not LDP.. Example 1. Consider data x = (x1, x2) where x1, x2 are i.i.d. Bern( 1. 2), and let Y = {0, 1}. Let. M be defined by the table below, with parameters a, b, c  (0, 1]. Mechanism M is not LDP(and hence, neither BDP nor CDP). However, for BCDP, the terms P{M(x) = y|xi = 0} and. P{M(x) = y|xi = 1} have a finite multiplicative ranges in y = 0, 1, for both i = 1, 2. For instance,. if a = b = c = 1/2, the resulting M is (log(2), log(2))-BCDP.. 9. P{M(x) = 1|x}. x1. 01. x2. 0ab. 10c. The takeaway from the above example is that there can be priors and mechanisms such that the. prior-mechanism pair is -BCDP but not -DP for any value of  < . Intuitively, BCDP quantifies. privacy loss for each coordinate. In Example 1, one cannot distinguish much about any particularcoordinate when the output is observed due to the BCDP guarantee. However, one may still beable to make conclusions over the coordinates jointly. For instance, if the mechanism output in. Example 1 is 1, then the input could not have been x = (0, 1).As this result suggests, just ensuring BCDP is not sufficient to guarantee overall privacy. In. other words, the right way to think of BCDP is that it is a tool that allows us to quantify the privacyoffered by an algorithm for each coordinate. The user can demand overall -LDP along with. -BCDP where the value i could be significantly smaller than  for those coordinates that the user. feels more concerned about their privacy.. 3.4Achieving BCDP via CDP. As we discussed earlier, CDP does not imply the BCDP, as it does not take into account the. correlation among the features. Therefore, a natural question arises: could CDP, along with some. bound on the dependence among the features, lead to a bound with respect to the BCDP definition?. The next result provides an answer to this question. We denote the total variation distance between. two distributions P and Q as TV(P, Q).. Proposition 4. (CDP to BCDP) Suppose mechanism M is c-CDP and there exists q1, . . . , qd such that. TV(xi|xiSi, xi|xiSi)  qi  Si, Si  F+Xi.(13). Then:. 1. The mechanism is ni=1 ci-LDP and -BCDP with i = ci + log(1 + qiej=i cj  qi).. 2. In case that the mechanism is -LDP for some   0, then it is -BCDP with i = min{ci + log(1 +. qie  qi), }.. For the proof, see Appendix A.4. To better highlight this result, consider a simple example inwhich that the inputs d coordinates are redundant copies of the same value and we have accessto a c-CDP mechanism M. Then, as Proposition 4 suggests, and given that we have qi = 1 here,this mechanism is -BCDP with i = j cj. This is, of course, intuitive, as all coordinates reveal. 10. information about the same value. It also reinforces that the BCDP is sensitive to correlations,. unlike CDP. The first part of Proposition 4 provides an immediate method for constructing a BCDPmechanism using mechanisms that offer CDP guarantees, such as applying off-the-shelf LDPmechanisms per coordinate. This can also be done without requiring full knowledge of the prior.The second part of Proposition 4 suggests that in cases where we design an algorithm that has atighter LDP guarantee than just ni=1 ci, we can further improve the BCDP bound. One exampleof such a construction is provided in our private mean estimation algorithm in Section 4. Onechallenge with this approach is translating the constraint  to a vector c. In Appendix A.5, we. provide a tractable approach to do so by imposing a linear constraint on c.. 3.5Do Composition and Post-Prossesing Hold for BCDP?. Composition is a widely used and well-known property of LDP. Nonetheless, as the following. example highlights, the composition, as traditionally applied in differential privacy settings, does. not hold in the BCDP setting.. Example 2. Let x  R3 be a Bernoulli vector where coordinates distributions are independent and. each one is Ber(1/2). Consider the mechanism. M(x) =. . [x1  x2, x3]with probability 1. 2[x2, x1  x3]with probability 1. 2,(14). where  denotes the sum in Z2. It is straightforward to verify that M() is in fact 1 = 0-BCDP with. respect to the first coordinate. However, having two independent copies of M() is not BCDP withrespect to the first coordinate. To see this, consider the case where x1 = 1 and, in two copies of. M(x), one adds x1 to x2 and the other adds it to x3. By observing the two copies, we can determine. x1 = 1.. On the other hand, the post-processing property holds for BCDP (see Appendix A.6 for the. proof).. Proposition 5. (Post processing for BCDP) Let M() be a -BCDP mechanism and K : Y  Z be a. measurable function. Then, K  M is also -BCDP.. 4Private Mean Estimation. To demonstrate the practical use of the BCDP framework, we consider the private mean. estimation problem.. Problem Setup:Let there be n users submitting their data to a central server via Local DP. mechanisms. Let user js data be x(j)  [1, 1]d, where x(j) is drawn from prior (j) on [1, 1]d; the. server and users are not assumed to know the priors (which may be different for each individual. 11. Mechanism 1 Proposed locally private mechanism Mmean(x, c). Input: user data x  [1, 1]d and non-decreasing vector c.c0  0For k  [d], set wk = (ckck1)2. (dk+1). for k  [d] do. Y k  MLDP(xk:d, ck  ck1,. d  k + 1)  Rdk+1. end fori  (ik=1 Y ki+1kwk)/(ik=1 wk) for i  [d]. Return. x1x2. . .xd1xd. cd  cd1. Y d1. cd1  cd2. Y d11Y d12.... .... c1  c0. Y 11Y 12. . .Y 1d1Y 1d. Figure 2: The figure illustrates how the mechanism Mmean in Mechanism 1 obtains the estimate .For example, the vector Y k is obtained by taking the vector (xk, xk+1, . . . , xd) and using the LDPchannel MLDP with privacy parameter ck  ck1. i is obtained by taking a linear combination ofthe component of {Y k}k[d] corresponding to xi, i.e., directly below xi in the figure.. user). Our goal is to estimate the empirical mean x = nj=1x(j). n [1, 1]d under -LDP and. -BCDP. We consider l2-norm as our error metric. Without loss of generality, we assume that  is in. non-decreasing order. We make the following assumption regarding the priors.. Assumption 1. For every i  [d  1] and every j  [n], we have. TV((j)xi|xiSi, (j)xi|xiSi)  qSi, Si  F+Xi.(15). Here q can be interpreted as a measure of how much different coordinates are interdependent.. For instance, in the case where coordinates are independent we have q = 0, and in the case wherethey are fully equal, we have q = 1. Note that we do not need (15) to hold for the last coordinate(i = d). While this slight relaxation does not play a role in this section, it will be useful when we. move to the private optimization application.. It is also worth emphasizing that our analysis extends to cases where we have heterogeneousbounds across different coordinates (similar to (13)) and different across users. However, we optfor a universal bound for the sake of simplifying the presentation of results. We assume that theupper bound q is known to the designer of the local channels, which can be either the end user. 12. Mechanism 2 Local DP channel for data in B2(r) proposed by Duchi et al. [2013a]. Input: user data v, Local-DP level   0, and bound r such that v  B2(r).d dimension(v). B  dr e+1. e1( d+1. 2 ). ( d. 2 +1). K  Bern(e. e+1). S  Bern( 1. 2 + v2. 2r ). v  (2S  1) rv. v2. Return Z  Uniform(z  Rd : (2K  1)zT v > 0, z2 = B). themselves or the central server.Our proposed privacy mechanism Mmean (Mechanism 1) builds upon any LDP mechanism. MLDP, capable of handling different dimensional inputs, that can be used for mean estimationas a black-box tool. While Algorithm 1 satisfies the privacy constraints for any black-box LDP. mechanism MLDP, we shall focus on the mechanism presented by Duchi et al. [2013a] for provingerror bounds. The mechanism of Duchi et al. [2013a], which is known to be minimax optimalfor mean estimation, is outlined in Mechanism 2 for completeness. A figure explaining Mmean is. presented in Figure 2.Theorem 2 shows that our proposed mechanism satisfies the desired privacy constraints and. provides a bound on the mean-squared error (MSE). The notation a  b refers to min{a, b}. later in. this section we present a refined result with a more interpretable MSE bound in Corollary 1.. Theorem 2. Suppose Assumption 1 holds and let i := min{i, } for any i  [d]. Then, Mmean, i.e.,. Mechanism 1, with parameters. cd = min. . log(e1 + q  1. q), d. . , c0 = 0,(16). i < d, ci =. . cdif cd  i,. i  log(1 + qecd  q)otherwise,. where   (0, 1] is a free parameter, is -BCDP and -LDP. Moreover, using Mechanism 2 as MLDP, we. obtain the following mean-squared error bound. E. . x   1. n. n. j=1Mmean(x(j), c). 2. 2. x(1), . . . , x(n). . 1. n. d. i=1. 1. ik=1(ckck1)2. dk+1. . d,(17). where () denotes projection into [1, 1]d and the expectation is taken over the randomness in Mmean().. The proof is presented in Appendix B.2. Here, we provide a proof sketch for the privacyguarantee. We first establish that Mmean is c-CDP and cd-LDP. The latter implies that Mmean also. satisfies cd1-BCDP. Thus, if i  cd, we have already shown that Mmean is i-BCDP with respect to. 13. the i-th coordinate. For the case where cd > i, we use Proposition 4, which essentially implies that. Mmean is ci + log(1 + qecd  q)-BCDP with respect to the i-th coordinate. Substituting the value of cifrom the statement of Theorem 2 gives us the desired i-BCDP guarantee for coordinate i. Finally,. the choice of cd in (16) ensures that the proposed cis are all non-negative and valid.We next make a few remarks regarding this result. First, note that while this bound appliesto empirical mean estimation, when the data points {x(j)}nj=1 are independent and identicallydistributed, the result can also be used to derive an out-of-sample error bound for the statisticalprivate mean estimation problem. In this case, an additional d. n term would be added to the error. bound. Secondly, note that in the special case where there is no coordinate-level privacy requirement(i.e., i = ), setting  = 1 implies c1 = . . . = cd = , which yields the error bound O(d2/(n2)).This matches the -LDP lower bound provided in Duchi et al. [2013a]. Lastly, we would like to. discuss the role of parameter . Note that when the correlation parameter q is sufficiently high orthe most sensitive privacy requirement 1 is low enough such that cd is determined by the firstterm, log((e1 + q  1)/q), in (16), one can verify that c1 = (1  )1. As  increases, c1 decreases,. leading to a worse estimate for the first coordinate. However, cd increases, indicating that, for the. less-sensitive coordinates, we are increasing the privacy parameter, potentially resulting in lower. estimation error for those coordinates. In other words, the parameter  allows us to control how we. allocate the privacy budget of the most sensitive coordinate, 1, between the estimation of the first. coordinate and the privacy cost imposed by its correlation with other coordinates.We next present a corollary that studies a special case where the coordinates are divided into. more sensitive and less sensitive groups. In this case, users requests a general -LDP guarantee for. all coordinates and a -BCDP guarantee specifically for the more sensitive coordinates. Under thisscenario, we further simplify the error bound which allows us to better highlight implications of. Theorem 2.. Corollary 1. Suppose we are under the premise of Theorem 2, with i =  for 1  i  k and i =  > 2. for d  i > k. Then, by choosing  = 0.5, the mean-squared error upper bound in (17) simplifies to. O(1) 1. n dk. 2 + (d  k)2. 2. ,(18). for the case q  e/21. e1 , and to. O(1) 1. n. dk2 +(d  k)2. dk. d 2 + (cd  /2)2. . ,(19). otherwise, with cd = log( e/2+q1. q) decreasing from  to /2 as q increases from (e/2  1)/(e  1) to 1.. Under the special structure of  and  in Corollary 1, the MSE can be thought of as sum ofMSE of the more sensitive and the less sensitive coordinates. For low levels of correlation, the. MSE of less private coordinates behave like the MSE of an -LDP mechanism on d  k dimensionalvector but the MSE of the more sensitive part depends on the dimension d of the whole vector, not. 14. just the sensitive part, showing how the more sensitive part of the data affects the whole vector.As the correlation increases q  1, we have cd  /2 and the MSE matches that of a min -LDP. mechanism. A supporting experiment is deferred to Appendix B.1.. 5Private Least-Squares Regression. We focus on how the BCDP framework can be employed on least-squares problems. Considera setting in which n datapoints are distributed among n users, where x(i) = [z(i) l(i)]  Rd. represents the data of user i, with z(i) as the feature vector for the i-th data point and l(i) as its label.. Our goal is to solve the following empirical risk minimization problem privately over the data of n. users:. min f() := 1. n. n. i=1(, x(i)),(20). where (, x(i)) := 1. 2(z(i)  l(i))2 and   Rd1 is a compact convex set. We denote the solution. of (20) by f, achieved at some point .The customary approach in private convex optimization is to use a gradient-based method,. perturbing the gradient at each iteration to satisfy the privacy requirements. However, this approachis not suitable for cases in which coordinate-based privacy guarantees are needed. The difficultyis that each coordinate of the gradient is typically influenced by all coordinates of the data. As a. result, and it becomes challenging to distinguish the level of privacy offered to different coordinatesof the data. Instead, we first perturb the users data locally and then compute the gradient at the. perturbed point to update the model at the server.However, computing the gradient based on perturbed data introduces its own challenges. In. particular, the gradient of (, x) with x = [zl], given by. (, x) = zz  lz,(21). is not a linear function of the data. As a result, although the privacy mechanism outputs anunbiased estimate of the true data, this non-linearity introduces an error in the gradient with anon-zero mean, preventing the algorithm from converging to the optimal solution. To overcome. this challenge, we observe that the aforementioned non-linearity in the data arises from the productterms zz and lz. Therefore, if we create two private copies of the data and replace each term of. the product with one of the copies, we obtain a stochastic unbiased estimate of the gradient at the. server. The catch, of course, is that we need to divide our privacy budget in half.. A summary of the algorithm is presented in Algorithm 3. There, OPT(g, ) refers to any convexoptimization algorithm of choice, e.g., gradient descent, that finds the minimizer of the function. g() over the set  and up to an error of . We make the assumption that the data is bounded.. Assumption 2. x(i)  [1, 1]d for all i  [n].. We also need Assumption 1, which bounds the dependence of each coordinate on the other. 15. Algorithm 3 BCDP and LDP least-squares regression. Input: Users data {x(i)}ni=1 and vector c. for i  [n] do. //user i sends locally perturbed data to the serverx(i,1)  Mmean(x(i), c/2)x(i,2)  Mmean(x(i), c/2). end for//server optimizes with the perturbed dataf() :=12nni=1z(i,1)z(i,2)  2l(i,1)z(i,2). OPT( f, 1. n). Return. coordinates of the data. Recall that the last coordinate is exempt from this assumption, which iswhy we position the label as the last coordinate of the data. It is not reasonable to assume the. feature vector is weakly correlated with the label, as that would violate the premise of regression.. We next state the following result on Algorithm 3.. Theorem 3. Suppose Assumptions 1 and 2 hold. Then, Algorithm 3, with c set similar to Theorem 2, is. -BCDP and -LDP. Moreover, using Mechanism 2 as MLDP, we obtain the following error rate. Ef()  fx(1), . . . , x(n) max(2 + )log dn (r2 + rd)  d,(22). with r2 = di=1 1/(ik=1(ckck1)2. dk+1).. The proof of this theorem can be found in Appendix C.1. It is worth noting that r2 appears in. the mean-squared error bound in Theorem 2. That said, if we consider special cases similar to the. setting in Corollary 1, the term r2 would simplify in a similar manner.. We conclude this section by noting that, while we focus on linear regression here, our approachcan provide an unbiased gradient and guarantee privacy in any setting where the loss function. (, x) has a quadratic (or even low-degree polynomial) dependence on x, such as neural networks. with 2 loss. However, the optimization error in such cases may require further considerations, as. non-convex loss functions could arise.. 6Acknowledgements. This work was done in part while M.A. was a research fellow at the Simons Institute for theTheory of Computing. A.F. and M.J. acknowledge support from the European Research Council(ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the. authors only and do not necessarily reflect those of the European Union or the European ResearchCouncil Executive Agency. Neither the European Union nor the granting authority can be held. 16. responsible for them. S.C. acknowledges support from AI Policy Hub, U.C. Berkeley; S.C. and T.C.. acknowledge support from NSF CCF-1750430 and CCF-2007669.. References. J. Acharya, K. Bonawitz, P. Kairouz, D. Ramage, and Z. Sun. Context aware local differential. privacy. In Proceedings of the 37th International Conference on Machine Learning, pages 5262. PMLR,. 2020.. M. Alvim, K. Chatzikokolakis, C. Palamidessi, and A. Pazii. Invited paper: Local differentialprivacy on metric spaces: Optimizing the trade-off with utility. In 2018 IEEE 31st Computer. Security Foundations Symposium (CSF), pages 262267, 2018.. M. E. Andres, N. E. Bordenabe, K. Chatzikokolakis, and C. Palamidessi. Geo-indistinguishability:. Differential privacy for location-based systems. In Proceedings of the 2013 ACM SIGSAC conference. on Computer & communications security, pages 901914, 2013.. H. Asi, V. Feldman, and K. Talwar. Optimal algorithms for mean estimation under local differential. privacy. In International Conference on Machine Learning, pages 10461056. PMLR, 2022.. A. Beimel, K. Nissim, and E. Omri. Distributed private data analysis: Simultaneously solvinghow and what. In Advances in CryptologyCRYPTO 2008: 28th Annual International CryptologyConference, Santa Barbara, CA, USA, August 17-21, 2008. Proceedings 28, pages 451468. Springer,. 2008.. A. Beimel, K. Nissim, and U. Stemmer. Private learning and sanitization: Pure vs. approximatedifferential privacy. In International Workshop on Approximation Algorithms for Combinatorial. Optimization, pages 363378. Springer, 2013.. S. Biswas, Y. Dong, G. Kamath, and J. Ullman. Coinpress: Practical private mean and covariance. estimation. Advances in Neural Information Processing Systems, 33:1447514485, 2020.. T.-H. H. Chan, E. Shi, and D. Song. Optimal lower bound for differentially private multi-partyaggregation. In Proceedings of the 20th Annual European Conference on Algorithms, ESA12, page. 277288. Springer-Verlag, 2012. ISBN 9783642330896.. K. Chatzikokolakis, M. E. Andres, N. E. Bordenabe, and C. Palamidessi. Broadening the scope of. differential privacy using metrics. In Privacy Enhancing Technologies: 13th International Symposium,. PETS 2013, Bloomington, IN, USA, July 10-12, 2013. Proceedings 13, pages 82102. Springer, 2013.. K. Chaudhuri and D. Hsu. Sample complexity bounds for differentially private learning. In. Proceedings of the 24th Annual Conference on Learning Theory, pages 155186. JMLR Workshop and. Conference Proceedings, 2011.. 17. D. Desfontaines. A list of real-world uses of differential privacy. https://desfontain.es/. blog/real-world-differential-privacy.html, 2020. Accessed: 2024-05-22.. Differential Privacy Team, Apple. Learning with privacy at scale. https://machinelearning.. apple.com/research/learning-with-privacy-at-scale, 2017. Accessed: 2024-05-22.. B. Ding, J. Kulkarni, and S. Yekhanin. Collecting telemetry data privately. Advances in Neural. Information Processing Systems, 30, 2017.. J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local privacy, data processing inequalities, and. statistical minimax rates. arXiv preprint arXiv:1302.3203, 2013a.. J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local privacy and statistical minimax rates. In 2013. IEEE 54th Annual Symposium on Foundations of Computer Science, pages 429438. IEEE, 2013b.. C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data. analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York,. NY, USA, March 4-7, 2006. Proceedings 3, pages 265284. Springer, 2006.. C. Dwork, A. Roth, et al. The algorithmic foundations of differential privacy. Foundations and. Trends in Theoretical Computer Science, 9(34):211407, 2014.. U. Erlingsson, V. Pihur, and A. Korolova. Rappor: Randomized aggregatable privacy-preserving or-. dinal response. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications. Security, pages 10541067, 2014.. A. Evfimievski, J. Gehrke, and R. Srikant. Limiting privacy breaches in privacy preserving datamining.In Proceedings of the Twenty-second ACM SIGMOD-SIGACT-SIGART Symposium on. Principles of Database Systems, pages 211222, 2003.. V. Feldman and K. Talwar. Lossless compression of efficient private local randomizers. In Interna-. tional Conference on Machine Learning, pages 32083219. PMLR, 2021.. B. Ghazi, N. Golowich, R. Kumar, P. Manurangsi, and C. Zhang. Deep learning with label differential. privacy. Advances in Neural Information Processing Systems, 34:2713127145, 2021.. B. Ghazi, R. Kumar, P. Manurangsi, and T. Steinke. Algorithms with more granular differential. privacy guarantees. arXiv preprint arXiv:2209.04053, 2022.. X. He, A. Machanavajjhala, and B. Ding. Blowfish privacy: tuning privacy-utility trade-offs using. policies. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data,. SIGMOD 14, page 14471458. Association for Computing Machinery, 2014. ISBN 9781450323765.. J. Imola, S. Kasiviswanathan, S. White, A. Aggarwal, and N. Teissier. Balancing utility and scalability. in metric differential privacy. In Uncertainty in Artificial Intelligence, pages 885894. PMLR, 2022.. 18. P. Kairouz, S. Oh, and P. Viswanath. The composition theorem for differential privacy. In F. Bach. and D. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37. of Proceedings of Machine Learning Research, pages 13761385, Lille, France, 0709 Jul 2015. PMLR.. V. Karwa and S. Vadhan. Finite sample differentially private confidence intervals. arXiv preprint. arXiv:1711.03908, 2017.. S. P. Kasiviswanathan and A. Smith.On the semantics of differential privacy: A Bayesian. formulation. Journal of Privacy and Confidentiality, 6(1), 2014.. S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn. privately? SIAM Journal on Computing, 40(3):793826, 2011.. K. Kenthapadi, A. Korolova, I. Mironov, and N. Mishra. Privacy via the johnson-lindenstrauss. transform. Journal of Privacy and Confidentiality, 5(1), Aug. 2013. doi: 10.29012/jpc.v5i1.625.. D. Kifer and A. Machanavajjhala. No free lunch in data privacy. In Proceedings of the 2011 ACM. SIGMOD International Conference on Management of data, pages 193204, 2011.. D. Kifer and A. Machanavajjhala. Pufferfish: A framework for mathematical privacy definitions.. ACM Transactions on Database Systems (TODS), 39(1):136, 2014.. D. Kifer, A. Smith, and A. Thakurta.Private convex empirical risk minimization and high-dimensional regression. In S. Mannor, N. Srebro, and R. C. Williamson, editors, Proceedingsof the 25th Annual Conference on Learning Theory, volume 23 of Proceedings of Machine Learn-ing Research, pages 25.125.40, Edinburgh, Scotland, 2527 Jun 2012. PMLR.URL https:. //proceedings.mlr.press/v23/kifer12.html.. S. Mahloujifar, C. Guo, G. E. Suh, and K. Chaudhuri. Machine learning with feature differentialprivacy. In Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and. Opportunities, 2023.. A. G. Thakurta, A. H. Vyrros, U. S. Vaishampayan, G. Kapoor, J. Freudiger, V. R. Sridhar, and. D. Davidson. Learning new words, 2017.. A. Triastcyn and B. Faltings. Bayesian differential privacy for machine learning. In International. Conference on Machine Learning, pages 95839592. PMLR, 2020.. J. A. Tropp. The expected norm of a sum of independent random matrices: An elementary approach.. In High Dimensional Probability VII: The Cargese Volume, pages 173202. Springer, 2016.. S. Vadhan. The complexity of differential privacy. Tutorials on the Foundations of Cryptography:. Dedicated to Oded Goldreich, pages 347450, 2017.. 19. D. Vu and A. Slavkovic. Differential privacy for clinical trial data: Preliminary evaluations. In. 2009 IEEE International Conference on Data Mining Workshops, pages 138143, 2009. doi: 10.1109/. ICDMW.2009.52.. D. Wang and J. Xu. On sparse linear regression in the local differential privacy model. In International. Conference on Machine Learning, pages 66286637. PMLR, 2019.. S. L. Warner. Randomized response: A survey technique for eliminating evasive answer bias.. Journal of the American Statistical Association, 60(309):6369, 1965.. L. Wasserman and S. Zhou. A statistical framework for differential privacy. Journal of the American. Statistical Association, 105(489):375389, 2010.. H. Xiao and S. Devadas. Pac privacy: Automatic privacy measurement and control of data. processing. In Annual International Cryptology Conference, pages 611644. Springer, 2023.. X. Xiong, S. Liu, D. Li, Z. Cai, and X. Niu. A comprehensive survey on local differential privacy.. Security and Communication Networks, 2020:129, 2020.. M. Yang, T. Guo, T. Zhu, I. Tjuawinata, J. Zhao, and K.-Y. Lam. Local differential privacy and its. applications: A comprehensive survey. Computer Standards & Interfaces, page 103827, 2023.. K. Zheng, W. Mou, and L. Wang. Collect at once, use effectively: Making non-interactive locally. private learning possible. In International Conference on Machine Learning, pages 41304139. PMLR,. 2017.. 20. AProofs for Section 2 and Section 3. A.1Proof of Proposition 1. Proposition 1. If M is -LDP then (, M) is -BDP. Conversely, if (, M) is -BDP, then (assuming Y is. a Polish space) there exists a mechanism M which is -LDP such that P{M(x) = M(x)} = 1.. Proof. (A) Forward direction:. For any R  FY and S, S  F+X , we have. x(R)  ex(R)(23). =. xSx(R)d(x)  e. xSx(R)d(x)(24). = P{M(x)  R, x  S}  e(S)x(R),(25). = P{M(x)  R|x  S}  ex(R).(26). Similarly integrate again. xS d(x) to get the desired result.. (B) Converse:. Fix R  FY and let. U(R) = ess supxX x(R),(27). L(R) = ess infxX x(R),(28). where the essential supremum and infimum are with respect to (X, FX , ). Fix any  > 0 and. define. S = {x  X : x(R) > U(R)  }, S = {x  X : x(R) < L(R) + }.(29). By definition of essential supremum and infimum and using the fact that  is a Markov kernel,. S, S  F+X .. Using the -BDP condition, we can obtain. U(R)   1. (S). . xSx(R)d(x)(30). e. (S). . xS x(R)d(x)(31). e(L(R) + ).(32). Since  > 0 is arbitrary, we get U(R)  eL(R) R  FY.. Now, for R, define. E(R) := {x  X : x(R) / [L(R), U(R)]},(33). and note that (E(R)) = 0 by definition of essential supremum and infimum and using the fact. that  is a Markov kernel.. 21. We shall now modify M on a judiciously chosen -null set to obtain an -LDP mechanism M.Since Y is second countable as it is a Polish space, there exists a countable collection of open sets. V = {Vi}i1  FY such that every open set R  FY can be written as a disjoint union of some. countable subset of V.. Define E = i1E(Vi) and note that (E) = 0 by countable sub-additivity. Fix any x  Ec and. define the modification M of M via. M(x) =. . M(x)if x  E. M(x)otherwise.(34). Equivalently, M is represented by the Markov kernel  : X  FY  [0, 1] defined by. x(R) =. . x(R)if x  E. x(R)otherwise.(35). Since (E) = 0, it is easy to see that P{M(x) = M(x)} = 1. By construction, since E =. i1E(Vi),. x(Vi)  U(Vi)  eL(Vi)  ex(Vi) x, x  X, Vi  V.(36). By second countability, and -additivity, we conclude,. x(R)  ex(R) x, x  X, open R  FY.(37). Every second countable space is Radon, so by outer regularity of both measures x and x, we. finally obtain. x(R)  ex(R) x, x  X, R  FY.(38). Hence, M is -LDP as desired.. A.2Proof of Theorem 1. Theorem 1. A pair (, M) is -BCDP iff the following condition holds for every coordinate i  [d]:. ei(Ri, M, Si, Si) + (Ri, M, Si, Si)  1,(12). for any Si, Si  F+Xi and Ri  FY.. Proof. By definition (Ri, M, Si, Si) = P{M(x)  Ri|x  Si} and. P{M(x)  Ti|xi  Si}  eiP{M(x)  Ti|xi  Si} Si, Si  F+Xi, Ti  FY(39). Setting Ti = RCi , get (Ri, M, Si, Si) + ei(Ri, M, Si, Si)  1. One can also switch Si, Si above and. set Ti = Ri to get ei(Ri, M, Si, Si) + (Ri, M, Si, Si)  1.. 22. The converse is straightforward as well.. ei(Ri, M, Si, Si) + (Ri, M, Si, Si)  1 Si, Si  F+Xi, Ri  FY(40). P{M(x)  Ri|xi  Si}  eiP{M(x)  Ri|xi  Si} Si, Si  F+Xi, Ri  FY.(41). A.3Proof of Proposition 3. Proposition 3. (LDP to BCDP) An -LDP mechanism satisfies 1-BCDP.. Proof. By Proposition 1, an -DP algorithm is -BDP as well. Recall P{M(x)  R|x  S} = P(SR). (S) .. Thus, -BDP guarntee ensures that S, S  F+X ,. P(S  R). (S) e P(S  R). (S).. Restricting to the sets of form S = Si  Xi and S = Si  Xi for Si, Si  F+Xi above yields the. desired BCDP result.. A.4Proof of Proposition 4. Proposition 4. (CDP to BCDP) Suppose mechanism M is c-CDP and there exists q1, . . . , qd such that. TV(xi|xiSi, xi|xiSi)  qi  Si, Si  F+Xi.(13). Then:. 1. The mechanism is ni=1 ci-LDP and -BCDP with i = ci + log(1 + qiej=i cj  qi).. 2. In case that the mechanism is -LDP for some   0, then it is -BCDP with i = min{ci + log(1 +. qie  qi), }.. Proof. Consider the sets R  FY such that P{M(x)  R} = 0, then for Si, Si  F+Xi, we have. P{M(x)  R|xi  Si} = P{M(x)  R|xi  Si} = 0. Thus, to bound the  in -BCDP guarantee,. we can restrict to R in the set F+Y := {R  FY|P{M(x)  R} > 0}.. For Si, Si  F+Xi, R  F+Y ,. P{M(x)  R|xi  Si}P{M(x)  R|xi  Si} = P{M(x)  R, xi  Si}. {xi  Si}{xi  Si}. P{M(x)  R, xi  Si}.(42). 23. For the set R under consideration, define. l(xi) = infxiXi xi,xi(R),(43). u(xi) = supxiXixi,xi(R).(44). By CDP property, we have u(xi)  ecil(xi). Now note that. P{M(x)  R, xi  Si}. {xi  Si}=1. {xi  Si}. . xiXi,xiSixi,xi(R)d(xi, xi)(45). 1. {xi  Si}. . xiXi,xiSiu(xi)d(xi, xi)(46). =. xiXiu(xi)dxi|xiSi(xi).(47). Similarly, we have. P{M(x)  R, xi  Si}. P{xi  Si}. xiXil(xi)dxi|xiSi(xi).(48). Using (47) and (48) in (42), we obtain. P{M(x)  R|xi  Si}P{M(x)  R|xi  Si}. . xiXi u(xi)dxi|xiSi(xi). xiXi l(xi)dxi|xiSi(xi)(49). eci. xiXi l(xi)dxi|xiSi(xi). xiXi l(xi)dxi|xiSi(xi)(50). eci1 + TV(xi|xiSi, xi|XiSi)maxxi l(xi)  minxi l(xi). minxi l(xi). (51). eci1 + TV(xi|xiSi, xi|xiSi)(ej=i cj  1),(52). where we usedmaxxi l(xi)minxi l(xi)  ej=i cj by CDP property.. The i ci-LDP property follows by straightforward composition.. Finally, notice that, in case that we know the mechanism is -LDP, we can boundmaxxi l(xi)minxi l(xi) by. e. In addition, by Proposition 3, we know i  . Thus, we get i  min{ci + log(1 + qie  qi), }. A.5Additional Results for Section 3.4. Proposition 6. Let. A[i, j] =. . . 1ii = j,. 1. log1+ ei 1. qi. else.(53). 24. Then, the condition Ac  1 implies i  ci + log(1 + qiej=i cj  qi) for all i  [d].. Proof. Without loss of generality, we establish the proof for i = 1. For a given value of c1, we have. the constraint thatd. j=2cj  log1 + e1c1  1. q1. .(54). Note that g(x) = log1 + e1x1. q1. is concave and g(0) = log1 + e11. q1. and g(1) = 0. Thus,. g(x) 1  x. 1. log1 + e11. q1. . Therefore, we can ensure (54) by imposing. d. j=2cj 1  c1. 1. log1 + e1  1. q1. .(55). A.6Proof of Proposition 5. Proposition 5. (Post processing for BCDP) Let M() be a -BCDP mechanism and K : Y  Z be a. measurable function. Then, K  M is also -BCDP.. Proof. Let the -field on Z be denoted by FZ. For T  FZ, Y (T) := {y  Y|K(y)  T}  FY since. K is a measurable function. Thus, we have the following for any i  [d], T  FZ and Si, Si  F+Xiby BCDP guarantees on M. P{K(M(x))  T|xi  Si} = P{M(x)  Y (T)|xi  Si}(56). eiP{M(x)  Y (T)|xi  Si}(57). = eiP{K(M(x))  T|xi  Si}.(58). BPrivate Mean Estimation. B.1Numerical Experiment. Here, we provide a simple numerical experiment to validate the performance of our proposedalgorithm. For baseline comparison, we consider the MLDP privacy mechanism from Duchi et al.[2013b] with privacy parameter min . For our proposed privacy mechanism Mmean, we have adegree of freedom in choosing . We note that, as q  1, we would want   1 since in this case,. the best algorithm is to do min -LDP. As q  0, we would want ci = i as CDP is ideal in this case.. Thus, we settle with a heuristic choice  = (1 + q)/2.. 25. <image: DeviceRGB, width: 1280, height: 960, bpc: 8>. Figure 3: MSE as q is varied keeping  and  constant for Mmean and min -LDP.. We first construct a distribution for which Assumption 1 holds. We consider the distribution. where. Z  Bern1. 2. ,(59). (x1, . . . , xd) =. . (Z, . . . , Z)w.p. q. i.i.d. Bern 1. 2else.(60). It is easy to verify that TV(xi|xi=1, xi|xi=0) = qi in this setting. We fix d = 10,  =. (0.2, 0.2, , , . . . , ), and  = 2. This coordinate-wise privacy demand captures the impact of. requiring more privacy protection for the first two-coordinates on the algorithms error.We vary the correlation q and plot the median MSE values, along with the 25-th and 75-thquantiles, for our proposed algorithm BCDP and the baseline algorithm in Figure 3. We set. n = 10000, and sample user data i.i.d. 2Bern( 1. 2)  1. The experiment is run over 1000 trials.When q  1, there is a high degree of correlation between the coordinates and our algorithm. resembles min -LDP. For other values of q, the proposed BCDP algorithm can leverage the hetero-. geneous nature of the privacy demand and obtain better performance.. B.2Proofs. Theorem 2. Suppose Assumption 1 holds and let i := min{i, } for any i  [d]. Then, Mmean, i.e.,. Mechanism 1, with parameters. cd = min. . log(e1 + q  1. q), d. . , c0 = 0,(16). 26. i < d, ci =. . cdif cd  i,. i  log(1 + qecd  q)otherwise,. where   (0, 1] is a free parameter, is -BCDP and -LDP. Moreover, using Mechanism 2 as MLDP, we. obtain the following mean-squared error bound. E. . x   1. n. n. j=1Mmean(x(j), c). 2. 2. x(1), . . . , x(n). . 1. n. d. i=1. 1. ik=1(ckck1)2. dk+1. . d,(17). where () denotes projection into [1, 1]d and the expectation is taken over the randomness in Mmean().. Proof. Our proof has two main parts that are presented below: proof of privacy guarantee, and. proof of the error bounds.Proof of privacy: Note that we use the channel MLDP(.) a total of d times with varying size. inputs in Mechanism 1. Focusing on any particular data point x, let. M(x) :=MLDP(xi:d, ci  ci1,. d  i + 1)d. i=1 .(61). First, notice that, by composition, M is cd-LDP. Given that cd  d  , this immediately implies. that our algorithm is -LDP. To show the BCDP bound, given Proposition 5, it suffices to show M is. -BCDP. First, notice that Proposition 3, along with choice of cd  d, implies that this algorithm is. d-BCDP with respect to coordinate d. In fact, for any i for which cd  i, a similar argument impliesthat M is i-BCDP with respect to coordinate i. Hence, we could only focus on other coordinates. i < d for which cd > i.Next, we claim that M(x) is c-CDP. To see this, notice that, by perturbing the k-th coordinateof x, the terms affected in M(x) are MLDP(xi:d, ci  ci1,. d  i + 1) for i  k; in Figure 2, this. corresponds to the bottom i rows of the Y (s), i.e., {Y k}k[i]. Thus, it suffices to show. Mk(x) :=MLDP(xi:d, ci  ci1,. d  i + 1)k. i=1(62). is ck-CDP with respect to coordinate k. However, this immediately follows as Mk is ck-LDP,. implying it is also ck-CDP with respect to the k-th coordinate.. As a result, by the second part of Proposition 4, and given that M is cd-LDP and Assumption 1. holds, we obtain that M is ci + log(1 + qecd  q)-BCDP with respect to coordinate i. It remains to. show that ci + log(1 + qecd  q)  i for any i < d. This also follows from the choice of ci for i < d. and cd > i, which are the conditions we are operating under. It is worth noting that the condition. cd  log(e1 + q  1. q)(63). guarantees that the chosen cis are nonnegative, given   (0, 1).. 27. Proof of the mean-squared error bound:Duchi et al. [2013a] show that the channel MLDP()s output is unbiased and it holds that for. any x  [1, 1]d, we have. E[MLDP(x, , d)  x22|x]  O(d) d. 2  1.(64). Note that, due to the symmetry of Algorithm 2, the error is identically distributed across each. coordinate in expectation. Hence, for any vector x and any k  i, the random variable Y kik+1 is an. unbiased estimators of xi with its variance bounded by O(1)di+1. (cici1)2  1.Note that, the errors from different runs of the algorithm are independent. Therefore, usingthe CauchySchwarz inequality, we can see that the minimum variance in estimating xi througha linear combination of Y (i,j) is achieved by choosing weights proportional to the inverse of the. variances which would result in the following estimator. ik=1 Y ki+1kwkik=1 wk(65). with the error rate given by. O(1)1. ik=1(ckck1)2. dk+1 1.(66). Also, note that, the algorithms runs over different users are independent. Therefore, taking average. of Mmean(x(j), c) over j, i.e., different users data, gives us an estimator of. 1n. n. j=1x(j)i(67). with the error rate. O(1) 1. n 1. ik=1(ckck1)2. dk+1 1.(68). Summing this over the coordinates gives us the following upper bound:. O(1) 1. n. d. i=1. 1. ik=1(ckck1)2. dk+1 d.(69). Corollary 1. Suppose we are under the premise of Theorem 2, with i =  for 1  i  k and i =  > 2. for d  i > k. Then, by choosing  = 0.5, the mean-squared error upper bound in (17) simplifies to. O(1) 1. n dk. 2 + (d  k)2. 2. ,(18). 28. for the case q  e/21. e1 , and to. O(1) 1. n. dk2 +(d  k)2. dk. d 2 + (cd  /2)2. . ,(19). otherwise, with cd = log( e/2+q1. q) decreasing from  to /2 as q increases from (e/2  1)/(e  1) to 1.. Proof. For convenience, we set  = 0.5. We do not focus on refining the constants in the Mean-. Squared Error (MSE) bound with a potentially better choice of . Now consider two cases for the. correlation bound q. 1. q  [0, e/21. e1 ]: in this case, we have ck+1 = ck+2 = . . . = cd =  and c1 = c2 = . . . = ck =. log(1+qe q). Using the range of q, we get  log(1+qe q)  [/2, ]. Thus, replacing. the c1 to ck values by the worst-case of /2 we get an error of. MSE  d. n. k2 +d  k. 2 +d. dk(  /2)2. . d(70). Using   2, we get. MSE  dk. n2 + (d  k)2. n2. d(71). 2. q  ( e/21. e1 , 1]: in this case we have ck+1 = . . . = cd = log e/2+q1. qand c1 = . . . = ck =. log(1 + qecd  q) = /2. Thus, we get an error of. MSE. . dk. n2 +(d  k)d. n2 +d. dk(cd  /2)2. . d(72). CLeast-Squares Regression. C.1Proof of Theorem 3. Theorem 3. Suppose Assumptions 1 and 2 hold. Then, Algorithm 3, with c set similar to Theorem 2, is. -BCDP and -LDP. Moreover, using Mechanism 2 as MLDP, we obtain the following error rate. Ef()  fx(1), . . . , x(n) max(2 + )log dn (r2 + rd)  d,(22). with r2 = di=1 1/(ik=1(ckck1)2. dk+1).. Proof. Proof of privacy: The proof of the privacy guarantee follows a similar structure to the proofof Theorem 2. Note that while composition does not hold for BCDP, it does hold for LDP and. 29. CDP. This is why we divide c by two, rather than dividing  by two. Additionally, since the OPT. algorithm only observes the private copies of the data, by the post processing inequality, it does not. matter how the algorithm finds the minimizer (e.g., how many passes it makes over each private. data point), as the privacy guarantee remains unchanged.. Proof of error rate: Without loss of generality, we can ignore the constant term in f() that does. not depend on , and hence, with a slight abuse of notation, we can rewrite f() as. f() := 1. 2n. n. i=1. z(i)z(i)  2l(i)z(i).(73). Also, recall that f() is given by. f() := 1. 2n. n. i=1. z(i,1)z(i,2)  2l(i,1)z(i,2).(74). Denote the minimizer of f() over  by ({x(i,1), x(i,2)}ni=1). Also, we denote the output of. Algorithm 3 by ({x(i,1), x(i,2)}ni=1). Finally, recall that  denotes the minimizer of function f.For convenience and brevity, we drop the conditioning on x(1), . . . , x(n) from the notation, but all. expectations henceforth are conditioned on x(1), . . . , x(n).. Next, note that. 1. By the definition of OPT, we have. f({x(i,1), x(i,2)}ni=1) f({x(i,1), x(i,2)}ni=1) 1. n.(75). 2. By the definition of ({x(i,1), x(i,2)}ni=1), we have. f({x(i,1), x(i,2)}ni=1) f()  0.(76). 3. Given the construction of f, it is an unbiased estimator of f for any fixed . Hence, we have. E[ f()  f()] = 0.(77). Summing all the three, given us. Ef({x(i,1), x(i,2)}ni=1) f() 1. n.(78). However, we need to bound. Ef({x(i,1), x(i,2)}ni=1) f().(79). 30. Therefore, it suffices to bound. Ef({x(i,1), x(i,2)}ni=1) f({x(i,1), x(i,2)}ni=1).(80). Observe that this term is upper bounded by. Esup(f()  f()).(81). Note that (81) is upper bounded by. sup2E. 1n. n. i=1z(i)z(i)  z(i,1)z(i,2). . + supE. 1n. n. i=1l(i)z(i)  l(i,1)z(i,2). . .(82). We bound the first term above. The second term can be bounded similarly. To do so, first note that. we can decompose z(i,1)z(i,2)  z(i)z(i) as. (z(i,1)  z(i))z(i) + z(i)(z(i,2)  z(i)) + (z(i,1)  z(i))(z(i,2)  z(i)).(83). Hence, it suffices to bound the expected norm of the average of each term separately, and all of. them can be handled similarly. We will focus on the last term in particular, i.e., we will bound. E. 1n. n. i=1Ai. . (84). with Ai := (z(i,1)  z(i))(z(i,2)  z(i)). Next, we make the following claim regarding matrices. {Ai}ni=1, proved in Appendix C.2.. Lemma 1. Let. r2 :=supx[1,1]d E[Mmean(x, c/2)  x2|x].(85). Then, the following two results hold:. . iE[AiAi]  nr4,(86). E[maxiAi2]  nr4.(87). Let us first show how this lemma completes the proof. Using Theorem 1 in Tropp [2016],. Jensens inequality, along with this lemma, we can immediately upper bound (84) by. O(1) log(d) r2. n.(88). Similarly, the terms (z(i,1)z(i))z(i) and z(i)(z(i,2)z(i)) can be upper bounded by O( log(d)rdn). 31. using similar steps as proof of Lemma 1.. Recall r2 = O(1) di=11. ik=1(ckck1)2. dk+1. Note that we use Mmean without the truncation to [1, 1]d. to preserve unbaisedness, and hence, the minimum with d does not appear in r2, unlike Theorem 2.. Thus we obtain the bound. Esup(f()  f()) max(2 + )r2 log dn+ rd log dn. .(89). We can also project the estimate  to the set  to obtain the bound max(2 + )d. Taking. minimum of the terms, we get Theorem 3.. C.2Proof of Lemma 1. First, note that. AiAi = z(i,2)  z(i)2(z(i,1)  z(i))(z(i,1)  z(i)),(90). which yields. E[AiAi]  r2E[(z(i,1)  z(i))(z(i,1)  z(i))].(91). Using the result that uu = u2 for any vector u completes the proof of the first part of the. lemma. To prove the second part, note that. E[maxiAi2]  E[. n. i=1Ai2] =. n. i=1E[Ai2].(92). Using the result that uv  uv for any two vectors u and v implies. E[Ai2]  Ez(i,1)  z(i)2z(i,2)  z(i)2.(93). Finally, the conditional independence of z(i,1) and z(i,2) given z(i) completes the proof of lemma.. 32", "2410.18407v1.On_topological_solutions_to_a_generalized_Chern_Simons_equation_on_lattice_graphs.pdf": "arXiv:2410.18407v1  [math.AP]  24 Oct 2024. On topological solutions to a generalized Chern-Simons equationon lattice graphs. Songbo Hou. Department of Applied Mathematics, College of Science, China Agricultural University, Beijing, 100083, P.R. China. Xiaoqing Kong. Department of Applied Mathematics, College of Science, China Agricultural University, Beijing, 100083, P.R. China. <image: None, width: 1, height: 1, bpc: 1>. Abstract. In this paper, we study a generalized self-dual Chern-Simons equation on the lattice graph Zn forn  2 as given by. u = eu(eu  1)2p+1 + 4. M. j=1n jpj,. where  denotes the Laplacian operator,  is a positive constant, p is a non-negative integer,n1, . . ., nM are positive integers, and p1, . . . , pM are distinct points with pj representing the Diracdelta function at p j. We establish the existence of a topological solution that is maximal amongall possible solutions. Our ndings extend those of of Hua et al. [arXiv:2310.13905], Chao andHou [J. Math. Anal. Appl. 519(1), 126787(2023)], and Hou and Qiao [J. Math. Phys. 65(8),081503(2024)].. Keywords: Chern-Simons equation, lattice graph, topological solution, maximal solution2020 MSC: 35A01 35A16 35J91 35R02. <image: None, width: 1, height: 1, bpc: 1>. 1. Introduction. The Chern-Simons theory was indeed proposed by Shiing-Shen Chern and James Simons in1974. This theory was initially developed within the eld of mathematics to study the geometricstructures on three-dimensional manifolds. Later, it found broad applications in physics, particu-larly in quantum physics and condensed matter physics. The theory quickly gained signicancein understanding topological phase transitions, especially in the context of quantum Hall eects,topological insulators, and high-temperature superconductors [8, 44, 45].Considerable research has been conducted in the area of eld theory models inuenced byChern-Simons type dynamics, such as [34, 35, 41] and so on. From a mathematical perspective,the equations of motion for various Chern-Simons models pose signicant analytical challenges,even in scenarios involving radial symmetry and static conditions [32]. The discovery of the. <image: None, width: 1, height: 1, bpc: 1>. Corresponding author: Songbo HouEmail addresses: housb@cau.edu.cn (Songbo Hou ), kxq@cau.edu.cn (Xiaoqing Kong)Preprint submitted to ***October 25, 2024. self-dual structure in the Abelian Chern-Simons model in [21, 31] has inspired a lot of subse-quent research. Many problems of existence can be transformed into studies of elliptic partialdierential equations or systems of equations, with a particular focus on exploring topologicaland non-topological solutions [3, 4, 18, 19, 29, 49].Partial dierential equations on discrete graphs have recently garnered signicant interestand are now widely applied in diverse elds such as image processing, social network analysis,bioinformatics, and machine learning. Recent advancements have extended some traditionalmethods for solving partial dierential equations in Euclidean spaces to the context of graphs.In seminal works by Grigoryan et al. [1517], a variational approach was developed to studyYamabe type equations, the Kazdan-Warner equation, and some nonlinear equations. These stud-ies primarily focused on establishing the existence of solutions. Subsequent research has exten-sively explored various types of partial dierential equations on graphs. Notably, the existenceresults for Yamabe type equations have been detailed in references such as [11, 13, 52]. Studiesaddressing Kazdan-Warner equations are found in [10, 12, 37, 47, 51], and the results concerningSchrodinger equations appear in [5, 40, 43, 50]. Additionally, signicant ndings related to theheat equations are documented in [36, 38].In this paper, we study a generalized self-dual Chern-Simons equation given by. u = eu(eu  1)2p+1 + 4. M. j=1n jpj,(1.1). where  denotes the Laplacian operator,  is a positive constant, p is a non-negative integer,n1, . . ., nM are positive integers , and p1, . . . , pM are distinct points with pj being the Dirac deltafunction at p j. In the case of Euclidean spaces, a solution u(x) to equation (1.1) is classied astopological if u(x)  0 as |x|  +, and non-topological if u(x)   as |x|  +.When p = 0 and ni = 1 (for i = 1, 2, . . ., M), Eq.(1.1) reduces to. u = eu(eu  1) + 4. M. j=1pj.(1.2). Caarelli et al. [2] and Tarantello [48] investigated Eq.(1.2) in a doubly periodic region or the2-torus in R2 and established the existence of solutions. Huang et al. [27] and Hou et al. [24]studied Eq.(1.2) on nite graphs and obtained the existence of solutions. When p = 2 and ni = 1(for i = 1, 2, . . ., M), Eq.(1.1) is reduced to. u = eu(eu  1)5 + 4. M. j=1pj.(1.3). Han [20] established the existence of multi-vortices for Eq.(1.3) over a doubly periodic region inR2. Chao et al. [6] and Hu [25] obtained multiple solutions to Eq.(1.3) on nite graphs. Furtherstudies on the Chern-Simons models on graphs include [7, 9, 22, 23, 26, 30, 33, 39].Next, we introduce the lattice Zn for n  2. The lattice Zn is a discrete graph which consistsof the vertex set V and the edge set E:. V = Zn = {x = (x1, . . . , xn)  Rn : xi  Z for all i  {1, . . ., n}} ,. E =. {x, y} : x, y  V with. n. i=1|xi  yi| = 1. .. 2. Write x  y if {x, y} is an edge in E. The distance on Zn is dened by. d(x, y) =. n. i=1|xi  yi| ,x, y  Zn,. with d(x) = d(x, 0) representing the distance from the origin.For a nite subset   Zn, the boundary of  is delineated by. := {y  Zn \\  : x   such that y  x} ,. and the closure of  is designated as  =   .Now, we introduce function spaces and operators on graphs to prepare for the subsequentanalysis. Denote C(Zn) as the set of functions dened on Zn. For a nite subset   Zn, wedene C() as the set of functions dened on . The lp-norm (1  p < ) of u  C(Zn) isdened as. ulp(Zn) =. . xZn|u(x)|p. 1. <image: None, width: 1, height: 1, bpc: 1>. p,. and the l-norm of u  C(Zn) is dened as. ul(Zn) = supxZn |u(x)|.. We also dene the following seminorm:. |u|1,p :=. . xZn. . yx|u(y)  u(x)|p. 1. <image: None, width: 1, height: 1, bpc: 1>. p.. For any u  C(Zn), the Laplacian operator is expressed as. u(x) =. d(x,y)=1(u(y)  u(x)).. Additionally, the dierence operator is specied by. xyu = u(y)  u(x),for any u  C(Zn) and x, y  Zn.. For functions u, v  C( ), a bilinear form is introduced:. E(u, v) := 1. <image: None, width: 1, height: 1, bpc: 1>. 2. . x,yxy. xyuxyv +. x,yxy. xyuxyv,(1.4). and the Dirichlet energy of u on  is dened as E(u) = E(u, u). The directional derivativeoperator u. <image: None, width: 1, height: 1, bpc: 1>. n at x   isu. <image: None, width: 1, height: 1, bpc: 1>. n(x) :=. yxy. (u(x)  u(y)).. There also holds Greens identity [14]:. E(u, v) =. xu(x)v(x) +. xu(x) v. <image: None, width: 1, height: 1, bpc: 1>. n(x).(1.5). 3. In this study, we focus primarily on topological solutions to the generalized Chern-Simonsequation dened on Zn:. u = eu(eu  1)2p+1 + 4. M. j=1n jpj,. limd(x)+ u(x) = 0,. (1.6). where we aim to construct a topological solution that is maximal among all possible solutions.Our main result is presented as follows.. Theorem 1.1. The equation (1.6) admits a topological solution u  l2p+2(Zn) on Zn for n  2,which is maximal among all possible solutions.. In our proof, we employ methods from [26, 46]. First, we establish an iterative scheme, whichyields a monotone sequence {uk} addressing the Dirichlet problem. Subsequently, we introducethe functional J(u) and demonstrate that J(uk) is uniformly bounded. By using the discreteGagliardo-Nirenberg-Sobolev inequality, we ensure that the sequence {ukl2p+2()} is uniformlybounded. As we pass to the limit, we ascertain a solution on . Utilizing the exhaustion method,we extend this solution to Zn. Additionally, we establish the maximality of the solution.. 2. Proof of Theorem 1.1. Initially, we recall a foundational result: the maximum principle.. Lemma 2.1. [26] Let  be a nite subset of Zn. Consider any positive function f  C(. <image: None, width: 1, height: 1, bpc: 1>. ).Suppose that a function v  C(. <image: None, width: 1, height: 1, bpc: 1>. ) satises the following conditions:(  f)v  0on ,v  0on .. Then, it follows that v  0 on. <image: None, width: 1, height: 1, bpc: 1>. .. Next, we use the maximum principle to establish the iterative sequence. Let 0 be a nitesubset of Zn such that 0  {p j}Mj=1. Additionally, let  be an arbitrary connected nite subsetsatisfying 0    Zn. Let g = 4 Mj=1 n jpj and N = 4 Mj=1 n j. Choose a constantK > (2p + 2) > 0. Set u0 = 0 and consider the following iterative equations: (  K) uk = euk1 (euk1  1)2p+1 + g  Kuk1 on ,uk = 0 on .(2.1). Lemma 2.2. Let the sequence {uk} be dened as in (2.1). Then, for each k, uk is uniquely denedand satises0 = u0  u1  u2  . . . .. Proof. First, we establish the following equation for u1:(  K) u1 = gon ,u1 = 0on .(2.2). 4. By the variation method in Lemma 2.2 in [22], we get that (2.2) admits a unique solution. Uti-lizing Lemma 2.1, it follows that u1  0.Assuming that0 = u0  u1  u2  . . .  ui,. and given thateui (eui  1)2p+1 + g  Kui  l2(),. we can also guarantee the existence and uniqueness of ui+1 by the variation method again.Analyzing the iterative equation (2.1), we derive that. (  K) (ui+1  ui) = eui (eui  1)2p+1  eui1 (eui1  1)2p+1  K(ui  ui1). e (e  1)2p (2p + 2)e  1 (ui  ui1)  K(ui  ui1). K(e2  1)(ui  ui1)  0,. where  is a function satisfying ui    ui1. This indicates that ui+1  ui, thereby armingthe lemma using Lemma 2.1.. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. Next, we dene the natural functional J(u) and demonstrate that J(uk) decreases as k in-creases. Utilizing Greens identities, we introduce the following functional dened over :. J(u) = 1. <image: None, width: 1, height: 1, bpc: 1>. 2 E(u) +. x. . <image: None, width: 1, height: 1, bpc: 1>. 2p + 2(eu(x)  1)2p+2 + g(x)u(x).(2.3). Lemma 2.3. Let {uk} be the sequence dened in Eq.(2.1). Then, the following inequality holds:. C  J(u1)  J(u2)  . . .  J(uk)  . . . ,. where the constant C depends only on n, , p and N.. Proof. By multiplying Eq.(2.1) by the dierence uk  uk1 and performing a summation acrossthe domain , we derive:. x(  K) uk(x) [uk(x)  uk1(x)]. =. x. euk1 (euk1  1)2p+1 (uk  uk1)  Kuk1 (uk  uk1) + g (uk  uk1)(x).(2.4). Utilizing the Greens identity as specied in (1.5), we have. xuk(x) (uk(x)  uk1(x)) = E(uk  uk1, uk) = E(uk) + E(uk1, uk).(2.5). Integrating this with equation (2.4), we conclude. E(uk)  E(uk1, uk) +. xK (uk(x)  uk1(x))2. =. x. euk1 (euk1  1)2p+1 (uk  uk1) + g(x) (uk  uk1).(2.6). 5. Now we introduce a concave function for x  0:. h(x) =. <image: None, width: 1, height: 1, bpc: 1>. 2p + 2 (ex  1)2p+2  K. <image: None, width: 1, height: 1, bpc: 1>. 2 x2.. It is easy to see that. h (uk1)  h (uk)  h (uk1)(uk1  uk) = (euk1  1)2p+1 euk1  Kuk1(uk1  uk) ,. which yields that. . <image: None, width: 1, height: 1, bpc: 1>. 2p + 2 (euk  1)2p+2. <image: None, width: 1, height: 1, bpc: 1>. 2p + 2 (euk1  1)2p+2 + K. <image: None, width: 1, height: 1, bpc: 1>. 2 (uk  uk1)2. + euk1 (euk1  1)2p+1 (uk  uk1) .(2.7). It follows from (1.4) that. |E (uk1, uk)| 1. <image: None, width: 1, height: 1, bpc: 1>. 2. . x,yxy. xyuk1xyuk +. x,yxy. xyuk1xyuk. 1. <image: None, width: 1, height: 1, bpc: 1>. 4. . x,yxy. xyuk12 +xyuk2+ 1. <image: None, width: 1, height: 1, bpc: 1>. 2. . x,yxy. xyuk12 +xyuk2. =1. <image: None, width: 1, height: 1, bpc: 1>. 2 E (uk1) + 1. <image: None, width: 1, height: 1, bpc: 1>. 2 E (uk) .. (2.8). Combining (2.6), (2.7) and (2.8), we conclude that. J (uk)  J (uk) + K. <image: None, width: 1, height: 1, bpc: 1>. 2 uk1  uk2l2()  J (uk1) .. Next, we estimate the upper bound for J(u1). Noting that. E (u1) = 1. <image: None, width: 1, height: 1, bpc: 1>. 2. . x,yxy. xyu12 +. x,yxy. xyu12. . x,yxy. u1(x)2 + u1(y)2+ 2. x,yxy. u1(x)2 + u1(y)2. 4n u12l2() ,. and |eu1  1| = 1  eu1  u1, we get. J (u1)  1. <image: None, width: 1, height: 1, bpc: 1>. 2  4n u12l2() +. <image: None, width: 1, height: 1, bpc: 1>. 2p + 2. . xu1(x)2p+2 + 1. <image: None, width: 1, height: 1, bpc: 1>. 2. . x. g(x)2 + u1(x)2. = c1 + c2u12l2() + u12p+2l2p+2(),. where c1, c2 are constants that only depend on n, p,  and N.Multiplying (2.2) by u1 and summing over , we have. E (u1) + K. xu1(x)2 =. xg(x)u1(x).. 6. This yields. K. xu1(x)2  1. <image: None, width: 1, height: 1, bpc: 1>. 2K. . xg(x)2 + K. <image: None, width: 1, height: 1, bpc: 1>. 2. . xu1(x)2.. Hence,. xu1(x)2 g2l2(Zn). <image: None, width: 1, height: 1, bpc: 1>. K2,. which yields. xu1(x)2p+2. . xu1(x)2. p+1. g2l2(Zn). <image: None, width: 1, height: 1, bpc: 1>. K2. . p+1. .. We derive that J(u1)  C, where C depends only on n, , p and N, and complete the proof.. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. Next, utilizing the discrete Gagliardo-Nirenberg-Sobolev inequality along with Lemma 2.3,we establish the bound of ukl2p+2(). The proof of Theorem 4.1 in [42] provides the followingdiscrete Gagliardo-Nirenberg-Sobolev inequality:. Lemma 2.4. [42] Let n  2, q > 1,   q and q =q. <image: None, width: 1, height: 1, bpc: 1>. q1. Then for any u  lq (Zn), we have. u. ln. <image: None, width: 1, height: 1, bpc: 1>. n1 (Zn)  C(q, n, )|u|1,qu1l(1)q(Zn).. Lemma 2.1 in [28] implies that. ulp(Zn)  ulp(Zn),. for any p  p. Letting  = 2(p + 1), q = 2 and q = 2, we have for u  l2(Zn),. ul4p+4(Zn)  ul2n(p+1). <image: None, width: 1, height: 1, bpc: 1>. n1(Zn)  C(n, p)|u|. 1. <image: None, width: 1, height: 1, bpc: 1>. 2p+21,2 u. 2p+1. <image: None, width: 1, height: 1, bpc: 1>. 2p+2l4p+2(Zn).(2.9). Lemma 2.5. Let {uk} be the sequence dened by Eq.(2.1). For any k  1, we have. ukl2p+2()  C2 (J(uk) + 1)  C1,(2.10). where C2 and C1depend only on n, , p and N.. Proof. Let uk denote the null extension of uk to Zn as follows:. uk(x) = uk(x)on ,0on c.(2.11). It is evident that uk  l2(Zn). By (2.9), the following inequality holds:. uk2p+2l4p+4(Zn)  (C(n, p))2p+2|uk|1,2uk2p+1l4p+2(Zn).. Referring to (2.11), we obtain:. uk4p+4l4p+4(Zn) =. xuk(x)4p+4,. uk4p+2l4p+2(Zn) =. xuk(x)4p+2,. 7. and|uk|1,2  (2E(uk))1. <image: None, width: 1, height: 1, bpc: 1>. 2 .. Thus, it follows that:. xuk(x)4p+4  C3E(uk). xuk(x)4p+2,(2.12). where C3 = 2(C(n, p))4p+4.Using the identity 1  euk = 1  e|uk| = e|uk|1. <image: None, width: 1, height: 1, bpc: 1>. e|uk||uk|. <image: None, width: 1, height: 1, bpc: 1>. 1+|uk|, and (2.3), we deduce:. J(uk) = 1. <image: None, width: 1, height: 1, bpc: 1>. 2 E(uk) +. x. . <image: None, width: 1, height: 1, bpc: 1>. 2p + 2. euk(x)  12p+2 + g(x)uk(x). 1. <image: None, width: 1, height: 1, bpc: 1>. 2E(uk) +. <image: None, width: 1, height: 1, bpc: 1>. 2p + 2. . x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2 gl4p+4. <image: None, width: 1, height: 1, bpc: 1>. 4p+3 ()ukl4p+4(). 1. <image: None, width: 1, height: 1, bpc: 1>. 2E(uk) +. <image: None, width: 1, height: 1, bpc: 1>. 2p + 2. . x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2 C4 (E(uk))1. <image: None, width: 1, height: 1, bpc: 1>. 4p+4. xuk(x)4p+2. 1. <image: None, width: 1, height: 1, bpc: 1>. 4p+4,. where C4 is a uniform constant depending only on n, p and N.Let  > 0 is a constant to be chosen later. By Yungs inequality, we have. C4 (E(uk))1. <image: None, width: 1, height: 1, bpc: 1>. 4p+4. xuk(x)4p+2. 1. <image: None, width: 1, height: 1, bpc: 1>. 4p+4. =. C4 2p+1. <image: None, width: 1, height: 1, bpc: 1>. 2p+22p + 2. <image: None, width: 1, height: 1, bpc: 1>. 2p + 1. 2p+1. <image: None, width: 1, height: 1, bpc: 1>. 2p+2(E(uk))1. <image: None, width: 1, height: 1, bpc: 1>. 4p+4. . 2p+1. <image: None, width: 1, height: 1, bpc: 1>. 2p+22p + 2. <image: None, width: 1, height: 1, bpc: 1>. 2p + 1. 2p+1. <image: None, width: 1, height: 1, bpc: 1>. 2p+2. xuk(x)4p+2. 1. <image: None, width: 1, height: 1, bpc: 1>. 4p+4. ukl4p+2(Zn) + C5E(uk)1. <image: None, width: 1, height: 1, bpc: 1>. 2. ukl2p+2(Zn) + 1. <image: None, width: 1, height: 1, bpc: 1>. 4E(uk) + C6,. where C5 and C6 depend only on , n, N and p.Hence, we obtain. J(uk) 1. <image: None, width: 1, height: 1, bpc: 1>. 2E(uk) +. <image: None, width: 1, height: 1, bpc: 1>. 2p + 2. . x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2 ukl2p+2(Zn)  1. <image: None, width: 1, height: 1, bpc: 1>. 4 E(uk)  C6. =1. <image: None, width: 1, height: 1, bpc: 1>. 4E(uk) +. <image: None, width: 1, height: 1, bpc: 1>. 2p + 2. . x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2 ukl2p+2()  C6.. (2.13). 8. By the inequality (2.12), we have the following estimate:. xuk(x)2p+2. 2=. . x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. p+1(1 + |uk(x)|)p+1|uk(x)|p+1. 2. . x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2. x(1 + |uk(x)|)2p+2uk(x)2p+2. 22p+2. x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2. x. uk(x)2p+2 + uk(x)4p+4. 22p+2. x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2. xuk(x)2p+2 + 22p+2C3. x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2E(uk). xuk(x)4p+2. 22p+2. x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2. xuk(x)2p+2 + 22p+2C3. x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2E(uk). . xuk(x)2p+2. 2p+1. <image: None, width: 1, height: 1, bpc: 1>. p+1. 1. <image: None, width: 1, height: 1, bpc: 1>. 4. . xuk(x)2p+2. 2+ C7. . . x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2. 2+. x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2E(uk). . xuk(x)2p+2. 2p+1. <image: None, width: 1, height: 1, bpc: 1>. p+1. 1. <image: None, width: 1, height: 1, bpc: 1>. 2. . xuk(x)2p+2. 2+ C8. . . x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2. 2+. . x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2. 2p+2E(uk)2p+2. 1. <image: None, width: 1, height: 1, bpc: 1>. 2. . xuk(x)2p+2. 2+ C9. 1 +. . x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2. 4p+4+ E(uk)4p+4 ,. which yields that. ukl2p+2()  C10. 1 +. x. |uk(x)|. <image: None, width: 1, height: 1, bpc: 1>. 1 + |uk(x)|. 2p+2+ E(uk). ,(2.14). where C7-C10 are constants depending only on n and p.. Choosing  =min{ 1. <image: None, width: 1, height: 1, bpc: 1>. 8 ,. <image: None, width: 1, height: 1, bpc: 1>. 4p+4 }. <image: None, width: 1, height: 1, bpc: 1>. C10and combining (2.13) and (2.14), we get. ukl2p+2()  C2(J(uk) + 1).. Furthermore, by Lemma 2.3, we conclude. ukl2p+2()  C2(J(uk) + 1)  C1,. where C2 and C1 depend only on n, , p and N.. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. The boundedness of ukl2p+2() ensures the existence of a solution to the Chern-Simons equa-tion on .. Lemma 2.6. Let  be a nite subset of Zn containing the distinct points {p j}Mj=1. Consider theboundary value problemu = eu(eu  1)2p+1 + gon ,u(x) = 0on .9. A solution u :   R exists for this problem. This solution is maximal among all possiblesolutions and satises the condition ul2p+2()  C1, where C1 depends only on n, , p and N.. Proof. By Lemmas 2.5 and 2.2, we have. uk  u in l2p+2(),. andul2p+2()  C0.. Since  is a local operator, and given the pointwise convergence, the function u in l2p+2() isthe solution to the equationu = eu (eu  1)2p+1 + g on ,u(x) = 0 on .. This completes the main proof.The remaining task is to demonstrate that this solution is maximal. We claim that for anyfunction V  C(. <image: None, width: 1, height: 1, bpc: 1>. ) satisfyingV  eV eV  12p+1 + g on ,V(x)  0 on ,. it holds thatu0  u1  . . .  uk  . . .  u  V.(2.15). Initially, it is noted that. V  eV eV  12p+1 + g  eV eV  12p+1 .. We assert that supx V(x)  0. Should this not hold, and V(x0) = supx V(x) > 0 for somex0  , then0  V(x0)  eV(x0) eV(x0)  12p+1 > 0,. resulting in a contradiction and thus establishing the claim.Assuming V  uk, then. (  K)(uk+1  V)  euk (euk  1)2p+1  eV eV  12p+1  K(uk  V). e (e  1)2p (2p + 2)e  1 (uk  V)  K(uk  V). Ke2  1(uk  V)  0,. where the function  satises V    uk  0. This ensures that V  uk+1 by Lemma 2.1.Hence V  u. If u is a another solution, we conclude that u  u, which means that u ismaximal.. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. <image: None, width: 1, height: 1, bpc: 1>. We are now in a position to prove Theorem 1.1. Let i be nite and connected subsetssatisfying. 0  1  . . .  k  . . .,. . i=1i = Zn.. 10. We will use these lemmas to establish Theorem 1.1. For any integers 1  j  k, since  j  k,and noting that uk  0 on. <image: None, width: 1, height: 1, bpc: 1>. j, by (2.15), we conclude. uk  uj on. <image: None, width: 1, height: 1, bpc: 1>. j.. Let uk be the null extension of uk to Zn. Then,. 0  u1  u2  . . .  uk  . . .. on Zn. Noting thatukl2p+2(Zn)  C1 for any k  1, we have the pointwise convergence. uk(x)  u(x),x  Zn,. and u  l2p+2 (Zn). Hence, u satises the equations. u = eu (eu  1)2p+1 + 4M. j=1pj on Zn,. limd(x)+ u(x) = 0,. which constitutes a topological solution.Assume there exists another topological solution u to (1.6). We now prove that u(x)  0 forall x  Zn. If not, there exists x0  Zn such that u(x0) > 0. Noting that limd(x)+ u(x) = 0, weconclude there exists a domain i such that u(x1) = supxi u(x) > 0 for some x1  i. Then, weobtain0  u(x1)  eu(x1) eu(x1)  12p+1 > 0,. which yields a contradiction.Applying (2.15) on  j, we haveu  uj.. For a xed integer k  1, and for j  k, we have. u(x)  lim. <image: None, width: 1, height: 1, bpc: 1>. juj(x) = u(x) on k.. Thus, we obtain u  u on Zn, and the solution u is maximal among all possible solutions.. ACKNOWLEDGMENTSThis work is partially supported by the National Key Research and Development Program ofChina 2020YFA0713100 and by the National Natural Science Foundation of China (Grant No.11721101).. References. [1] Martin T. Barlow. Random walks and heat kernels on graphs, volume 438 of London Mathematical Society LectureNote Series. Cambridge University Press, Cambridge, 2017.[2] Luis A. Caarelli and Yi Song Yang. Vortex condensation in the Chern-Simons Higgs model: an existence theorem.Comm. Math. Phys., 168(2):321336, 1995. 2. [3] Dongho Chae and Namkwon Kim. Topological multivortex solutions of the self-dual Maxwell-Chern-Simons-Higgs system. J. Dierential Equations, 134(1):154182, 1997. 2. 11. [4] Hsungrow Chan, Chun-Chieh Fu, and Chang-Shou Lin. Non-topological multi-vortex solutions to the self-dualChern-Simons-Higgs equation. Comm. Math. Phys., 231(2):189221, 2002. 2. [5] Xiaojun Chang, Ru Wang, and Duokui Yan. Ground states for logarithmic Schrodinger equations on locally nitegraphs. J. Geom. Anal., 33(7):Paper No. 211, 26, 2023. 2. [6] Ruixue Chao and Songbo Hou. Multiple solutions for a generalized Chern-Simons equation on graphs. J. Math.Anal. Appl., 519(1):Paper No. 126787, 16, 2023. 2. [7] Ruixue Chao, Songbo Hou, and Jiamin Sun. Existence of solutions to a generalized self-dual Chern-Simons systemon nite graphs. arXiv preprint arXiv:2206.12863, 2022. 2. [8] Gil Young Cho and Joel E. Moore. Topological BF eld theory description of topological insulators. Ann. Physics,326(6):15151535, 2011. 1. [9] Jia Gao and Songbo Hou. Existence theorems for a generalized Chern-Simons equation on nite graphs. J. Math.Phys., 64(9):Paper No. 091502, 12, 2023. 2. [10] Huabin Ge. Kazdan-Warner equation on graph in the negative case. J. Math. Anal. Appl., 453(2):10221027, 2017.. 2. [11] Huabin Ge. A p-th Yamabe equation on graph. Proc. Amer. Math. Soc., 146(5):22192224, 2018. 2[12] Huabin Ge. The pth Kazdan-Warner equation on graphs. Commun. Contemp. Math., 22(6):Paper No. 1950052,17, 2020. 2. [13] Huabin Ge and Wenfeng Jiang. Yamabe equations on innite graphs. J. Math. Anal. Appl., 460(2):885890, 2018.. 2. [14] Alexander Grigoryan. Introduction to analysis on graphs, volume 71 of University Lecture Series. AmericanMathematical Society, Providence, RI, 2018. 3. [15] Alexander Grigoryan, Yong Lin, and Yunyan Yang. Kazdan-Warner equation on graph. Calc. Var. Partial Dier-ential Equations, 55(4):Art. 92, 13, 2016. 2. [16] Alexander Grigoryan, Yong Lin, and Yunyan Yang. Yamabe type equations on graphs. J. Dierential Equations,261(9):49244943, 2016.[17] Alexander Grigoryan, Yong Lin, and YunYan Yang. Existence of positive solutions to some nonlinear equationson locally nite graphs. Sci. China Math., 60(7):13111324, 2017. 2. [18] Boling Guo and Fangfang Li. Existence of topological vortices in an Abelian Chern-Simons model. J. Math. Phys.,56(10):101505, 10, 2015. 2. [19] Jongmin Han and Hee-Seok Nam. On the topological multivortex solutions of the self-dual Maxwell-Chern-Simonsgauged O(3) sigma model. Lett. Math. Phys., 73(1):1731, 2005. 2. [20] Xiaosen Han. The existence of multi-vortices for a generalized self-dual Chern-Simons model. Nonlinearity,26(3):805835, 2013. 2. [21] Jooyoo Hong, Yoonbai Kim, and Pong Youl Pac. Multivortex solutions of the abelian Chern-Simons-Higgs theory.Phys. Rev. Lett., 64(19):22302233, 1990. 2. [22] Songbo Hou and Xiaoqing Kong. Existence and asymptotic behaviors of solutions to Chern-Simons systems andequations on nite graphs. arXiv preprint arXiv:2211.04237, 2022. 2, 5. [23] Songbo Hou and Wenjie Qiao. Solutions to a generalized ChernSimons Higgs model on nite graphs by topolog-ical degree. J. Math. Phys., 65(8):Paper No. 081503, 2024. 2. [24] Songbo Hou and Jiamin Sun. Existence of solutions to Chern-Simons-Higgs equations on graphs. Calc. Var. PartialDierential Equations, 61(4):Paper No. 139, 13, 2022. 2. [25] Yuanyang Hu. Existence of solutions to a generalized self-dual Chern-Simons equation on nite graphs. J. KoreanMath. Soc., 61(1):133147, 2024. 2. [26] Bobo Hua, Genggeng Huang, and Jiaxuan Wang. The existence of topological solutions to the chern-simons modelon lattice graphs. arXiv preprint arXiv:2310.13905, 2023. 2, 4. [27] An Huang, Yong Lin, and Shing-Tung Yau. Existence of solutions to mean eld equations on graphs. Comm. Math.Phys., 377(1):613621, 2020. 2. [28] Genggeng Huang, Congming Li, and Ximing Yin.Existence of the maximizing pair for the discrete Hardy-Littlewood-Sobolev inequality. Discrete Contin. Dyn. Syst., 35(3):935942, 2015. 7. [29] Hsin-Yuan Huang, Youngae Lee, and Chang-Shou Lin. Uniqueness of topological multi-vortex solutions for askew-symmetric Chern-Simons system. J. Math. Phys., 56(4):041501, 12, 2015. 2. [30] Hsin-Yuan Huang, Jun Wang, and Wen Yang. Mean eld equation and relativistic Abelian Chern-Simons modelon nite graphs. J. Funct. Anal., 281(10):Paper No. 109218, 36, 2021. 2. [31] R. Jackiw and Erick J. Weinberg. Self-dual Chern-Simons vortices. Phys. Rev. Lett., 64(19):22342237, 1990. 2[32] C. Nagraj Kumar and Avinash Khare. Charged vortex of nite energy in nonabelian gauge theories with Chern-Simons term. Phys. Lett. B, 178(4):395399, 1986. 1. [33] Jiayu Li, Linlin Sun, and Yunyan Yang. Topological degree for Chern-Simons Higgs models on nite graphs. Calc.Var. Partial Dierential Equations, 63(4):Paper No. 81, 21, 2024. 2. [34] Chang-Shou Lin and Jyotshana V. Prajapat. Vortex condensates for relativistic abelian Chern-Simons model with12. two Higgs scalar elds and two gauge elds on a torus. Comm. Math. Phys., 288(1):311347, 2009. 1. [35] Chang-Shou Lin and Yisong Yang. Non-Abelian multiple vortices in supersymmetric eld theory. Comm. Math.Phys., 304(2):433457, 2011. 1. [36] Yong Lin and Yiting Wu. The existence and nonexistence of global solutions for a semilinear heat equation ongraphs. Calc. Var. Partial Dierential Equations, 56(4):Paper No. 102, 22, 2017. 2. [37] Yang Liu and Yunyan Yang. Topological degree for Kazdan-Warner equation in the negative case on nite graph.Ann. Global Anal. Geom., 65(4):Paper No. 29, 20, 2024. 2. [38] Yang Liu and Mengjie Zhang. A heat ow with sign-changing prescribed function on nite graphs. J. Math. Anal.Appl., 528(2):Paper No. 127529, 17, 2023. 2. [39] Yingshu Lu and Peirong Zhong. Existence of solutions to a generalized self-dual Chern-Simons equation on graphs.arXiv preprint arXiv:2107.12535, 2021. 2. [40] Shoudong Man. On a class of nonlinear Schrodinger equations on nite graphs. Bull. Aust. Math. Soc., 101(3):477487, 2020. 2. [41] Kwan Hui Nam. Vortex condensation in U(1)  U(1) Chern-Simons model with a general Higgs potential on atorus. J. Math. Anal. Appl., 407(2):305315, 2013. 1. [42] Alessio Porretta. A note on the Sobolev and Gagliardo-Nirenberg inequality when p > N. Adv. Nonlinear Stud.,20(2):361371, 2020. 7. [43] Zidong Qiu and Yang Liu. Existence of solutions to the nonlinear Schrodinger equation on locally nite graphs.Arch. Math. (Basel), 120(4):403416, 2023. 2. [44] S. Randjbar-Daemi, A. Salam, and J. Strathdee. Chern-Simons superconductivity at nite temperature. NuclearPhys. B, 340(2-3):403447, 1990. 1. [45] Gordon W. Semeno and Pasquale Sodano. Nonabelian adiabatic phases and the fractional quantum Hall eect.Phys. Rev. Lett., 57(10):11951198, 1986. 1. [46] Joel Spruck and Yi Song Yang. Topological solutions in the self-dual Chern-Simons theory: existence and approx-imation. Ann. Inst. H. Poincare C Anal. Non Lineaire, 12(1):7597, 1995. 4. [47] Linlin Sun and Liuquan Wang. Brouwer degree for Kazdan-Warner equations on a connected nite graph. Adv.Math., 404:Paper No. 108422, 29, 2022. 2. [48] Gabriella Tarantello.Multiple condensate solutions for the Chern-Simons-Higgs theory.J. Math. Phys.,37(8):37693796, 1996. 2. [49] Guofang Wang and Liqun Zhang. Non-topological solutions of the relativistic SU(3) Chern-Simons Higgs model.Comm. Math. Phys., 202(3):501515, 1999. 2. [50] Ning Zhang and Liang Zhao. Convergence of ground state solutions for nonlinear Schrodinger equations on graphs.Sci. China Math., 61(8):14811494, 2018. 2. [51] Xiaoxiao Zhang and Yanxun Chang. p-th Kazdan-Warner equation on graph in the negative case. J. Math. Anal.Appl., 466(1):400407, 2018. 2. [52] Xiaoxiao Zhang and Aijin Lin. Positive solutions of p-th Yamabe type equations on innite graphs. Proc. Amer.Math. Soc., 147(4):14211427, 2019. 2. 13", "2410.18410v1.FreCaS__Efficient_Higher_Resolution_Image_Generation_via_Frequency_aware_Cascaded_Sampling.pdf": "FRECAS: EFFICIENT HIGHER-RESOLUTION IMAGEGENERATION VIA FREQUENCY-AWARECASCADED SAMPLING. Zhengqiang Zhang1,2, Ruihuang Li1,2, Lei Zhang1,2,. 1The Hong Kong Polytechnic University2OPPO Research Institutezhengqiang.zhang@connect.polyu.hk, cslzhang@comp.polyu.edu.hk. ABSTRACT. While image generation with diffusion models has achieved a great success, gen-erating images of higher resolution than the training size remains a challengingtask due to the high computational cost. Current methods typically perform theentire sampling process at full resolution and process all frequency componentssimultaneously, contradicting with the inherent coarse-to-fine nature of latent dif-fusion models and wasting computations on processing premature high-frequencydetails at early diffusion stages. To address this issue, we introduce an efficientFrequency-aware Cascaded Sampling framework, FreCaS in short, for higher-resolution image generation. FreCaS decomposes the sampling process into cas-caded stages with gradually increased resolutions, progressively expanding fre-quency bands and refining the corresponding details.We propose an innova-tive frequency-aware classifier-free guidance (FA-CFG) strategy to assign differ-ent guidance strengths for different frequency components, directing the diffu-sion model to add new details in the expanded frequency domain of each stage.Additionally, we fuse the cross-attention maps of previous and current stages toavoid synthesizing unfaithful layouts. Experiments demonstrate that FreCaS sig-nificantly outperforms state-of-the-art methods in image quality and generationspeed. In particular, FreCaS is about 2.86 and 6.07 faster than ScaleCrafterand DemoFusion in generating a 20482048 image using a pre-trained SDXLmodel and achieves an FIDb improvement of 11.6 and 3.7, respectively. FreCaScan be easily extended to more complex models such as SD3. The source code ofFreCaS can be found at https://github.com/xtudbxk/FreCaS.. 1INTRODUCATION. In recent years, diffusion models, such as Imagen (Saharia et al., 2022), SDXL (Podell et al., 2023),PixelArt- (Chen et al., 2023) and SD3 Esser et al. (2024), have achieved a remarkable success ingenerating high-quality natural images. However, these models face challenges in generating veryhigh resolution images due to the increased complexity in high-dimensional space. Though efficientdiffusion models, including ADM (Dhariwal & Nichol, 2021), CascadedDM (Ho et al., 2022) andLDM (Rombach et al., 2022), have been developed, the computational burden of training diffusionmodels from scratch for high-resolution image generation remains substantial. As a result, populardiffusion models, such as SDXL (Podell et al., 2023) and SD3 (Esser et al., 2024), primarily focuson generating 1024  1024 resolution images. It is thus increasingly attractive to explore training-free strategies for generating images at higher resolutions, such as 2048  2048 and 4096  4096,using pre-trained diffusion models.. MultiDiffusion (Bar-Tal et al., 2023) is among the first works to synthesize higher-resolution imagesusing pre-trained diffusion models. However, it suffers from issues such as object duplication, whichlargely reduces the image quality. To address these issues, Jin et al. (2024) proposed to manuallyadjust the scale of entropy in the attention operations. He et al. (2023) and Linjiang Huang (2024)attempted to enlarge the receptive field by replacing the original convolutional layers with stridedones, while Zhang et al. (2023) explicitly resizes the intermediate feature maps to match the train-. Corresponding author.. 1. arXiv:2410.18410v1  [cs.CV]  24 Oct 2024. ing size. Du et al. (2024) and Lin et al. (2024) took a different strategy by generating a referenceimage at the base resolution and then using it to guide the whole sampling process at higher resolu-tions. Despite the great advancements, these methods still suffer from significant inference latency,hindering their broader applications in real world.. In this paper, we propose an efficient Frequency-aware Cascaded Sampling framework, namelyFreCaS, for training-free higher-resolution image generation. Our proposed FreCaS framework isbased on the observation that latent diffusion models exhibit a coarse-to-fine generation manner inthe frequency domain. In other words, they first generate low-frequency contents in early diffusionstages and gradually generate higher-frequency details in later stages. Leveraging this insight, wegenerate higher-resolution images through multiple stages of increased resolutions, progressivelysynthesizing details of increased frequencies. FreCaS avoids unnecessary computations during theearly diffusion stages as high-frequency details are not yet required.. In the latent space, the image representation expands its frequency range as the resolution increases.To encourage detail generation within the expanded frequency band, we introduce a novel frequency-aware classifier-free guidance (FA-CFG) strategy, which prioritizes newly introduced frequencycomponents by assigning them higher guidance strengths in the sampling process. Specifically,we decompose both unconditional and conditional denoising scores into two parts: low-frequencycomponent, which captures content from earlier stages, and high-frequency component, which cor-responds to the newly increased frequency band. FA-CFG applies the classifier-free guidance todifferent frequency components with different strengths, and outputs the final denoising score bycombining the adjusted components. The FA-CFG strategy can synthesize much clear details whilemaintaining computational efficiency. Additionally, to alleviate the issue of unfaithful layouts, suchas duplicated objects mentioned in Jin et al. (2024), we reuse the cross-attention maps (CA-maps)from the previous stage, which helps maintaining consistency in image structure across differentstages and ensuring more faithful object representations.. In summary, our main contributions are as follows:. We propose FreCaS, an efficient frequency-aware cascaded sampling framework fortraining-free higher-resolution image generation. FreCaS leverages the coarse-to-fine na-ture of the latent diffusion process, thereby reducing unnecessary computations associatedwith processing premature high-frequency details.. We design a novel FA-CFG strategy, which assigns different guidance strengths to compo-nents of different frequencies. This strategy enables FreCaS to focus on generating contentsof newly introduced frequencies in each stage, and hence synthesize clearer details. In ad-dition, we fuse the CA-maps of previous stage and current stage to maintain a consistentimage layouts across stages.. We demonstrate the efficiency and effectiveness of FreCaS through extensive experimentsconducted on various pretrained diffusion models, including SD2.1, SDXL and SD3, vali-dating its broad applicability and versatility.. 2RELATED WORKS. 2.1DIFFUSION MODELS. Diffusion models have gained significant attentions due to their abilities to generate high-qualitynatural images. Ho et al. (2020) pioneered the use of a variance-preserving diffusion process tobridge the gap from natural images to pure noises. Dhariwal & Nichol (2021) exploited various net-work architectures and achieved superior image quality than contemporaneous GAN models. Ho &Salimans (2022) introduced a novel classifier-free guidance strategy that attains both generated im-age quality and diversity. However, the substantial model complexity makes high-resolution imagesynthesis challenging. Ho et al. (2022) proposed a novel cascaded framework that progressively in-creases image resolutions. Rombach et al. (2022) performed the diffusion process in the latent spaceof a pre-trained autoencoder, enabling high-resolution image synthesis with reduced computationalcost. (Esser et al., 2024) presented SD3, which employs the rectified flow matching (Lipman et al.,2022; Liu et al., 2022) at the latent space and demonstrates superior performance. Despite the greatprogress, it still requires substantial efforts to train a high-resolution diffusion model from scratch.Therefore, training-free higher-resolution image synthesis attracts increasing attentions.. 2. 2.2TRAINING-FREE HIGHER-RESOLUTION IMAGE SYNTHESIS. A few methods have been developed to leverage pre-trained diffusion models to generate imagesof higher resolutions than the training size. MultiDiffusion (Bar-Tal et al., 2023) is among thefirst methods to bind multiple diffusion processes into one unified framework and generates seam-less higher-resolution images. However, the results exhibit unreasonable image structures such asduplicated objects. AttnEntropy (Jin et al., 2024) alleviates this problem by re-normalizing the en-tropy of attention blocks during sampling. On the other hand, ScaleCrafter (He et al., 2023) andFouriScale (Linjiang Huang, 2024) expand the receptive fields of pre-trained networks to matchhigher inference resolutions, thereby demonstrating improved image quality. HiDiffusion (Zhanget al., 2023) dynamically adjusts the feature sizes to match the training dimensions. DemoFu-sion (Du et al., 2024) and AccDiffusion (Lin et al., 2024) first generate a reference image at thestandard resolution and then use this image to guide the generation of images at higher resolutions.Despite their success, the above mentioned approaches neglect the coarse-to-fine nature of imagegeneration and generate image contents of all frequencies simultaneously, resulting in long infer-ence latency and limiting their broader applications.. To address this issue, we propose an efficient FreCaS framework for training-free higher-resolutionimage synthesis. FreCaS divides the entire sampling process into stages of increasing resolutions,gradually synthesizing components of different frequency bands, thereby reducing the unnecessarycomputation of handling premature high-frequency details in early sampling stages. It is worthnoting that DemoFusion (Du et al., 2024) employs a progressive upscaling scheme that may appearsimilar to our approach. However, there exist fundamental differences: DemoFusion utilizes thegenerated image at the base resolution as a reference to guide the entire sampling process at targetresolution, whereas FreCaS continues the sampling process at higher resolutions. This distinctionmakes our method significantly more efficient than DemoFusion, achieving better image qualitywith largely reduced computations.. 3METHOD. This section presents the details of the proposed FreCaS framework, which leverages the coarse-to-fine nature of latent diffusion models and constructs a frequency-aware cascaded sampling strategyto progressively refine high-frequency details. We first introduce the notations and concepts thatform the basis of our approach (see Section 3.1). Then, we delve into the key components of ourmethod: FreCaS framework (see Section 3.2), FA-CFG strategy (see Section 3.3), and CA-mapsre-utilization (see Section 3.4).. 3.1PRELIMINARIES. Diffusion models. Diffusion models (Ho et al., 2020; Dhariwal & Nichol, 2021) transform compleximage distributions into the Gaussian distribution, and vice versa. They gradually inject Gaussiannoises into the image samples, and then use a reverse process to remove noises from them, achievingimage generation. Most recent diffusion models operate in the latent space and utilize a discretetimestep sampling process to synthesize images. Specifically, for a T-step sampling process, alatent noise zT is drawn from a standard Gaussian distribution, and then iteratively refined througha few denoising steps until converged to the clean signal latent z0. Finally, the natural image x isdecoded from z0 using a decoder D. The whole process can be written as follows:. zT  N(0, I)  zT 1      z1  z0  x = D(z0).(1). For each denoising step, current works typically adopt the classifier-free guidance (CFG) (Ho &Salimans, 2022) to improve image quality. It predicts an unconditional denoising score unc and aconditional denoising score c. The final denoising score is obtained via a simple extra-interpolationprocess as  = (1  w)  unc + w  c, where w denotes the guidance strength.. Resolution and frequency range.The resolution of a latent z determines its sampling fre-quency (Rissanen et al., 2023), thereby influencing its frequency domain characteristics. Specifi-cally, if a latent of unit length has a resolution of s  s, its sampling frequency fs can be definedas the number of samples per unit length, which is s. The Nyquist frequency is then obtained as. 3. <image: DeviceRGB, width: 3397, height: 874, bpc: 8>. power spectral density (PSD). Low Freq.High Freq.(a) z900(b) z600(c) z300(d) z0. PSD of latents. PSD of noise parts. PSD of signal parts. Figure 1: From (a) to (d), the sub-figures show the PSD curves of latents z900, z600, z300 and z0of SDXL, respectively. One can see that the energy of synthesized clean signals (the red slashedregions) first emerges in the low-frequency band and gradually expands to high-frequency band.. fs2 =s2. Therefore, the frequency of the latent z ranges from [0, s. 2]. Reducing its resolution tosl  sl narrows the frequency range to [0, sl. 2 ]. As a result, higher resolutions capture a broaderfrequency domain, while lower resolutions lead to a narrower frequency spectrum.. 3.2FREQUENCY-AWARE CASCADED SAMPLING. Pixel space diffusion models exhibit a coarse-to-fine behavior in the image synthesis process (Rissa-nen et al., 2023; Teng et al., 2024). In this section, we show that such a behavior is also exhibited forlatent diffusion models during the sampling process, which inspires us to develop a frequency-awarecascaded sampling framework for generating higher-resolution images.. PSD curves in latent space. The power spectral density (PSD) is a powerful tool for analyzing theenergy distribution of signals along the frequency spectrum. Rissanen et al. (2023) and Teng et al.(2024) have utilized PSD to study the behaviour of intermediate states in the pixel diffusion process.Here, we compute the PSD of the latent signals over a collection of 100 natural images using thepre-trained SDXL model (Podell et al., 2023). Figure 1 shows the PSD curves of z900, z600, z300 andz0. The solid line denotes the PSD curve of intermediate noise corrupted latent, while the dashedline represents the PSD of Gaussian noise corrupted into the latent. The inner area between the twocurves (marked with red slashes) indicates the energy of clean signal latent being synthesized. Onecan see that the clean image signals emerge from the low-frequency band (see z900 and z600) andgradually expand to the high-frequency band (see z300 and z0) during the sampling process. Theseobservations confirm the coarse-to-fine nature of image synthesis in the latent diffusion process,where low-frequency content is generated first, followed by high-frequency details.. Framework of FreCaS. Based on the above observation, we developed an efficient FreCaS frame-work to progressively generate image contents of higher frequency bands, reducing unnecessarycomputations in processing premature high-frequency details in early diffusion stages. As shown inFigure 2(a), our FreCaS divides the entire T-step sampling process into N + 1 stages of increasingresolutions. The initial stage performs the sampling process at the default training size s0 with afrequency range of [0, s0. 2 ]. Each of the subsequent stages increases the sampling size to its prede-cessor, gradually expanding the frequency domain. At the final stage, the latent reaches the targetresolution sN, achieving a full frequency range from 0 to sN. 2 .. Specifically, we begin with a pure noise latent zs0T at stage s0, and iteratively perform reverse sam-pling until obtaining the last latent in this stage, denoted by zs0L . Next, we transition zs0L to the firstlatent, denoted by zs1F , in next stage, as illustrated by the blue dashed arrow in Figure 2(a). This pro-cedure is repeated until the latent feature reaches the target size, resulting in zsN0 . The final imagex is obtained by applying the decoder to zsN0so that x = D(zsN0 ). With such a sampling pipeline,FreCaS ensures a gradual refinement of details across coarse-to-fine scales, ultimately producing ahigh-quality and high-resolution image with minimum computations.. For the transition between two adjacent stages, we perform five steps to convert the last latent ofprevious stage zsi1Lto the first latent of next stage zsiF :. zsi1Ldenoise zsi10decode xsi1interpolate xsiencode zsi0diffuse zsiF ,(2). 4. <image: DeviceRGB, width: 8192, height: 6470, bpc: 8>. CA-maps. CA-maps. Stage 0. zs0Tzs0L. CFG. Stage 1. zs1Fzs1L. FA-CFG. Stage i. Stage N. zsNFzsN0. FA-CFGD. PSD along Freq.. PSD along Freq.. PSD along Freq.. (a) FreCaS Framework. PSD curvesFreq. bands ofprevious stages. Expanded Freq.bands. Sample process. Transition process. zsiF , zsiL. The first/last stateof stage i. CA-maps reuse. zt. text. Uncond Net. Cond Net. unc. c. hunc. hc. lunc. lc. 1-wh. wh. 1-wl. wl. h. l. High Freq. part. Low Freq. part. CFG. Freq. decomposeor Freq. combine(b) FA-CFG. Figure 2: (a) The overall framework of FreCaS. The entire T-step sampling process is divided intoN + 1 stages of increasing resolutions and expanding frequency bands. FreCaS starts the samplingprocess at the training size and obtains the last latent zs0L at that stage. Then, FreCaS continues thesampling from the first latent zs1F at the next stage with a larger resolution and expanded frequencydomain. This procedure is repeated until the final latent zsN0at stage N is obtained. A decoderis then used to generate the final image. (b) FA-CFG strategy. We separate the original denoisingscores into low-frequency and high-frequency components and assign a higher CFG strength to thehigh-frequency part. The two parts are then combined to obtain the final denoising score .. where denoise and diffuse are standard diffusion operations, decode and encode are per-formed using the decoder and encoder, respectively, and interpolation adjusts the resolutions usingthe bilinear interpolation. To determine the timestep of zsiF , we follow previous works (Hoogeboomet al., 2023; Chen, 2023; Gu et al., 2023; Teng et al., 2024) to keep the signal-to-noise ratio (SNR)equivalence between zsi1Land zsiF . Please refer to Appendix A for more details.. 3.3FA-CFG STRATEGY. Our FreCaS framework progressively transitions the latents to stages with higher resolutions and ex-tended high-frequency bands. To ensure that the diffusion models focus more on generating contentsof newly introduced frequencies, we propose a novel FA-CFG strategy, which assigns higher guid-ance strength to the new frequency components. In FreCaS, upon transitioning to stage si, the latentincreases its resolution from si1 to the higher resolution si, thereby expanding the frequency bandfrom [0, si1. 2 ] to [0, si. 2 ]. This inspires us to divide the latents into two components: a low-frequencycomponent ranging from [0, si1. 2 ] and a high-frequency component covering the frequency interval( si1. 2 , si. 2 ]. The former preserves the generated contents from previous stages, whereas the latter isreserved for the contents to be generated in this stage. Our goal is to encourage the diffusion modelsto generate natural details and textures in the newly expanded frequency band.. To achieve the above mentioned goal, we propose to perform CFG on the two frequency-aware partswith different guidance strengths. The entire process is illustrated in Figure 2(b). First, we obtainthe unconditional denoising score unc and conditional denoising score c using the pre-traineddiffusion network. Then, we split the scores into a low-frequency part and a high-frequency part.The former is extracted by downsampling the scores and then resizing them back, while the latter is. 5. the residual by subtracting the low-frequency part from the original denoising scores. Subsequently,we apply the CFG strategy to the two parts with different weights. Specifically, for the low-frequencypart, we assign the normal guidance strength wl, while for the high-frequency part, we use a muchhigher weight wh to prioritize content generation in this frequency band. The final denoising scoreis obtained by summing up the two parts. This process can be expressed as:. = l + h = (1  wl)  lunc + wl  lc + (1  wh)  hunc + wh  hc ,(3). where l and h are the low-frequency and high-frequency parts of , respectively. Similarly, lunc,hunc, lc and hc follow the same notation.. 3.4CA-MAPS REUTILIZATION. When applied to higher resolutions, pre-trained diffusion models often present unreasonable imagestructures, such as duplicated objects. To address this issue, we propose to reuse the CA-maps fromthe previous stage to maintain layout consistency across stages. The CA-maps represent attentionweights from cross-attention interactions between spatial features and textual embeddings, effec-tively capturing the semantic layout of the generated images. Specifically, we average the CA-mapsof all cross-attention blocks when predicting zsi1Lat stage si1. After transitioning to stage si,we replace the current CA-maps of each cross-attention block using its linear interpolation with theaveraged CA-maps Msi1Las follows:. M sit= (1  wc)  M sit + wc  Msi1L,(4). where M sit is the CA-maps at step t of stage si. In this way, FreCaS can effectively maintain contentconsistency and prevent unexpected objects or textures during higher-resolution image generation.. 4EXPERIMENTS. 4.1EXPERIMENTAL SETTINGS. Implementation details. We evaluate FreCaS on three widely-used pre-trained diffusion models:SD2.1 (Rombach et al., 2022), SDXL (Podell et al., 2023) and SD3 (Esser et al., 2024). The sizesof generated images are 4 and 16 the original training size. Specifically, we generate images of10241024 and 20482048 for SD2.1, while 20482048 and 40964096 for SDXL. For SD3, weonly generate images of 2048  2048 due to the GPU memory limitation. We randomly select 10K,5K, and 1K prompts from the LAION5B aesthetic subset for generating images of 10241024,20482048, and 40964096, respectively. We follow the default settings and perform a 50-stepsampling process with DDIM sampler for SD2.1 and SDXL, and perform a 28-step sampling processwith a flow matching based Euler solver for SD3. For 4 experiments, we employ two samplingstages at the training size and target size, respectively. For 16 experiments, we employ threesampling stages at the training size, 4 training size and 16 training size, respectively. Moredetails can be found in Appendix B.. Evaluation metrics. We leverage the Frechet Inception Distance (FID) (Heusel et al., 2017) and In-ception Score (IS) (Salimans et al., 2016) to measure the quality of generated images. Following Heet al. (2023), the FID is computed between the samples of training size and target size, denoted asFIDb. As suggested by Du et al. (2024), we further report FIDp and ISp, which compute the metricsat patch level, to better evaluate the image details. The CLIP score (Radford et al., 2021) is utilizedto measure the text prompt alignment of generated images. As in previous works (Zhang et al.,2023), we measure the model latency on a single NVIDIA A100 GPU with a batch size of 1. Wegenerate five images and report the averaged latency of the last three images for all methods.. 4.2EXPERIMENTS ON SD2.1 AND SDXL. For experiments on SD2.1, we compare FreCaS with DirectInference, MultiDiffusion (Bar-Tal et al.,2023), AttnEntropy (Jin et al., 2024), ScaleCrafter (He et al., 2023), FouriScale (Linjiang Huang,2024) and HiDiffusion (Zhang et al., 2023). For experiments on SDXL, we compare with Direct-Inference, AttnEntropy, ScaleCrafter, FouriScale, HiDiffusion, AccDiffusion (Lin et al., 2024) andDemoFusion (Du et al., 2024).. 6. Table 1: Experiments on 4 and 16 generation of SD2.1 and SDXL. DO means duplicatedobject, which indicates whether the method takes the duplicated object problem into considera-tion. SpeedUP denotes the efficiency speed-up over the DirectInference baseline. The red andblue indicate the best and second one among all methods that consider the duplicated object problem.. MethodsDO FIDbFIDpISISpCLIPSCORELatency(s) SpeedUP. SD2.1. 4. DirectInference34.5423.8415.0017.2632.015.501xMultiDiffusion22.4414.6817.4618.2932.49120.210.046. AttnEntropy30.6321.3415.6717.7132.285.560.99ScaleCrafter13.1822.4417.4216.2932.886.360.86FouriScale15.3323.2617.1115.5732.9211.060.50HiDiffusion16.2125.2617.1316.1232.373.571.54. Ours13.1421.2317.5516.0432.332.562.16. 16. DirectInference128.350.238.8415.3027.6749.271MultiDiffusion74.1515.288.7518.8231.14926.330.05. AttnEntropy127.646.529.3116.2528.3349.331.00ScaleCrafter34.5557.4713.0212.1231.4492.860.53FouriScale34.1358.0112.7913.1531.6890.130.55HiDiffusion34.1770.5813.4911.8731.0918.222.70. Ours20.1143.7115.2213.7431.9213.353.69. SDXL. 4. DirectInference43.8329.7111.5214.6032.5134.101. AttnEntropy41.3027.6711.6915.0432.7134.360.99ScaleCrafter24.2323.1714.1014.9732.7039.640.86FouriScale26.8827.2413.9714.4432.9066.180.52HiDiffusion20.6921.8015.5615.9332.6218.381.86. AccDiffusion17.6221.1117.0716.1532.66102.460.33DemoFusion16.3318.7717.1017.2133.1683.950.41. Ours12.6317.9117.1817.3133.2813.842.46. 16. DirectInference151.362.396.4111.6628.24312.361. AttnEntropy148.960.546.4612.4428.46312.461.00ScaleCrafter75.1173.218.689.8130.76560.910.56FouriScale77.6384.058.009.4130.78534.080.58HiDiffusion83.41120.19.799.5629.18101.593.07. AccDiffusion48.1546.0712.1111.7532.26763.230.41DemoFusion44.5435.5212.3813.8233.03649.250.48. Ours40.6339.8212.6814.1633.0385.873.64. Quantitative results. Table 1 presents quantitative comparisons for 4 and 16 generation be-tween FreCaS and its competitors. We can see that FreCaS not only outperforms other methods onsynthesized image quality but also exhibits significantly faster inference speed. In specific, FreCaSachieves the best FIDb scores in all experiments of SD2.1 and SDXL, achieving clear advantagesover the other methods. In terms of the IS metric, FreCaS performs the best in most cases, onlyslightly lagging behind DemoFusion on the 16 experiment of SDXL. (Note that DirectInferenceand MultiDiffusion occasionally achieve higher FIDp and ISp scores because they disregard the is-sue of duplicated objects.) For CLIP score, FreCaS obtains the best results on 3 out of the 4 cases,except for the less challenging 4 generation with SD2.1.. While having superior image quality metrics, FreCaS demonstrates impressive efficiency. It showsmore than 2 speedup over DirectInference on 4 generation experiments, and shows more than3.6 speedup on the 16 generation experiments. DemoFusion, which is overall the second bestmethod in terms of image quality, is significantly slower than FreCaS. Its latency is about 6 and7.5 longer than FreCas on 4 and 16 experiments, respectively. On the other hand, HiDiffusion,which is the second faster method, sacrifices image quality for speed. For example, on the 16. 7. <image: DeviceRGB, width: 8192, height: 8192, bpc: 8>. 4 on SD2.116 on SD2.1. DirectInferenceMultiDiffusionAttnEntropyScaleCrafterHiDiffusionFreCaS(ours). DirectInferenceMultiDiffusionAttnEntropyScaleCrafterHiDiffusionFreCaS(ours). <image: DeviceRGB, width: 8192, height: 8192, bpc: 8>. 4 on SDXL16 on SDXL. ScaleCrafterFouriScaleHiDiffusionAccDiffusionDemoFusionFreCaS(ours). ScaleCrafterFouriScaleHiDiffusionAccDiffusionDemoFusionFreCaS(ours)Figure 3: Visual comparison on 4 and 16 experiments of SD2.1 and SDXL. From top to bottom,the prompts used in the four groups of examples are: 1. A cosmic traveler, floating in zero gravity,spacesuit reflecting the Earth below, stars twinkling in the distance. 2. A fierce Viking, axe inhand, leading a raid, the longship slicing through the waves. 3. A bustling flower market, stallsfilled with bouquets, the air thick with fragrance, people selecting their favorites. 4. Tokyo JapanRetro Skyline, Airplane, Railroad Train, Moon etc. - Modern Postcard. Zoom-in for better view.. 8. <image: DeviceRGB, width: 4080, height: 1177, bpc: 8>. Figure 4: Ablation studies on wl and wh in FA-CFG strategy and wc in CA-maps reutilization.. <image: DeviceRGB, width: 3205, height: 1289, bpc: 8>. Figure 5: Visual results of adjusting wh in the FA-CFG strategy. From top to bottom, the promptsare Eccentric Shaggy Woman with Pet - Little Puppy and Rabat Painting - Mdina Poppies Maltaby Richard Harpum, respectively.. experiment with SD2.1, HiDiffusion achieves a latency of 18.22s but its FIDb score is 34.17. Incontrast, FreCaS is faster (13.35s) and has a much better FIDb score (20.11).. Qualitative results. Figure 3 illustrates visual comparisons between FreCaS and competitive ap-proaches. From top to bottom are four groups of examples, presenting the results of 4 generationof SD2.1, 16 generation of SD2.1, 4 generation of SDXL, and 16 generation of SDXL, re-spectively. In each group, the top row shows the generated images, while the bottom row shows thezoomed region for better observation. From Figure 3, we can see that FreCaS effectively synthesizesthe described contents while maintaining a coherent scene structure. DirectInference, MultiDiffu-sion and AttnEntropy often produce duplicated objects, such as the many astronauts and warriors.ScaleCrafter and HiDiffusion achieve reasonable image contents in experiments of SD2.1 but gener-ate unnatural layouts in the experiments of SDXL, such as the excessive flowers on the ceiling in 4experiment. Our FreCaS consistently maintains coherent image contents and layout in experimentsof both SD2.1 and SDXL. AccDiffusion and DemoFusion also achieve natural image contents, butFreCaS generates clearer details such as the flowers and trains. Please refer to Appendix C for morevisual results, including images with other aspect ratios.. 4.3EXPERIMENTS ON SD3. SD3 (Esser et al., 2024) adopts a rather different network architecture from SD2.1 and SDXL, andmany existing methods cannot be applied. We can only compare FreCaS with DirectInference andDemoDiffusion Du et al. (2024). Due to page limitation, please refer to Appendix D for the results.. 4.4ABLATION STUDIES. In this section, we conduct ablation studies on the 4 experiments of SDXL to investigate thesettings of the FA-CFG strategy, the CA-maps re-utilization and the inference schedule of FreCaS.. FA-CFG strategy. The FA-CFG strategy aims to guide the model to generate content within theexpanded frequency band. To achieve this, FA-CFG introduces two parameters, wl and wh, toadjust the guidance strength on the low and high frequency components, respectively. When wl =. 9. <image: DeviceRGB, width: 5030, height: 2123, bpc: 8>. Figure 6: Visual results on adjusting wc in CA-maps reutilization. The prompt is Blueberries andStrawberries Art Print.wh, the FA-CFG strategy degenerates to the conventional CFG approach. We conduct a series ofexperiments to explore the optimal settings of the two parameters. First, we fix wl at 7.5 and varywh. The results are shown in Figure 4(a). We observe that as wh increases from 1.0 to 45, the FIDband FIDp metrics initially decrease, indicating improved image quality. However, as wh becomestoo high, the metrics begin to deteriorate. The sweet spot lies between 25 and 35, achieving a lowFIDb of nearly 12.65 and a low FIDp of 17.91. We then fix wh at 35 and vary wl. The resultsare presented in Figure 4(b). Reducing wl below 7.5 leads to a slight increase in FIDp from 17.91to 18.06, whereas increasing wl over 7.5 deteriorates FIDr from 12.81 to 13.00. Compared towh, adjusting wl brings much smaller effects on those two metrics. Thus, we set wl to 7.5 forexperiments on SD2.1 and SDXL, and set it to 7.0 for SD3.. Figure 5 provides visual examples of adjusting wh. Increasing wh enhances the sharpness of details,such as clearer hair strands and more vivid flower petals. However, an excessively high value of wh(e.g., 45) will introduce artifacts, as highlighted by the red boxes in the figure. This underscoresthe importance of selecting an appropriate wh value to strike a balance between detail enhancementand artifact suppression. Based on these findings, we set wl to 7.5 and wh to 35 yields favorableoutcomes in most of the cases.. CA-maps re-utilization. To evaluate the effect of weight wc in the re-utilization of CA-maps, weconduct an ablation study by varying wc from 0 to 1. The results are shown in Figure 4(c). Increasingwc continuously decreases FIDb but increases FIDp, indicating an improvement on the image layoutbut a drop on image details. To balance between the two metrics, we set wc = 0.6. A visual exampleis shown in Figure 6. We see that this setting leads to a clearer textures on strawberry compared towc = 1.0 and prevents the unreasonable surface of the blueberry in wc = 0.0.. Inference schedule. FreCaS uses two factors to adjust the inference schedule. The first one is thecount of additional stages N. The second factor is the timestep L of last latent in each stage. Weconduct experiments on the selection of these two factors. The details can be found in Appendix E.Based on results, we set L to 200, and set N to 2 for 4 experiments and 3 for 16 experiments.. 5CONCLUSION. We developed a highly efficient Frequency-aware Cascaded Sampling framework, namely Fre-CaS, for training-free higher-resolution image generation. FreCaS leveraged the coarse-to-fine na-ture of latent diffusion process, reducing unnecessary computations in processing premature high-frequency details. Specifically, we divided the entire sampling process into several stages havingincreasing resolutions and expanding frequency bands, progressively generating image contents ofhigher frequency details. We presented a Frequency-Aware Classifier-Free Guidance (FA-CFG)strategy to enable diffusion models effectively adding details of the expanded frequencies, leadingto clearer textures. In addition, we fused the cross-attention maps of previous stages and currentone to maintain consistent image layouts across stages. FreCaS demonstrated advantages over pre-vious methods in both image quality and efficiency. In particular, with SDXL, it can generate a highquality 4096  4096 resolution image in 86 seconds on an A100 GPU.. 10. REFERENCES. Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths forcontrolled image generation. 2023.. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, JamesKwok, Ping Luo, Huchuan Lu, et al. Pixart-\\alpha: Fast training of diffusion transformer forphotorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.. Ting Chen.On the importance of noise scheduling for diffusion models.arXiv preprintarXiv:2301.10972, 2023.. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advancesin neural information processing systems, 34:87808794, 2021.. Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion:Democratising high-resolution image generation with no $$$. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 61596168, 2024.. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, YamLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers forhigh-resolution image synthesis. In Forty-first International Conference on Machine Learning,2024.. Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel Angel Bautista, and Joshua M Susskind. f-dm: Amulti-stage diffusion model via progressive signal transformation. In The Eleventh InternationalConference on Learning Representations, 2023.. Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, XintaoWang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higher-resolution visualgeneration with diffusion models. In The Twelfth International Conference on Learning Repre-sentations, 2023.. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances inneural information processing systems, 30, 2017.. Jonathan Ho and Tim Salimans.Classifier-free diffusion guidance.arXiv preprintarXiv:2207.12598, 2022.. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances inneural information processing systems, 33:68406851, 2020.. Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Sali-mans. Cascaded diffusion models for high fidelity image generation. Journal of Machine LearningResearch, 23(47):133, 2022.. Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion forhigh resolution images. In International Conference on Machine Learning, pp. 1321313232.PMLR, 2023.. Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Training-free diffusion model adaptation forvariable-sized text-to-image synthesis. Advances in Neural Information Processing Systems, 36,2024.. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Ad-vances in neural information processing systems, 34:2169621707, 2021.. Zhihang Lin, Mingbao Lin, Meng Zhao, and Rongrong Ji. Accdiffusion: An accurate method forhigher-resolution image generation. arXiv preprint arXiv:2407.10738, 2024.. Aiping Zhang Guanglu Song Si Liu Yu Liu Hongsheng Li Linjiang Huang, Rongyao Fang.Fouriscale: A frequency perspective on training-free high-resolution image synthesis. arxiv, 2024.. 11. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matchingfor generative modeling. arXiv preprint arXiv:2210.02747, 2022.. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate andtransfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022.. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, JoePenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution imagesynthesis. arXiv preprint arXiv:2307.01952, 2023.. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International conference on machine learning, pp.87488763. PMLR, 2021.. Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissi-pation. In The Eleventh International Conference on Learning Representations, 2023.. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-ence on computer vision and pattern recognition, pp. 1068410695, 2022.. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistictext-to-image diffusion models with deep language understanding. Advances in neural informa-tion processing systems, 35:3647936494, 2022.. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.Improved techniques for training gans. Advances in neural information processing systems, 29,2016.. Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang.Relay diffusion: Unifying diffusion process across resolutions for image synthesis. 2024.. Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Zhenyuan Chen, Yao Tang, Yuhao Chen, WengangCao, and Jiajun Liang. Hidiffusion: Unlocking high-resolution creativity and efficiency in low-resolution trained diffusion models. arXiv preprint arXiv:2311.17528, 2023.. 12. Appendix to FreCaS: Efficient Higher-Resolution ImageGeneration via Frequency-aware Cascaded Sampling. In this appendix, we provide the following materials:. A Details of timestep shifting in the transition process (referring to Sec. 3.2 in the main paper);B The detailed settings of FreCaS on 4 and 16 generation for SD2.1, SDXL and SD3 (referringto Sec. 4.1 in the main paper);. C More visual results and visual comparisons (referring to Sec. 4.2 in the main paper);D Experimental results of generation of SD3 (referring to Sec. 4.3 in the main paper);E Ablation studies on inference schedule (referring to Sec. 4.4 in the main paper);. ASHIFTING TIMESTEP IN THE TRANSITION PROCESS. As mentioned in Sec. 3.2 of the main paper, FreCaS employs a five-step transition process to trans-form the last latent in the current stage zsi1Lto the first latent in the next stage zsiF . In additionto changing the resolution, we adjust the timestep from L to F to ensure that the signal-to-noiseratio (SNR) (Kingma et al., 2021) could be a constant in the transition process. Given a state zt= tz0 + 1  t at timestep t, the SNR is defined as SNR(zt) =t. 1t , where 1, . . . , Trepresent the noise schedule, and  is Gaussian noise. It has been found (Hoogeboom et al., 2023;Chen, 2023) that the SNR maintains a consistent ratio across resolutions for diffusion models usingthe same noise schedule:SNR(zst ) = SNR(zst ) s. s. ,. where s and s denote different resolutions. The value of  is typically set to 2.. Teng et al. (2024) and Gu et al. (2023) proposed to redesign the noise schedule to keep SNR con-sistent when changing the resolutions of intermediate states. Since the pre-trained diffusion modelshave fixed noise schedules, in this paper we adjust the timestep, instead of the noise schedule, toensure consistent SNR between zsi1Land zsiF :. SNR(zsi1L) = SNR(zsiF )  F = 1. . . si1. si. L. 1 +si1. si. 1 L. . ,(5). where 1 is the inverse function of t. Proper adjustment of  can yield additional improvements.. Besides, SD3 (Esser et al., 2024) employs a similar formula to shift the timestep when varyingresolutions:. F =. . si. si1  L. 1 + (. si. si1  1)  L.(6). BEXPERIMENTAL SETTING DETAILS. The experimental setting details of our FreCaS are listed in Table 2.. CMORE VISUAL RESULTS. C.1MORE VISUAL RESULTS. Figure 7 illustrates more visual results of FreCaS, including those with varying aspect ratios. Fromtop to bottom, and left to right, the prompts used in examples are: 1. Beautiful winter wallpapers.2. A regal queen adorned with jewels. 3. A majestic phoenix, wings ablaze, rising from ashes,the flames casting a warm glow. 4. Lady in Red oil portrait painting won the John Singer Sargent. 13. Table 2: Detailed settings of FreCaS on the experiments. N denotes the count of additional stages.Steps presents the sampling steps in each stage. L presents the timestep of last latent in each stageexcept for the final one.  denotes the SNR ratio in the transition process. wl, wh and wc are thehyper-parameters of the proposed FA-CFG and CA-maps re-utilization.. N + 1StepsLwlwhwc. SD2.14240,101003.07.545.00.6. 16330,10,10200,2003.07.535.00.4. SDXL4240,102001.57.535.00.6. 16330,5,15400,2002.07.535.00.6. SD34220,850-7.035.00.5. Table 3: Experiments on 4 generation of SD3.. MethodsFIDbFIDpISISpCLIPSCORELatency (s)SpeedUP. DirectInference35.6845.3512.5212.6031.4538.531. Demodiffusion15.1944.3417.8414.9931.0963.330.61. Ours9.7626.6217.8316.7231.1715.942.42. Peoples award. 5. Star of the day  Actress Evelyn Laye - 1917. 6. Photograph - Clouds OverDaicey Pond by Rick Berk. 7. little-boy-with-large-bulldog-in-a-garden-france. 8. 03-Brussels-Maja-Wronska-Travels-Architecture-Paintings., 9. Red Fox Pup Print by William H. Mullins. 10.Lovely Illustrations Of Cityscapes Inspired By Southeast Asia Malaysian digital illustrator ChongFei Giaps illustrations of cityscapes are lovely and inspiring. Fantasy Landscape, Landscape Art,Illustrator, Japon Tokyo, Animation Background, Art Background, Matte Painting, Anime Scenery,Jolie Photo. 11. A plate with creamy chicken and vegetables, a side of onion rings, a cup of coffeeand a slice of cheesecake. 12. Hyper-Realistic Portrait of Redhead Girl Drawn with Bic Pens.. C.2MORE VISUAL COMPARISONS. We show more visual comparisons in Figure 8. From top to bottom, the prompts used in the fourgroups of examples are: 1. A small den with a couch near the window. 2. A painting of acandlestick holder with a candle, several pieces of fruit and a vase, with a gold frame around thepainting. 3. A noble knight, riding a white horse, the castle gates opening. 4. Mystical LandscapeDigital Art - Lonely Tree Idyllic Winterlandscape by Melanie Viola.. DEXPERIMENTS ON SD3. In this section, we present the results of the 4 generation experiments on SD3. SD3 employsa transformer-based denoising network. It eliminates all convolutional layers, thereby preventingthe application of many existing methods, such as ScaleCrafter and FouriScale. Besides, SD3 ex-hibits fine details in the central region but shows corrupted textures in the surrounding regions (seeFigure 9). This issue with the image layout also significantly impacts the performance of othermethods, such as DemoFusion. Therefore, we only compare our FreCaS with DirectInference andDemoFusion. Table 3 and Figure 9 present the quantitative and qualitative results, respectively.. From Table 3, it is evident that FreCaS achieves superior performance in terms of image qualityand inference speed. Specifically, FreCaS achieves the best results on FIDb, FIDp, IS, and ISp, andonly slightly lags behind DirectInference in terms of CLIP score. Moreover, FreCaS generates a2048  2048 image in about 16 seconds, achieving a speed-up of 2.42 and 3.97 compared toDirectInference and DemoFusion, respectively. Figure 9 illustrates the generated images. Directlyemploying the pre-trained SD3 model to generate higher-resolution images, DirectInference leads. 14. <image: DeviceRGB, width: 8192, height: 8192, bpc: 8>. Figure 7: Visual results of FreCaS on SDXL. Please zoom-in for better view.. to unreasonable image layout with the surrounding parts being corrupted, such as the road and trees.The results of DemoFusion exhibits strange artifacts, such as the car faces and eyes. In contrast, ourFreCaS successfully maintains the natural image structure while obtaining fine details.. 15. <image: DeviceRGB, width: 8192, height: 8192, bpc: 8>. 4 on SD2.116 on SD2.1. DirectInferenceMultiDiffusionAttnEntropyScaleCrafterHiDiffusionFreCaS(ours). DirectInferenceMultiDiffusionAttnEntropyScaleCrafterHiDiffusionFreCaS(ours). <image: DeviceRGB, width: 8192, height: 8192, bpc: 8>. 4 on SDXL16 on SDXL. ScaleCrafterFouriScaleHiDiffusionAccDiffusionDemoFusionFreCaS(ours). ScaleCrafterFouriScaleHiDiffusionAccDiffusionDemoFusionFreCaS(ours)Figure 8: Visual comparisons on 4 and 16 experiments of SD2.1 and SDXL. Please zoom-in forbetter view.. EABLATION STUDIES ON INFERENCE SCHEDULE. In this section, we conduct experiments on the selection of N (number of additional stages) andL (the timestep of last latent in each stage). The two factors are employed to adjust the inference. 16. <image: DeviceRGB, width: 8192, height: 8192, bpc: 8>. DirectInferenceDemoFusionFreCaS(ours)DirectInferenceDemoFusionFreCaS(ours). DirectInferenceDemoFusionFreCaS(ours)DirectInferenceDemoFusionFreCaS(ours)Figure 9: Visual comparison on 4 experiments of SD3. From top to bottom, from left to right, theprompts used in the four groups of examples are: 1. Car Photograph - Ford In The Fog by Debra andDave Vanderlaan. 2. Rupert Young is Sir Leon in Merlin season 5 copy. 3. Watchtower, ShootingStar & Milky Way, Gualala, CA. 4. Colorful Autumn in Mount Fuji, Japan - Lake Kawaguchikois one of the best places in Japan to enjoy Mount Fuji scenery of maple leaves changing color givingimage of those leaves framing Mount Fuji.. Zoom-in for better view.. Table 4: Ablation studies on N in FreCaS.. NresolutionsFIDbFIDp. 0204843.8329.7111024  204812.6317.9121024  1536  204841.3628.68. Table 5: Ablation studies on L in FreCaS.. LFIDbFIDp. 012.5718.2010012.6918.1020012.6317.9130013.3018.5740013.3418.62. schedule of our FreCaS. We reports the scores of FIDb and FIDp by varying the two factors inTable 4 and Table 5, respectively.. Choice of N. From Table 4, we see that N = 1 achieves an FIDb score of 12.63 and an FIDpscore of 17.91, significantly better than N = 0 and N = 2 in the 4 generation task for SDXL.This could be attributed to the fact that a larger value of N introduces more transition steps, which. 17. can lead to much information loss. Conversely, a smaller value of N reduces the effectiveness ofFreCaS, degenerating it to the DirectInference method.. Choice of L. From Table 5, we can see that a smaller L improves FIDb score but deteriorates FIDp.This is because the details generated at lower resolutions conflict with those at higher resolutions.Thus, we set L to 200 to avoid generating excessive unwanted details in the early stages.. 18", "2410.18412v1.HardRace__A_Dynamic_Data_Race_Monitor_for_Production_Use.pdf": "HardRace: A Dynamic Data Race Monitor for Production Use. XUDONG SUN, Nanjing University, ChinaZHUO CHEN, Nanjing University, ChinaJINGYANG SHI, Nanjing University, ChinaYIYU ZHANG, Nanjing University, ChinaPENG DI, Ant Group, ChinaXUANDONG LI, Nanjing University, ChinaZHIQIANG ZUO, Nanjing University, China. Data races are critical issues in multithreaded program, leading to unpredictable, catastrophic and difficult-to-diagnose problems. Despite the extensive in-house testing, data races often escape to deployed software andmanifest in production runs. Existing approaches suffer from either prohibitively high runtime overhead orincomplete detection capability. In this paper, we introduce HardRace, a data race monitor to detect races on-the-fly while with sufficiently low runtime overhead and high detection capability. HardRace firstly employssound static analysis to determine a minimal set of essential memory accesses relevant to data races. It thenleverages hardware trace instruction, i.e., Intel PTWRITE, to selectively record only these memory accessesand thread synchronization events during execution with negligible runtime overhead. Given the tracingdata, HardRace performs standard data race detection algorithms to timely report potential races occurred inproduction runs. The experimental evaluations show that HardRace outperforms state-of-the-art tools likeProRace and Kard in terms of both runtime overhead and detection capability  HardRace can detect all kindsof data races in read-world applications while maintaining a negligible overhead, less than 2% on average.. CCS Concepts:  Do Not Use This Code  Generate the Correct Terms for Your Paper; Generate theCorrect Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for YourPaper.. Additional Key Words and Phrases: data race, dynamic detection, hardware, static analysis. ACM Reference Format:Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. . HardRace:A Dynamic Data Race Monitor for Production Use. In . ACM, New York, NY, USA, 20 pages. https://doi.org/XXXXXXX.XXXXXXX. 1INTRODUCTION. In the post-Moore era, multithreaded software become prevalent; and concurrency errors becomemore and more common in multithreaded programs. Such errors cause critical issues such asprogram crashes [19], security vulnerabilities [13], and incorrect computations [22], leading toserious real-world social and economic hazards, e.g., the Northeast blackout, and mismatchedNasdaq Facebook share prices.In spite of extensive in-house testing, data races often escape to deployed software and manifestin production runs [20, 24]. This is because data races are highly sensitive to the execution states,including program inputs, thread interleavings, platform configurations, and other executionenvironments [20]. Such huge execution space can hardly be completely covered by testing. For the. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without feeprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and thefull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requiresprior specific permission and/or a fee. Request permissions from permissions@acm.org.Conference acronym XX, , Copyright held by the owner/author(s). Publication rights licensed to ACM.https://doi.org/XXXXXXX.XXXXXXX. 1. arXiv:2410.18412v1  [cs.SE]  24 Oct 2024. Conference acronym XX, ,Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. same reason, production-run data races are difficult to reproduce and fix offline [12]. As a result, itis highly desirable to propose an online data race detector which is able to discover data races inproduction runs practically (with sufficiently low overhead) and effectively (with high accuracy).Prior Work. Over the past decades, extensive studies have been conducted, primarily classifiedas static and dynamic approaches. Static data race detectors, such as RacerD [5] and CHESS[21], analyze the code statically without executing it. It can report the potential races beforedeployment, thus being free of runtime overhead. However, due to the inherent limitation ofover-approximation, the static approaches inevitably report many false positive warnings, severelyaffecting its practicability.On the other hand, dynamic data race detectors [10, 23, 24], monitor the program during executionto identify actual data races. Compared to static approaches [5, 21] which suffer from high falsepositives, dynamic detectors have the advantage of high precision. However, as they demand tocollect and analyze massive execution data online, the program execution is dramatically sloweddown. For example, Googles ThreadSanitizer (a.k.a., TSan) [24], the most mature and widely usedtool in industry, can result in an average 7-12x slowdown [1]. FastTrack also incurs the runtimeoverhead of a similar magnitude as reported [10]. Such high overheads severely inhibit the practicalusage of these dynamic detectors  they nowadays only work in debugging/testing mode but notproduction-run environment.Recently, several attempts have been performed to lower the runtime overhead of dynamic racedetectors [1, 11, 14, 30, 31]. RaceMob [14] adopts crowdsourcing mechanism to lower the runtimeoverhead of each individual user. ProRace [30] samples memory accesses using hardware PMU (inparticular, Intels Precise Event Based Sampling), thus achieving low runtime overhead. However,like all sampling approaches [6, 25, 30], finding the right sampling rate is often challenging. Evenworse, the detection accuracy is usually not guaranteed. Kard[1] leverages Intel Memory ProtectionKeys (MPK) to achieve low detection overhead. However, due to the limitations of MPK, Kard canonly detect a specific type of races, namely Inconsistent Lock Usage (ILU). Other common racesremain undetectable.Our Work. This paper introduces HardRace, a novel data race detector which leverages modernhardware tracing module (in particular, Intel Processor Trace instructions PTWRITE) to monitordata races in production runs, with sufficiently low overhead and high detection capability. However,naively employing hardware tracing to dynamically detect data races faces two problems.First, naively tracing all the memory accesses via hardware still yields prohibitively high runtimeoverhead. The reason is that data race detection requires tracking extensive runtime information,including memory accesses and thread synchronization events. To trace such dynamic information,a massive number of hardware tracing instructions (i.e., ptwrite) have to be instrumented andexecuted, leading to non-negligible overhead. Our empirical experiments show that the overheadof naive hardware tracing for data race detection reaches 19.8% on average (see 7.2).Second, naively recording the intensive memory access and thread synchronization informationvia hardware results in severe data loss, greatly diminishing detection capability. This is becausethe hardware generates traces much faster than memory can keep up. As a result, certain traceswould be lost especially if the hardware trace instructions (i.e., PTWRITE) is too dense [32]. In ourexperiments, naive hardware tracing leads to frequent data loss, 37% on average, which significantlyaffects the detection capability (see 7.4).To tackle the above problems, HardRace firstly employs effective static analysis to safely eliminatemost memory accesses that are unlikely to be involved in data races. It then selectively instrumentsand traces only the remaining accesses via hardware.. 2. HardRace: A Dynamic Data Race Monitor for Production UseConference acronym XX, ,. Results. We implemented HardRace and evaluated it over a common set of benchmarks, includingthe widely used PARSEC/SPLASH-2x benchmark suite and a set of real-world applications withknown data races [29]. The experimental results show that the runtime overhead of HardRace is onlyaround 1.6% on average, which is significantly lower than that of the state-of-the-art approaches.Moreover, HardRace can detect the data races in all the experimental subjects without any falsenegatives, whereas the existing dynamic detection tools like ProRace and Kard miss them a lot.This paper makes the following contributions:. HardRace presents an effective data race detector with minimal overhead, that can be deployedto monitor production runs.. HardRace firstly employs binary static analysis to safely prune away unnecessary memoryaccesses, and then leverage modern hardware tracing module (i.e., Intel PTWRITE) to realizeselective tracing, achieving sufficiently low overhead and high detection precision. To the bestof our knowledge, HardRace is the first to utilize Intel PTWRITE for precise and low-overheaddata race detection.. The experimental evaluations show that HardRace outperforms state-of-the-art tools likeProRace and Kard in terms of both runtime overhead and detection capability  HardRacecan detect all kinds of data races in read-world applications while maintaining a negligibleoverhead compared to the existing solutions.. Outline. The rest of the paper is organized as follows. 2 gives the necessary background ofhardware tracing and dynamic data race detection. 3 provides the overview of HardRace. 4 and5 describe the key components we proposed, followed by the implementation in 6. We presentthe empirical evaluations in 7. We talk about the related work in 8. Finally, 9 concludes.. 2BACKGROUND. 2.1Intel PTWRITE. Intel PTWRITE is an extended hardware tracing feature, which is available at the commodity PCswith the 12th generation (Alder Lake) desktop processors. It provides the PTWRITE instruction toefficiently and flexibly record data values from registers or memory. If a register value of interestneeds to be recorded, users can insert a PTWRITE instruction with the register as the operand.When the instruction is executed as the program runs, the hardware module writes the dynamicvalue of the register into a specific system buffer, which can be then read for usage.Intel PTWRITE is particularly advantageous for tracing multithreaded programs, where tra-ditional software instrumentation often relies on expensive locking operations to determine theorder of memory events across different threads. Such heavy intervention not only leads to adramatic slowdown of the execution, but also significantly interferes with the original threadinterleaving. In contrast, Intel PTWRITE can efficiently and precisely record traces containingtimestamp information (like TSC packets) thanks to dedicated hardware. When combined withtimestamped thread-switching events recorded via OS tools (such as perf events), it becomes possi-ble to associate the hardware trace packets (i.e., PTW packet) with each thread, thereby allowingfor the reconstruction of a timeline of events across all threads.Despite that Intel PTWRITE is of much lower overhead than traditional software techniques, itstill encounters bottlenecks when recording the sheer volume of data. In particular, high-frequencyrecording scenarios can result in performance degradation and/or severe data loss [7]. Thus, naivelyapplying PTWRITE does not fully address the issue of multi-threaded recording. In this paper,we propose dedicated static analysis to eliminate the unnecessary tracing points, finally ensuringlow-overhead and avoiding data loss.. 3. Conference acronym XX, ,Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. 2.2Data Race DetectionGiven the memory access traces, a race detection algorithm can be employed to determine if arace potentially happens  two memory accesses may occur simultaneously. Over the past years,various existing data race detection algorithms are proposed, including lockset [23], happens-beforerelation [16], causally-precedes relation [27], weak-causally-precedes relation [15], etc.The lockset algorithm, as its name suggests, focuses on lock/unlock accesses in multithreadedprograms [23]. Its primary principle is that shared variables must be protected by the same lock;otherwise, it is assumed that simultaneous access exists. However, not all multithreaded programsuse locks to ensure logical non-concurrency. For instance, operations like signal and wait can alsoestablish a temporal ordering. A sequence like <access A -> signal -> wait -> access A> clearly doesnot constitute a data race. Therefore, the lockset algorithm is sometimes overly strict, leading to asignificant number of false positives.The happens-before algorithm considers whether there is a sequential execution relationshipbetween events across different threads (named as happens-before relationship) [16]. It avoids thefalse positives of lockset algorithm. However, its detection result is sensitive to the particular threadinterleaving, meaning that even if one execution does not exhibit a race, it does not guarantee thatother interleavings are also free of races. Thus, exploring a sufficient number of interleavings isnecessary to reduce the risk of false negatives. There are also other algorithms as the extensionof happens-before relation, such as causally-precedes relation [27] and weak-causally-precedesrelation [15]. Basically, by relaxing the relation constrains, more races can be detected.In this paper, we propose an online data race monitor HardRace, which focuses on reducingthe runtime overhead without sacrificing detection capability. In brief, HardRace first collects thenecessary data access events at runtime, which are then fed into an existing detection algorithmto generate the report. For the sake of fair comparison, the offline analysis of HardRace adoptsthe combination of happens-before and lockset algorithms which are used by the state-of-the-artdynamic race detectors [1, 10, 24, 30]. Note that the contribution of this paper is to propose alow-overhead and high-precision data access monitoring approach for multithreaded programsvia hardware tracing, which is orthogonal to the race detection algorithm. The race detectionalgorithms can benefit from our lightweight monitoring; and HardRace can also adopt any otherdetection algorithms in the offline analysis.. 3OVERVIEWHardRace is a data race monitor which can on-the-fly detect races happened in production runs.Figure 1 demonstrates the workflow of HardRace, consisting of three main stages: static selectiveinstrumentation, runtime trace collection, and offline trace analysis.. <image: DeviceRGB, width: 1006, height: 358, bpc: 8>. Fig. 1. Workflow of HardRace. 4. HardRace: A Dynamic Data Race Monitor for Production UseConference acronym XX, ,. Static Selective Instrumentation. The static selective instrumentation stage(see 4) is the coreof our contribution. It is designed to statically identify a minimum set of memory accesses thatare involved in data races and selectively instrument PTWRITE instructions to record them atruntime. It contains two key components: static trace-point selection (4.14.3) and static binaryinstrumentation (4.4). Specifically, HardRace takes the target binary file and some configurationsettings as input. Static trace-point selection utilizes a series of sound static analysis algorithms todetermine a set of memory accesses that must be not involved in data races and excludes themfrom the instrumentation points. Given a minimum set of memory access points to be recorded,the binary instrumentation module is responsible to insert PTWRITE instructions to record theidentified data accesses and thread synchronization events.. Runtime Trace Collection. At this stage, the instrumented binary is executed on the CPUs withIntel PTWRITE supported in production runs. With the appropriate setting, hardware trace packetsare continuously generated. We will give more implementation details in 6.. Offline Trace Analysis . The offline analysis takes the hardware traces as input, and employs racedetection algorithm to produce the final report. In particular, the tracked hardware traces are firstdecoded into per-thread memory access and synchronization event sequences (see 5.1). Thesesequences are then passed to race detection algorithm for efficient data race detection (see 5.2).The details about multithreaded-trace decoding and race detection will be elaborated in 5 shortly.. <image: DeviceRGB, width: 332, height: 376, bpc: 8>. (a) original binary. <image: DeviceRGB, width: 200, height: 116, bpc: 8>. (b) selected points. <image: DeviceRGB, width: 336, height: 530, bpc: 8>. (c) rewritten binary. <image: DeviceRGB, width: 400, height: 350, bpc: 8>. (d) decoded output. Fig. 2. A toy example. Example. Lets use a toy example to illustrate the entire workflow of HardRace. Suppose thereis an assembly fragment in the original binary as shown in Figure 2a. There are five instructionsthat involving memory accesses: L1, L2, L4, L5, and L6. Naively, the values we need to record arelock_arg@L1, ebx@L2, ebp@L4, unlock_arg@L5 and ebx@L6. After filtering by the selection module(4), we can narrow down the trace points to lock_arg@L1, ebx@L2, and unlock_arg@L5, as shownin Figure 2b. In this example, the two points ebp@L4 and ebx@L6 can be safely eliminated sinceebp@L4 is a stack address so we can easily notice it is race-free. The value of register ebx@L6can be statically deduced according to that of ebx@L2. During the static binary instrumentation,we insert three PTWRITE instructions into the binary, as illustrated in Figure 2c. This rewritten. 5. Conference acronym XX, ,Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. binary is then executed during runtime trace collection, which generates trace data includingthread information and PTWRITE packets. By decoding trace data, we get the sequence of memoryaccesses and synchronization events distinguished by threads, as shown in Figure 2d. Here weassume there are two threads in the output, and each event records thread id, event type, accessaddress and timestamp. Having the decoded traces, we can directly employ the existing data racedetection algorithm to obtain the final report. In this example, it is evident that the instructionsat L2 and L6 access the same memory address in different threads tid@1 and tid@2, and not beprotected by same lock, which leading to a data race.. 4STATIC SELECTIVE INSTRUMENTATION. As mentioned above, the purpose of static selective instrumentation is to identify a minimum set oftrace points including memory accesses and synchronization events. It filters out unnecessary tracepoints through a series of static analyses, thus reducing the runtime overhead. In the following,we will elaborate the static trace-point selection analysis in 4.1 to 4.3. Once having a subset oftrace points, we then insert Intel PTWRITE instructions into the subject binary so as to record thenecessary data at runtime. 4.4 gives the detailed description about static binary instrumentation.. Algorithm 1: Static Selective Instrumentation. Input: Binary Output: Rewritten binary. 1   construct the interprocedural control flow graph of. 2   MultiVSA() /*inter-procedural value set analysis formultithreaded programs (4.1)*/. 3   FindAllSharedAddresses() /*find all the race-relevant tracepoints involving shared variable addresses (4.1)*/. 4   MustRaceFree(, ) /*identify the trace points which mustnot be relevant to data races (4.2)*/. 5   RedundantAnalysis( ) /*identify the redundant trace pointswhose values can be statically deduced (4.3)*/. 6. 7   Instrument(,) /*instrument PTWRITE instructions (4.4)*/. Algorithm 1 provides a high-level description of how we progressively narrow down the set oftrace points  that require instrumentation. At first, we construct the interprocedural controlflow graph (ICFG) of the input program (Line 1). Next, we perform our inter-procedural valueset analysis tailored for multithreaded programs to produce the binary-level alias results (Line 2,see 4.1). Based on the value set results , we identify a set of trace points that involve shared variable addresses, i.e., global and heap addresses (Line 3). Memory accesspoints that only access stack addresses are filtered out at this stage, as they cannot cause data races.Given the value set results  and all the potential trace points, a must race-freeanalysis is then performed to identify the set of trace points  that are impossible to causeraces (Line 4, see 4.2). Having the remaining trace points in  , we further identifythe redundancies among them where the accessed addresses exhibit derivable relationships (Line 5,see 4.3). These redundant trace points are not necessary to be instrumented and traced at runtimesince their values can be deduced during offline analysis. With the three instruction sets identified,we compute        (Line 6), which represents the set of trace. 6. HardRace: A Dynamic Data Race Monitor for Production UseConference acronym XX, ,. points that actually need to be instrumented and tracked online. The static binary instrumentationmodule takes  as input and produces the instrumented program  (Line 7, see 4.4).. 4.1Value-Set Analysis for Multithreaded Programs. As mentioned above, in order to identify the set of trace points involving shared variable addresses, we need to determine from which memory region the operand (register) of an instructionoriginates, (i.e., stack region, heap region, or global region). We treat the registers stemming from aheap or global variable as shared addresses, which are likely involved in data races. The registersthat are only relevant to stack addresses are excluded safely as they cannot cause races. To this end,we need a value-set analysis to identify the relevant region (stack, heap, or global) for each register.Moreover, in the following must race-free analysis (4.2), we also need the value-set analysis resultsto determine if two different registers may reference to the same memory location.Value-set analysis [3] is a static binary analysis technique which over-approximates a set of valueseach register can take on at each program point. It can be considered as a binary-level alias analysis.Generally, value-set analysis involves two basic terms: abstract location and value set. An abstractlocation, or a-loc, is a variable-like entity, which can represent a register or an address in global,stack, and heap regions. For instance, for the instruction mov 0x4,%eax, both global address 0x4 andthe register %eax correspond to an a-loc. A value-set represents a set of a-locs, and it is usuallydivided into three separate sets: stack, heap, and global. The value set of an a-loc is a collectionof addresses and registers that can be accessed by referencing that a-loc. For the instruction mov0x4,%eax, the value set of %eax would be   {04},  {},  {}, meaning thatthe register %eax holds the value 0x4 from the global memory region after the instruction executes.Although multiple value-set analyses have been proposed [3, 8, 17], none of them supports mul-tithreaded programs well. In other words, existing value-set analysis cannot ensure the soundnessfor multithreaded programs. To be specific, the existing value-set analysis maintains a value-setfor a register at each instruction of a single thread. It does not consider the effects of shared(heap or global) accesses by multiple threads. Therefore, for a given register of an instruction, itsvalue-set may not be a safe over-approximation of the actual values at runtime. As a consequence,we may erroneously exclude certain trace-points (registers) which are actually related to sharedaccesses, leading to loss of detection capability. To guarantee soundness of value-set analysis, oneway is to extend the control-flow graph by adding all the possible edges because of interleavings.Unfortunately, such approach could cause the control-flow graph to be amplified dramaticallyconsidering the large number of interleavings, thus leading to poor analysis scalability.In this paper, we devise a dedicated value-set analysis ensuring both soundness and scalability.Similar to the existing analysis, we record the value set of a register at each instruction alongthe traditional control-flow graph. However, for global and heap locations, we maintain a sharedsummary across the entire program. In this way, the value-set of shared accesses can be guaranteedto be an over-approximation of the actual values. thus ensuring soundness. Moreover, the analysiscan scale well to large programs, since the analysis is performed along the original control-flowgraph and the value-set relevant to global and heap locations are flow-insensitive.Algorithm algorithm 2 shows the algorithm in details. At the beginning, localValueSet andsharedValueSet are initialized as empty (Lines 1-2), which represent the value-set results for stack andshared (global and heap) locations, respectively. The worklist algorithm processes each instruction,starting from the entry instruction of the program (Line 5). The transfer function is performedto update localValueSet[i] and sharedValueSet (Line 7), followed by propagating the value-set toall the successors (Line 8), until all localValueSet entries remain unchanged. Since the value-setupdates in the transfer function are performed using union operation, the algorithm is guaranteedto converge and terminate. In the transfer function, we update localValueSet and sharedValueSet. 7. Conference acronym XX, ,Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. Algorithm 2: Value-set analysis for multithreaded programs. Data: localValueSet[i][x], the value set of a-loc x for instruction i where x is a register orstack a-loc; sharedValueSet[y], the value set of a-loc y which is a global or heap a-loc. 1 localValueSet. 2 sharedValueSet. 3   put all the entry instructions into worklist. 4 while worklist   do. 5  .(). 6oldLocalValueSet[i]  localValueSet[i]. 7transfer(i, localValueSet, sharedValueSet). 8propagate(,()). 9if oldLocalValueSet[i]  localValueSet[i] then. 10worklist.push(()). 11 Function transfer(i, localValueSet, sharedValueSet). 12if i is mov then. 13src_alocs  (i, src_op, localValueSet, sharedValueSet). 14dst_alocs  (i, dst_op, localValueSet, sharedValueSet). 15foreach dst_aloc  dst_alocs do. 16if dst_aloc is register or stack a-loc then. 17localValueSet[i][dst_aloc].union(src_alocs). 18else. 19/*dst_aloc is global or heap.*/. 20sharedValueSet[dst_aloc].union(src_alocs). 21else if ... then. 22... /*The propagation of other instructions is omitted here.*/. according to the specific semantics of each type of instruction. Here, we only give the transfer logicof mov instruction due to space limit. Generally, it first retrieve the value-sets (i.e., localValueSet andsharedValueSet) to acquire the corresponding alias a-locs of the source and destination operandsof instruction i. Then, based on the type of destination a-loc, it decides to update localValueSet orsharedValueSet accordingly.Having the value-set results, we can obtain the set of trace points (i.e., registers), which arerelevant to shared (heap or global) memory regions. In particular, given a register of an instruction,if its value-set contains any heap or global a-locs, we include it into . Otherwise, if the value-set of a register only contains stack addresses, we can safely exclude it from tracing. Specifically,for an instruction i = mov 0x18(%eax), %edx, we check localValueSet[i][eax]. If it contains a-locsbelonging to global or heap regions, then eax could represent the address of a shared variable. Wethus put @ into . Otherwise, @ can be safely eliminated.. 4.2Must Race-free AnalysisIn this section, we would like to employ static analysis to further prune away the trace pointsirrelevant to data races. As is well known, data races occur only if three conditions are satisfied: 1). 8. HardRace: A Dynamic Data Race Monitor for Production UseConference acronym XX, ,. two memory accesses target the same address; 2) they are accessed concurrently; 3) at least one ofthem is a write operation. Based on this, we propose a static must race-free analysis. In brief, foreach memory trace-point in , we statically check if at least one of the above three conditionsmust be violated. If so, we consider it as race-free access. We can thus safely avoid it from tracing.The core logic of the algorithm is shown in Algorithm 3. We take the set of trace-points involving global/heap memory as input and iterate over all pairs of registers ,  . At Line6, we check if any one of the three conditions are violated. A trace point  is considered to belongto if and only if it does not form a data race with any possible point . The rest trace pointsof  would be treated as .In the following, we elaborate how to check the three conditions. notWrite can be checked easilyby simply determining whether x or y is a write operation. The determination of notAlias(x, y)is done on the basis of value-set analysis (4.1). The result of value-set analysis provides a setof potential values for a specific register at each program point, which essentially identifies thepossible may-alias relationships between the register and different a-locs (i.e., other registers andaddresses). As long as the intersection of the value-sets for x and y is empty, it can be concludedthat x and y access different addresses, i.e., notAlias(x, y) returns true. To determine whetherthe concurrent access condition is met, the core logic of the notConcurrent function is shown inLines 16-22. Basically, it follows the logic of lockset algorithm. If  or  is an allocated heap objectintra-procedurally and does not escape, we can reach that  and  cannot be executed concurrently.The detailed logic for determining isOwned is shown as Lines 23-34. More importantly, it alsoconsiders the accesses within critical sections enclosed by locks and unlocks. The function analyzes and returns the lock variables involved. If the intersection of () and() is non-empty, then  and  are protected by the same lock. Thus, true is returned.. 4.3Redundant Register Elimination. After the trace-point elimination discussed above, the set  may still have potentialfor further reduction. The rationale is that the addresses accessed in two instructions may have astatic relationship. In other words, the value of a register in one instruction can be statically derivedfrom that of another in other instruction. In such cases, we only need to record the deriving register.The values of subsequent registers can be deduced offline.. L1: mov 0x8(%ebp),%eaxL2: mov 0x4(%eax),%eaxL3: test %eax,%eaxL4: je L5. L5: mov ecx, edxL6: sub ecx, 1. L7: mov -0xc(%ebp),%eaxL8: mov %edx,0x8(%eax). . .L9: mov %edx,0xc(%eax)L10: jmp L11. L11: mov 0x8(%ebp),%eaxL12: mov 0xc(%eax),%eax. . .. Fig. 3. A toy example for redundant register elimination. For example, Figure 3 has four basic blocks, and the memory instructions that need to be trackedare highlighted in bold (i.e., L2, L8, L9, and L12). For L8 and L9, there are two memory accesses,0x8(%eax)@L8 and 0xc(%eax)@L9. Naively, we need to instrument and trace the value of eax at. 9. Conference acronym XX, ,Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. Algorithm 3: Must Race-free Analysis. Input: a set of shared trace-points Output: a set of trace-points that must be irrelevant to races. 1   ,. 2 foreach x   do. 3. 4if    then. 5foreach    do. 6if (,) or (,) or (,) then. 7continue. 8else. 9. 10.(). 11break. 12if   then. 13.(). 14else. 15.(). 16 Function notConcurrent(x, y). 17if ()  () then. 18/* means a heap object is allocated intra-procedurally and does not escape.*/. 19return true. 20if ()  ()   then. 21return true. 22return false. 23 Function IsOwned(x). 24i  the instruction of. 25alocs  localValueSet[i][x]. 26foreach aloc  alocs do. 27if aloc is heap and is allocated within the function of x then. 28foreach arg_aloc: the argument of callsites within the function of x do. 29if aloc  localValueSet[i][arg_aloc] then. 30return false /*escape to other function, not intra-procedural*/. 31if aloc  sharedValueSet then. 32return false /*escape to memory*/. 33return false. 34return true. both L8 and L9. In fact, eax at both L8 and L9 has the same value, which equals to -0xc(%ebp) whereebp is a local address. Thus, it is sufficient to record the value of %eax only at L8, rather than both.. 10. HardRace: A Dynamic Data Race Monitor for Production UseConference acronym XX, ,. Beyond the elimination within a basic block, we also consider the situations across basic blocks.For instance, the eax at L2 and L12 are also identical no matter which branch is taken, which equalsto 0x8(%ebp)@L1. Therefore, only one eax needs to be traced.Technically, given a memory access register, we perform an intra-procedural backward symbolicpropagation from it until the entry of function. If the symbolic expressions of two registers areidentical or have statically fixed relation, then we only keep one as the trace-point. The value ofanother will be statically deduced.. 4.4Static Binary InstrumentationBased on 4.1 to 4.3, we identified a minimal set of trace points to be recorded at runtime. Here,we exploit hardware tracing module to achieve low runtime overhead. In particular, we do this byinserting a PTWRITE instruction with the operand being the specific register. Taking Figure 2 as anexample, for the instruction <mov 0x4(%ebx), %eax> at location L2, the value to be recorded is ebx@L2,so a <ptwrite %ebx> instruction is inserted right before L2. Each register may be instrumentedmultiple times at different locations. Therefore, we need to distinguish which instruction eachPTWRITE packet corresponds to. To this end, during instrumentation, we manually maintain themapping between each PTWRITE instruction and the instruction to be traced. This allows us tofurther determine the exact register corresponding to the PTWRITE packet. For redundant registerelimination in 4.3, we maintain the arithmetic relationship between two registers. In the decodingphase, the eliminated memory accesses are reconstructed based on the recorded register value.. 5OFFLINE TRACE ANALYSIS. 5.1Hardware Trace DecodingThe hardware traces generated by Intel PTWRITE is stored in a compact packet format. Beforeanalyzing them, we need to firstly decode these packets into the memory read/write events andthread synchronization events required by the race detection algorithm on a per-thread basis.. <image: DeviceRGB, width: 200, height: 490, bpc: 8>. (a) hardware packets. <image: DeviceRGB, width: 196, height: 172, bpc: 8>. (b) sideband trace. <image: DeviceRGB, width: 278, height: 170, bpc: 8>. (c) decoded events. Fig. 4. A trace example. The trace data is stored separately for each CPU. We first consider the case of a single CPU. Forsimplicity, we represent the PTWRITE trace packets as shown in Figure 4a, which includes TSC,CYC, and PTWRITE packets. The TSC packet contains a specific timestamp. Similarly, each CYCpacket indicates the number of clock cycles elapsed since the previous CYC or TSC packet. Based on. 11. Conference acronym XX, ,Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. these TSC and CYC packets, we can compute the timestamp for each PTWRITE packet. For example,the timestamp of <PTWRITE x> in Figure 4a can be calculated as 2 = 0 +0 +1, while thetimestamp of <PTWRITE y> is 4 = 0 + 0 + 1 + 2. Meanwhile, the runtime data alsoincludes a sideband trace obtained using perf events, which contains thread switch informationwith timestamps, shown as Figure 4b. Suppose 1 < 2 < 3 < 4, thus <PTWRITE x> isexecuted after 1 but before 3. Therefore, we can derive that <PTWRITE x> belongs to thread1. Similarly, <PTWRITE y> can be determined to belong to thread 2. As such, we obtain the finalmemory access events as shown in Figure 4c. For multiple CPU cores, the event sequence fromeach CPU can be integrated according to their timestamps, resulting in a complete sequence ofaccesses.At this stage, we essentially know the thread ID (tid) and the timestamp corresponding to eachPTWRITE. Combined with the mapping information between each PTWRITE and the originalinstruction, we can determine the specific type of memory event (e.g., read, write, lock, or unlock).This ultimately allows us to reconstruct a trace similar to that shown in Figure 2d.. 5.2FastTrack-based Offline DetectorWith the per-thread memory access events and thread synchronization events decoded in theprevious stage, we can fully leverage dynamic data race detection algorithms for offline racedetection. For the sake of fair comparison, HardRace adopts the combination of happens-before andlockset algorithms which are utilized by the state-of-the-art dynamic race detectors [1, 10, 24, 30].In brief, the offline detector reads and processes memory access events and thread synchronizationevents in the recorded order. It simulates dynamic data race detection by using information such asthe recorded thread ID (TID), memory access addresses, lock variable addresses, and event types.Note that again the contribution of this paper is to propose a low-overhead and high-precision dataaccess monitoring approach for multithreaded programs via hardware tracing, which is orthogonalto the race detection algorithm. The race detection algorithms can benefit from our lightweightmonitoring; and HardRace can also adopt any other detection algorithms in the offline analysis.. 6IMPLEMENTATION. In the static selective instrumentation module, we use Capstone [2] and Angr [26] to construct aninter-procedural control flow graph (i.e., ICFG), and then perform our inter-procedural value setanalysis and must race-free analysis over it. The instrumentation part is implemented based onDyninst [4], a well-known binary instrumentation framework. The runtime trace collection modulemainly utilizes the perf_event_open function to configure the CPU buffer, and controls the relevantregisters to enable Intel PTWRITE and IP filtering. By default, we allocate a 128MB memory bufferto maintain the hardware traces for each CPU core. At the offline trace analysis part, the decoder isfurther developed based on the source code of the libipt library [9] provided by Intel. We directlyreuse the implementation of FastTrack-based detector in ProRace [30] as our detection module.. 7EVALUATIONSIn this section, we evaluate HardRace over a comprehensive set of benchmarks to answer thefollowing research questions.. How well does HardRace perform in terms of runtime overhead, static analysis scalability,instrumentation cost, and offline analysis efficiency? And how about comparison with thestate-of-the-arts with respect to runtime overhead? (7.2). What about the detection capability of HardRace compared with the state-of-the-arts? (7.3). 12. HardRace: A Dynamic Data Race Monitor for Production UseConference acronym XX, ,. How effective is our static selective instrumentation mechanism in reducing the runtimeoverhead and data loss? (7.4). Table 1. The performance of HardRace in terms of the time of static analysis (4.1, 4.2 and 4.3), static binaryinstrumentation (4.4), offline decoding (5.1), and offline race detection (5.2). #Inst and #Func represent thenumber of instructions and functions in the program, respectively.. subject#Inst#Funcstatic analysisinstrumentdecodingrace detection. streamcluster4278952.1s2.3s192.3s255.8sx2641786471173122.2s10.8s70.9s73.0svips8151857333462.3s59.3s32.2s20.8sbodytrack107186341732.7s3.2s43.7s47.1sfluidanimate72201082.8s2.5s117.6s122.0socean_cp246016012.5s2.7s0.2s0.0socean_ncp15376506.9s2.6s0.2s0.0sraytrace199471917.1s2.6s109.4s107.4swater_nsquared7754613.3s2.4s0.7s0.6swater_spatial8152623.3s2.4s0.2s0.0sradix3314482.2s2.4s0.8s0.5slu_ncb3098561.9s2.3s0.2s0.0slu_cb3725582.1s2.4s0.1s0.0sbarnes83851073.5s2.4s114.7s131.9sfft3908572.2s2.5s0.1s0.0s. (Arithmetic Mean)80718.4858.444.5s6.9s45.6s50.6s. 7.1Experimental Setup. Benchmarks. We primarily choose two representative sets of subject programs as our evaluationbenchmarks. At first, to understand the performance of HardRace, we take PARSEC/SPLASH-2xbenchmark suite (version 3.0beta-20150206), which is commonly used to measure the performancein related works, such as ProRace [30] and Kard [1]. The first three columns in Table 1 list thedetailed characteristics about each subject program in terms of subject name, the number ofinstructions, the number of functions.Moreover, to compare with the state-of-the-arts with respect to detection capability, we borrowthe benchmark from ProRace [30], which is a set of real-world applications with known data races[29]. We follow ProRaces measurement and test 11 buggy program versions, each with one differentrace triggered, including apache, mysql, cherokee, pbzip and aget. Note that ProRace originallytested 12 program versions. However, since pfscan is not publicly available in the repository, wecan only test 11 of them. Table 3 shows all the buggy versions and their manifest information.. Comparison Tools. We select Kard [1], ProRace [30] and naive hardware tracing approach as thecomparison approaches. Kard leverages Intel MPK to allocate keys for inter-thread memory accessprotection, thus achieving low runtime overhead. ProRace, on the other hand, samples memoryaccesses using hardware PMU (in particular, Intels Precise Event Based Sampling). We select thesetwo as comparison since they are the most recent work targeting low-overhead dynamic date racedetection. The naive hardware tracing approach is a variant of HardRace by disabling the staticselective instrumentation module. In other words, we naively treat all the registers involving shared. 13. Conference acronym XX, ,Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. memory accesses as the potential trace-points, and instrument PTWRITE instructions to record allof them at runtime.Environments. We conduct all the experiments on a workstation with an Intel Core i9-14900Kprocessor with 32 logical cores supporting Intel PTWRITE. The workstation has 64 GB memoryand is equipped with a solid-state drive (SSD). It runs Ubuntu 22.04 LTS with a kernel version of5.15.. 7.2Overall Performance. We run the subjects five times and report the average time for each metric. Table 1 gives the detailedperformance with respect to the time of static analysis (i.e., value-set analysis in 4.1, must race-freeanalysis in 4.2 and redundant register elimination in 4.3), static binary instrumentation (4.4),offline trace decoding (5.1), and offline race detection (5.2).As can be seen, the series of static selection analysis are efficient enough, which can be donewith a couple of seconds for most subjects. The average time cost is only about 14 seconds. Thisis reasonable since we treat the shared value-set results in our analysis as flow-insensitive soas to achieve soundness and efficiency, which is elaborated in 4.1. Moreover, the static binaryinstrumentation is also efficient enough as its complexity is linear to the program size. For theoffline part, both trace decoding and race detection can be easily finished with seconds on average.All of these validate that HardRace is efficient enough, and can scale well in practice.. Table 2. The runtime overhead (TO) of HardRace, compared with Kard, ProRace and Naive hardware tracingapproach. The three columns under ProRace indicate the overhead with different sampling rate.  and represent the number of instructions to be instrumented statically and to be traced dynamically,respectively.. Bench NameHardRaceKardProRace (TO)Naive. TOTO1/1001/10001/10000TO. streamcluster0.2%125173.7M0.3%>30%>5%>5%8.6%487464.2Mx2643.5%599452.1M3.0%---65.2%20665319.7Mvips0.1%2942623.8M1.3%---34.7%9970370.4Mbodytrack1.1%23437.0M10.4%>350%>5%>1%36.3%58550.0fluidanimate9.6%7396.5M61.9%>600%>90%>5%24.3%620104.1Mocean_cp4.9%1326.7K-5.9%---10.4%4266151.2Mocean_ncp4.0%706.5K0.0%---57.8%2247271.5Mraytrace4.3%44489.5M3.7%>290%>30%>1%30.7%172657.3Mwater_nsquared0.3%118320.7K18.0%---4.1%685255.9Mwater_spatial0.2%12321.5K5.6%---3.7%79992.4Mradix-0.0%129356.9K-1.0%---7.7%531141.1Mlu_ncb-8.1%916.5K-5.2%---5.3%39327.8Mlu_cb-2.1%946.5K-4.7%---9.3%53133.9Mbarnes3.6%22996.0M34.1%----5.7%827192.2Mfft2.5%115201.01.0%---4.6%55621.1M. (Arithmetic Mean)1.6%2493.138.0M8.2%>317.5%>32.5%>3.0%19.8%9326.1146.9M. In addition, we also compare the runtime overhead of HardRace with the state-of-the-arts, Kard[1], ProRace [30], and the naive hardware tracing approach. Table 2 shows the detailed results.ProRace is a sampling-based approach, whose overhead relies on the exact sampling rate. As such,we measure the overheads of ProRace under three different sampling rates (i.e., 1/100, 1/1000,. 14. HardRace: A Dynamic Data Race Monitor for Production UseConference acronym XX, ,. 1/10000). To calculate the overhead, we run the baseline subjects and the instrumented subjectseach five times and compute the average. The overhead is computed as the execution time ofinstrumented program divided by the execution time of original program minus 100%. Note thatdue to nondeterministic execution of the subjects and the tiny execution time, it is possible that theoverhead is negative. To understand further why the overheads differ greatly between HardRaceand naive tracing, we calculate the number of PTWRITE instructions to be instrumented staticallyand to be traced dynamically, denoted as  and  in Table 2, respectively. Furthermore, thesubjects evaluated by ProRace and Kard are not totally identical. We chose to align them with Kardsince it is more recent then ProRace. The symbol - of Table 2 indicates that the subject is notevaluated and no data is available for ProRace.We can observe that HardRace achieves a negligible overhead of 1.6% on average. Kard suffersfrom an overhead of 8.2%. But importantly, it only supports a limited types of races. We will discussthem shortly in 7.3. For ProRace, the overheads under different sampling rates vary significantly. Itreaches beyond 300% with 1/100 sampling rate. It also has around 3% overhead under 1/10000. Againas is well known, the low sampling rate usually corresponds to low detection capability (see Table 3).In contrast, HardRace performs selective tracing, which achieves low overhead while not sacrificingdetection capability. Additionally, the average overhead of naive hardware tracing approach is19.8%. Even worse, an immense amount of data loss happens  nearly 40% of traces are lost shownas Table 4. We also compared the number of static instrumentation points () and dynamicallydecoded instrumentation points () between HardRace and the naive method. To understandthe overhead differences between HardRace and Naive hardware tracing, the data of  and provides clues. The numbers of PTWRITE instructions to be instrumented statically (i.e.,) and to be executed dynamically (i.e., ) by HardRace are about 2400+ and 38M, which aredramatically smaller than that of naive hardware tracing (i.e., 9300+ and 146M), respectively. Thisalso indicates that our static selective instrumentation module significant prunes away unnecessarytrace points, thus reducing the overhead.. Table 3. Detection probability for each approach.. Bug manifestationTypeProRace/%HardRace/%Kard/%. 1/1001/10001/10000. apache-21287double freenon-ILU50301000apache-25520corrupted lognon-ILU5752151000apache-45605assertionnon-ILU601111000mysql-3596crashILU510100100mysql-644crashILU2161100100mysql-791missing outputILU5920100100cherokee-0.9.2corrupted lognon-ILU632981000cherokee-bug1corrupted lognon-ILU571951000pbzip2-0.9.4-crashcrashILU000100100pbzip2-0.9.4-benign-ILU100100100100100aget-bug2wrong record in logILU100100100100100. (Arithmetic Mean)52.029.420.9100.054.5. 7.3Detection CapabilityIn this section, we measure the detection capability of HardRace, and compare it with the state-of-the-arts. Each subject presented in Table 3 is reported to contain a hard-to-trigger data race.. 15. Conference acronym XX, ,Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. And the reporters provided corresponding patches to control thread interleaving, enabling thedata race to be triggered within a short period of time. We evaluate the detection capability interms of detection probability which is introduced by ProRace [30]. To be specific, we run eachbuggy program 100 times and to count how many runs each detection tool can report the race.A probability of 50% indicates that, among 100 runs, the data race is detected in 50 of them. Aprobability of 100% means that the tool can detect the race every time, indicating that there are nofalse negatives for that subject.Table 3 lists the detection probability data of various tools. HardRace is able to detect the bugs inall the runs without any false negatives. This is because HardRace only prunes away unnecessarymemory accesses in a safe manner. Technically, the memory accesses that can trigger data racesshould all be recorded in hardware traces, allowing for detection when a data race occurs. In contrast,ProRace shows average detection probabilities of 52.0%, 29.4%, and 20.9% for sampling rates 1/100,1/1K, and 1/10K, respectively. This means that even with very dense sampling, the detectionprobability only reaches about a half, while the overhead is prohibitively high (as mentioned earlierin Table 2). For Kard, due to the lack of relevant experimental data and the absence of source code, adirect comparison can hardly be conducted. However, based on our understanding of its approach,we can reason if each race can be detected by Kard. Specifically, Kard states that it can only detectInconsistent Lock Usage (ILU) races. Upon manual checking of the data race types of the subjects,we find that five of the eleven programs exhibit non-ILU type of data races. Thus, they cannot bedetected by Kard.. Table 4. The data loss times and percentage due to buffer overflow. subjectHardRaceNaive Hardware Tracing. loss percentloss timesloss percentloss times. streamcluster0.0%08.7%7x2640.0%084.0%317vips0.0%033.5%5bodytrack0.0%060.6%36fluidanimate0.0%022.0%14ocean_cp0.0%00.0%0ocean_ncp0.0%076.5%80raytrace0.0%061.3%10water_nsquared0.0%059.6%16water_spatial0.0%00.0%0radix0.0%051.6%12lu_ncb0.0%013.1%1lu_cb0.0%044.7%1barnes0.0%043.2%17fft0.0%00.0%0. (Arithmetic Mean)0.0%037.3%34. 7.4Effectiveness of Selective Instrumentation. As mentioned before, our static selective instrumentation module plays the crucial role in reducingruntime overhead and hardware data loss. In this section, we would like to validate the significance of. 16. HardRace: A Dynamic Data Race Monitor for Production UseConference acronym XX, ,. selective instrumentation (4) empirically. First, regarding overhead reduction, Table 2 provides theexperimental data about the runtime overhead, the number of PTWRITE instructions instrumentedstatically and traced dynamically by HardRace and naive hardware tracing. Apparently, the overheadwith selective instrumentation enabled (i.e., HardRace) is much smaller than that of naive hardwaretracing. Second, as for data loss, we measure the times of loss happened and the total percentageof data loss by HardRace and naive hardware tracing. As shown in Table 4, HardRace incurs nodata loss for all subject programs under stress testing. In contrast, the naive hardware tracing hasan average 37.3% of data loss, with an average of 34 times of data loss events per subject. It isnoteworthy that while the naive approach performs well on certain subjects such as streamcluster,ocean_cp, and water_spatial, it still experiences significant data loss in the majority of cases. Forx264 and ocean_ncp, it successfully collects less than 30% of the data. The reason is that for thesubjects like x264 and ocean_ncp, intensive memory accesses occur frequently in the program,leading to severe hardware data loss. All in all, we conclude that the selective analysis in HardRaceis highly effective in reducing data loss, thus ensuring detection capability.. 8RELATED WORKOver the past years, various approaches for data race detection have been proposed, includingstatic, dynamic, and hybrid approaches.Dynamic Race Detection. The lockset algorithm, introduced by Eraser [23], checks whethershared memory is consistently protected by locks to detect data races. Although prone to falsepositives, its efficiency makes it a reference point for many subsequent studies. For example, thewell-known ThreadSanitizer (TSAN) [24] employs a combination of happens-before and locksetalgorithms to balance detection accuracy and performance. Another prominent dynamic tool,FastTrack [10], optimizes the original happens-before approach for better performance. Otherdynamic detectors like ProRace [30], Kard[1], and TxRace [31], integrate hardware-based techniquesinto their detection processes. ProRace uses Intel Processor Trace (PT) to log the programs controlflow and PEBS to sample memory accesses, then applies the FastTrack algorithm offline to detectraces. Kard and TxRace, on the other hand, rely entirely on hardware-based methods. Kard usesMemory Protection Keys (MPK) to ensure that a shared object is accessible by only one threadwithin a critical section. TxRace utilizes hardware transactional memory (HTM) to detect dataraces dynamically, treating critical regions as atomic transactions and checking for conflicts. Atthe same time, some studies optimize the happens-before (HB) relationship at the algorithmiclevel to mitigate the shortcomings of dynamic data race detection. For instance, [27] introducesthe causally-precedes (CP) relationship, which is a subset of the HB relationship, allowing for theobservation of more races without sacrificing robustness. Meanwhile, WCP[15] further weakensthe CP relationship and enables race detection within linear time.Static Race Detection. Static detectors often exhibit high false positives and low false negatives.For instance, RacerD [5] is classified as a lockset-based detector, allowing it to avoid dealing withthe vast number of interleavings typical in static analysis. Conversely, O2 [18], which combineslockset and happens-before analysis, must construct a static happens-before graph (SHB) duringstatic analysis and explore as many interleavings as possible to minimize false negatives.. Hybrid Race Detection. Although relatively uncommon, some work has explored the combinationof static analysis followed by dynamic execution. For instance, RaceMob [14] integrates both staticand dynamic techniques: it first uses the static analysis tool RELAY [28] to detect potential dataraces, then breaks down these potential race conditions into tasks that are crowdsourced to users,ensuring minimal overhead for individual users.. 17. Conference acronym XX, ,Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. 9CONCLUSION. HardRace is a data race monitor which can on-the-fly detect races happened in production runs. Itscore technical contribution lies on a series of sound static analysis which are unitized to prune awayunnecessary memory accesses significantly, thus achieving super-low runtime overhead. To the bestof our knowledge, HardRace is the first work to leverage Intel PTWRITE for production-run datarace detection. The experimental evaluations validate that HardRace can achieve sufficiently lowoverhead while ensuring good detection capability. It to some extent proves that the holistic designcombining static analysis and hardware tracing is promising for multithreaded program monitoring.We are looking forward to more attempts along this direction for multithreaded programs.. DATA-AVAILABILITY STATEMENTWe would like to provide the artifact later and submit it for Artifact Evaluation. It would containthe source code of HardRace, the benchmarks used, as well as all the experimental results.. REFERENCES. [1] Adil Ahmad, Sangho Lee, Pedro Fonseca, and Byoungyoung Lee. 2021. Kard: lightweight data race detection withper-thread memory protection. In Proceedings of the 26th ACM International Conference on Architectural Support forProgramming Languages and Operating Systems (Virtual, USA) (ASPLOS 21). Association for Computing Machinery,New York, NY, USA, 647660. https://doi.org/10.1145/3445814.3446727. [2] Quynh Nguyen Anh. 2014. Capstone: Next generation disassembly framework. Proceedings of the 2014 Black Hat USA,Black Hat USA 14 (2014).. [3] Gogul Balakrishnan and Thomas Reps. 2010. WYSINWYX: What you see is not what you eXecute. ACM Trans.Program. Lang. Syst. 32, 6, Article 23 (aug 2010), 84 pages. https://doi.org/10.1145/1749608.1749612. [4] Andrew R. Bernat and Barton P. Miller. 2011. Anywhere, any-time binary instrumentation. In Proceedings of the 10thACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools (Szeged, Hungary) (PASTE 11). Associationfor Computing Machinery, New York, NY, USA, 916. https://doi.org/10.1145/2024569.2024572. [5] Sam Blackshear, Nikos Gorogiannis, Peter W. OHearn, and Ilya Sergey. 2018. RacerD: compositional static racedetection. Proc. ACM Program. Lang. 2, OOPSLA, Article 144 (oct 2018), 28 pages. https://doi.org/10.1145/3276514. [6] Michael D. Bond, Katherine E. Coons, and Kathryn S. McKinley. 2010. PACER: proportional detection of dataraces. In Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation(Toronto, Ontario, Canada) (PLDI 10). Association for Computing Machinery, New York, NY, USA, 255268. https://doi.org/10.1145/1806596.1806626. [7] Daming D. Chen, Wen Shih Lim, Mohammad Bakhshalipour, Phillip B. Gibbons, James C. Hoe, and Bryan Parno. 2021.HerQules: securing programs via hardware-enforced message queues. In Proceedings of the 26th ACM InternationalConference on Architectural Support for Programming Languages and Operating Systems (Virtual, USA) (ASPLOS 21).Association for Computing Machinery, New York, NY, USA, 773788. https://doi.org/10.1145/3445814.3446736. [8] Sanchuan Chen, Zhiqiang Lin, and Yinqian Zhang. 2021. SelectiveTaint: Efficient Data Flow Tracking With StaticBinary Rewriting. In 30th USENIX Security Symposium (USENIX Security 21). USENIX Association, 16651682. https://www.usenix.org/conference/usenixsecurity21/presentation/chen-sanchuan. [9] Intel Corporation. [n. d.]. libipt: an Intel(R) Processor Trace decoder library. https://github.com/intel/libipt Accessed:2024.. [10] Cormac Flanagan and Stephen N. Freund. 2009. FastTrack: efficient and precise dynamic race detection. In Proceedingsof the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation (Dublin, Ireland) (PLDI09). Association for Computing Machinery, New York, NY, USA, 121133. https://doi.org/10.1145/1542476.1542490. [11] Anup Holey, Vineeth Mekkat, and Antonia Zhai. 2013. HAccRG: Hardware-Accelerated Data Race Detection in GPUs.In Proceedings of the 2013 42nd International Conference on Parallel Processing (ICPP 13). IEEE Computer Society, USA,6069. https://doi.org/10.1109/ICPP.2013.15. [12] Jeff Huang, Charles Zhang, and Julian Dolby. 2013. CLAP: recording local executions to reproduce concurrencyfailures. In Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation(Seattle, Washington, USA) (PLDI 13). Association for Computing Machinery, New York, NY, USA, 141152. https://doi.org/10.1145/2491956.2462167. [13] Baris Kasikci, Weidong Cui, Xinyang Ge, and Ben Niu. 2017. Lazy Diagnosis of In-Production Concurrency Bugs.In Proceedings of the 26th Symposium on Operating Systems Principles (Shanghai, China) (SOSP 17). Association forComputing Machinery, New York, NY, USA, 582598. https://doi.org/10.1145/3132747.3132767. 18. HardRace: A Dynamic Data Race Monitor for Production UseConference acronym XX, ,. [14] Baris Kasikci, Cristian Zamfir, and George Candea. 2013. RaceMob: crowdsourced data race detection. In Proceedings ofthe Twenty-Fourth ACM Symposium on Operating Systems Principles (Farminton, Pennsylvania) (SOSP 13). Associationfor Computing Machinery, New York, NY, USA, 406422. https://doi.org/10.1145/2517349.2522736. [15] Dileep Kini, Umang Mathur, and Mahesh Viswanathan. 2017. Dynamic race prediction in linear time. In Proceedings ofthe 38th ACM SIGPLAN Conference on Programming Language Design and Implementation (Barcelona, Spain) (PLDI2017). Association for Computing Machinery, New York, NY, USA, 157170. https://doi.org/10.1145/3062341.3062374. [16] Leslie Lamport. 1978. Time, clocks, and the ordering of events in a distributed system. Commun. ACM 21, 7 (July 1978),558565. https://doi.org/10.1145/359545.359563. [17] Jian Lin, Liehui Jiang, Yisen Wang, and Weiyu Dong. 2019. A Value Set Analysis Refinement Approach Based onConditional Merging and Lazy Constraint Solving. IEEE Access 7 (2019), 114593114606. https://doi.org/10.1109/ACCESS.2019.2936139. [18] Bozhen Liu, Peiming Liu, Yanze Li, Chia-Che Tsai, Dilma Da Silva, and Jeff Huang. 2021. When threads meetevents: efficient and precise static race detection with origins. In Proceedings of the 42nd ACM SIGPLAN InternationalConference on Programming Language Design and Implementation (Virtual, Canada) (PLDI 2021). Association forComputing Machinery, New York, NY, USA, 725739. https://doi.org/10.1145/3453483.3454073. [19] Shan Lu, Soyeon Park, Eunsoo Seo, and Yuanyuan Zhou. 2008. Learning from mistakes: a comprehensive study on realworld concurrency bug characteristics. SIGOPS Oper. Syst. Rev. 42, 2 (mar 2008), 329339. https://doi.org/10.1145/1353535.1346323. [20] Shan Lu, Soyeon Park, Eunsoo Seo, and Yuanyuan Zhou. 2008. Learning from mistakes: a comprehensive study on realworld concurrency bug characteristics. In Proceedings of the 13th International Conference on Architectural Supportfor Programming Languages and Operating Systems (Seattle, WA, USA) (ASPLOS XIII). Association for ComputingMachinery, New York, NY, USA, 329339. https://doi.org/10.1145/1346281.1346323. [21] Madanlal Musuvathi, Shaz Qadeer, Thomas Ball, Gerard Basler, Piramanayagam Arumuga Nainar, and Iulian Neamtiu.2008. Finding and reproducing Heisenbugs in concurrent programs. In Proceedings of the 8th USENIX Conference onOperating Systems Design and Implementation (San Diego, California) (OSDI08). USENIX Association, USA, 267280.. [22] Robert OCallahan and Jong-Deok Choi. 2003. Hybrid dynamic data race detection. In Proceedings of the Ninth ACMSIGPLAN Symposium on Principles and Practice of Parallel Programming (San Diego, California, USA) (PPoPP 03).Association for Computing Machinery, New York, NY, USA, 167178. https://doi.org/10.1145/781498.781528. [23] Stefan Savage, Michael Burrows, Greg Nelson, Patrick Sobalvarro, and Thomas Anderson. 1997. Eraser: a dynamicdata race detector for multithreaded programs. ACM Trans. Comput. Syst. 15, 4 (nov 1997), 391411. https://doi.org/10.1145/265924.265927. [24] Konstantin Serebryany and Timur Iskhodzhanov. 2009. ThreadSanitizer: data race detection in practice. In Proceedingsof the Workshop on Binary Instrumentation and Applications (New York, New York, USA) (WBIA 09). Association forComputing Machinery, New York, NY, USA, 6271. https://doi.org/10.1145/1791194.1791203. [25] Tianwei Sheng, Neil Vachharajani, Stephane Eranian, Robert Hundt, Wenguang Chen, and Weimin Zheng. 2011.RACEZ: a lightweight and non-invasive race detection tool for production applications. In 2011 33rd InternationalConference on Software Engineering (ICSE). 401410. https://doi.org/10.1145/1985793.1985848. [26] Yan Shoshitaishvili, Ruoyu Wang, Christopher Salls, Nick Stephens, Mario Polino, Andrew Dutcher, John Grosen,Siji Feng, Christophe Hauser, Christopher Kruegel, and Giovanni Vigna. 2016. SOK: (State of) The Art of War:Offensive Techniques in Binary Analysis. In 2016 IEEE Symposium on Security and Privacy (SP). 138157.https://doi.org/10.1109/SP.2016.17. [27] Yannis Smaragdakis, Jacob Evans, Caitlin Sadowski, Jaeheon Yi, and Cormac Flanagan. 2012. Sound predictive racedetection in polynomial time. In Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles ofProgramming Languages (Philadelphia, PA, USA) (POPL 12). Association for Computing Machinery, New York, NY,USA, 387400. https://doi.org/10.1145/2103656.2103702. [28] Jan Wen Voung, Ranjit Jhala, and Sorin Lerner. 2007. RELAY: static race detection on millions of lines of code.In Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFTSymposium on The Foundations of Software Engineering (Dubrovnik, Croatia) (ESEC-FSE 07). Association for ComputingMachinery, New York, NY, USA, 205214. https://doi.org/10.1145/1287624.1287654. [29] Jie Yu and Satish Narayanasamy. 2009. A case for an interleaving constrained shared-memory multi-processor.In Proceedings of the 36th Annual International Symposium on Computer Architecture (Austin, TX, USA) (ISCA 09).Association for Computing Machinery, New York, NY, USA, 325336. https://doi.org/10.1145/1555754.1555796. [30] Tong Zhang, Changhee Jung, and Dongyoon Lee. 2017. ProRace: Practical Data Race Detection for Production Use. InProceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages andOperating Systems (Xian, China) (ASPLOS 17). Association for Computing Machinery, New York, NY, USA, 149162.https://doi.org/10.1145/3037697.3037708. 19. Conference acronym XX, ,Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Xuandong Li, and Zhiqiang Zuo. [31] Tong Zhang, Dongyoon Lee, and Changhee Jung. 2016. TxRace: Efficient Data Race Detection Using CommodityHardware Transactional Memory. In Proceedings of the Twenty-First International Conference on Architectural Supportfor Programming Languages and Operating Systems (Atlanta, Georgia, USA) (ASPLOS 16). Association for ComputingMachinery, New York, NY, USA, 159173. https://doi.org/10.1145/2872362.2872384. [32] Zhiqiang Zuo, Kai Ji, Yifei Wang, Wei Tao, Linzhang Wang, Xuandong Li, and Guoqing Harry Xu. 2021. JPortal:precise and efficient control-flow tracing for JVM programs with Intel processor trace. In Proceedings of the 42nd ACMSIGPLAN International Conference on Programming Language Design and Implementation (Virtual, Canada) (PLDI 2021).Association for Computing Machinery, New York, NY, USA, 10801094. https://doi.org/10.1145/3453483.3454096. 20", "2410.18416v1.SkiLD__Unsupervised_Skill_Discovery_Guided_by_Factor_Interactions.pdf": "SkiLD: Unsupervised Skill DiscoveryGuided by Factor Interactions. Zizhao Wang1Jiaheng Hu1Caleb Chuck1Stephen Chen1. Roberto Martn-Martn1 Amy Zhang1Scott Niekum2Peter Stone1,3. 1University of Texas at Austin2University of Massachusettes, Amherst3Sony AI{zizhao.wang,jiahengh,stephen.chen,robertomm}@utexas.eduamy.zhang@austin.utexas.edu, sniekum@umass.edu, {calebc,pstone}@cs.utexas.edu. Abstract. Unsupervised skill discovery carries the promise that an intelligent agent can learnreusable skills through autonomous, reward-free environment interaction. Existingunsupervised skill discovery methods learn skills by encouraging distinguishablebehaviors that cover diverse states. However, in complex environments withmany state factors (e.g., household environments with many objects), learningskills that cover all possible states is impossible, and naively encouraging statediversity often leads to simple skills that are not ideal for solving downstreamtasks. This work introduces Skill Discovery from Local Dependencies (SkiLD),which leverages state factorization as a natural inductive bias to guide the skilllearning process. The key intuition guiding SkiLD is that skills that induce diverseinteractions between state factors are often more valuable for solving downstreamtasks. To this end, SkiLD develops a novel skill learning objective that explicitlyencourages the mastering of skills that effectively induce different interactionswithin an environment. We evaluate SkiLD in several domains with challenging,long-horizon sparse reward tasks including a realistic simulated household robotdomain, where SkiLD successfully learns skills with clear semantic meaning andshows superior performance compared to existing unsupervised reinforcementlearning methods that only maximize state coverage. Code and visualizations areat https://wangzizhao.github.io/SkiLD/.. 1Introduction. Reinforcement learning (RL) achieves impressive successes when solving decision-making problemswith well-defined reward functions [65, 20, 33]. However, designing this reward function is often nottrivial [7, 61]. In contrast, humans and other intelligent creatures can learn, without external rewardsupervision, behaviors that produce repeatable and predictable changes in the environment [18].These behaviors, which we call skills, can be later repurposed to solve downstream tasks efficiently.One of the promises of this form of unsupervised RL is to endow artificial agents with similarcapabilities to discover reusable skills without explicit rewards.. One predominant strategy of prior skill discovery methods focuses on training skills to reach diversestates while being distinguishable [19, 59, 50]. However, in complex environments that contain manystate factorsdistinct elements such as individual objects in a household (a formal description inSec. 2.1), the exponential number of distinct states makes it impossible to learn skills that coverevery state. Consequently, these methods typically result in simple skills that only change theeasy-to-control factors (e.g., in a manipulation task moving the agent itself to diverse positions or. Indicates equal contribution. 38th Conference on Neural Information Processing Systems (NeurIPS 2024).. arXiv:2410.18416v1  [cs.LG]  24 Oct 2024. Figure 1: Skill Discovery from Local Dependencies (SkiLD) describes skills that encode interactions(i.e., local dependencies) between state factors. In contrast to prior diversity-based methods thatcan easily get stuck by moving the robot to diverse, but non-interactive states, and factor-basedmethods that are trained to manipulate the hammer and nail, but not their interactions, SkiLD not onlymanipulate each object (left, middle) but also induce interactions between them (right), by specifyingdifferent local dependencies. These skills are often more useful than the easy skill learned byprevious methods for downstream task-solving.. manipulating each factor independently), and fail to cover other desirable but challenging behaviors.Meanwhile, in a factored state space, many downstream tasks require inducing interactions betweenstate factors, e.g., cooking requires using a knife to cut the ingredients and cooking them in a pan, etc.Unsurprisingly, these simple skills often struggle to solve such tasks, resulting in poor downstreamperformance.. Our key insight is to utilize interactions between state factors as a powerful inductive bias for learninguseful skills. In factored state spaces and their downstream tasks, there usually exist bottleneck statesthat an agent must pass through to explore different regions of the environment, and many of themcan be characterized by interactions between state factors. For example, in a household environment,a robot must first grasp the knife before moving it to different locations, with the bottleneck beingthe interaction between the robot and the knife. In environments that have a large state space due tomany state factors, rather than inefficiently relying on randomly visiting different states to reach suchbottlenecks, we propose to train the agent to actively induce these critical interactions.. To this end, we introduce Skill Discovery from Local Dependencies (SkiLD), a novel skill discoverymethod that explicitly learns skills that induce diverse interactions. Specifically, SkiLD models theinteractions between state factors using the framework of local dependencies (where local refers tostate-specific, see details in Sec. 2.2) and proposes a novel intrinsic reward that 1) encourages the agentto induce specified interactions, and 2) encourages the agent to discover diverse ways of inducingspecified interaction, as visualized in Figure 1. During skill learning, SkiLD gradually discoversnew interactions and learns to induce them, based on the skills that it already mastered, resultingin a diverse set of interaction-inducing behaviors that can be readily repurposed for downstreamtasks. During task learning, the skill policy is reused, and a task-specific policy is learned to select (asequence of) skills to maximize task rewards efficiently.. We evaluate the performance of SkiLD on factor-rich environments with 10 downstream tasks againstexisting unsupervised reinforcement learning methods. Our experiments indicate that SkiLD learnsto induce diverse interactions and outperforms other methods on most of the examined tasks.. 2Background. In this paper, our unsupervised skill discovery method is set up in a factored Markov decision processand builds off previous diversity-based methods, as described in Sec. 2.1. To enhance the expressivityof skills, our method further augments the skill representation with interactions between state factors,which we formalize as local dependencies as described in Sec. 2.2.. 2.1Factored Markov Decision Process (Factored MDP). We consider unsupervised skill discovery in a reward-free Factored Markov Decision Process [8]defined by the tuple M = (S, A, p). S = S1      SN is a factored state space with N subspaces,where each subspace Si is a multi-dimensional continuous or discrete random variable. Then,correspondingly, each state s  S consists of N state factors, i.e., s = (s1, . . . , sN), si  Si. In thispaper, we use uppercase letters to denote random variables and lowercase for their specific values (e.g.,. 2. S denotes the random variable for states s). A is the action space, and p is an unknown Markoviantransition model that captures the probability distribution over the next state S  p(|S, A).. The factorization in S inherently exists in many environments, and is a common assumption in priorunsupervised skill discovery works [22, 29, 27]. For example, in robotics, an environment typicallyconsists of a robot and several objects to manipulate, and, for each object, Si would represent itsattributes of interest, like pose. In this work, we explore how we can utilize a given state factorizationto improve unsupervised skill discovery. In practice, the factorization can either be directly providedby the environment or obtained from image observations with existing disentangled representationlearning methods [47, 31].. Following prior work, our method consists of two stagesskill learning and task learning. During theskill learning phase, we seek to learn a skill policy (|s, z), which defines a conditional distributionover actions given the current state s and some skill representation z, where skills indicate the desiredbehaviors of the agent. Once the skills are learned, they can be chained together to solve downstreamtasks during the task learning phase through an extrinsic reward-optimizing policy. During tasklearning, a downstream task reward function r : S  A  R is provided by the environment. Ahigh-level policy (z|s) is then trained to optimize the expected return through outputting correctskills z given state s.. 2.2Identifying Local Dependencies between State Factors. A key insight of SkiLD is to utilize interactions between state factors (or, formally, local dependencies)as part of the skill representation. In later sections, these local dependencies are compiled into abinary matrix G(s, a, s) = {0, 1}N(N+1) representing the local dependencies between all factors.In this section, we first formally define local dependencies, introduce their identification, and finallydiscuss their application to factored MDPs.. SkiLD takes a causality-inspired approach for defining and detecting local dependencies [6, 58],where we use local to refer to a particular assignment of values for a random variable, as opposed toglobal which applies to all values. Formally, for an event of interest Y = y and its potential causesX = (X1, . . . , XN), given the value of X = x, local dependencies focus on which Xis are thestate-specific cause of the outcome event Y = y (for simplicity of presentation, in this section weoverload N as the number of potential causes rather than number of variables and p as the transitionfunction according to a subset of the variables). Formally, we denote the general data generationprocess of Y as p : X  Y and the data generation process when Y is only influenced by a subset ofX as p X : X  Y , where X  X. Then, given the value of all variables, X1 = x1,    , XN = xNand Y = y, we say Y locally depends on X, if X is the minimal subset of X such that knowing theirvalues is necessary and sufficient to generate the result of Y = y, i.e.,. arg minXX| X|s.t.pX(Y = y| X = x) = p(Y = y|X = x),(1). where | X| is the number of variables in X. For example, suppose that a robot opens a refrigeratordoor in a particular transition. The event of interest Y = y is the refrigerator door becoming open,and it locally depends on two factors: the robot and the refrigerator door, while other state factorssuch as objects inside the refrigerator do not locally influence Y .. To identify local dependencies, one can conduct a conditional independence test y  xi|{x/xi} toexamine whether a variable Xi is necessary for predicting Y = y. In prior works, one form of thistest is to examine whether the pointwise conditional mutual information (pCMI) is greater than 0,. pCMI(y; xi|{x/xi}) = logp(y|x). p{X/Xi}(y|{x/xi}) > 0.(2). If so, then it suggests that knowing Xi = x provides additional information about Y that is notpresent in {X/Xi}, and Y locally depends on Xi. As the data generation processes are generallyunknown, one has to approximate them with learned models. Recent work in RL has utilized variousapproximations such as attention weights [53], Granger causality [14], and input gradients [63].. In this work, for a transition (S = s, A = a, S = s), the event of interest is each next state factorbeing (Si) = (si), and we infer whether it locally depends on each state factor Sj and the action A(i.e., whether there is an interaction between state factors i and j, where factor j influences i). Then. 3. graph-selection policy. skill policy. Env. desired localdependencies. diversityindicator. dynamics. modelEq. (4). Eq. (3). task policy. frozen skill policy. Env. Skill Learning PhaseTask Learning Phase. Figure 2: During skill learning of SkiLD, the graph-selection policy specifies desired local depen-dencies for the skill policy to induce, and the induced dependency graph is identified by the dynamicsmodel and used to update both policies. During task learning (right), the skill policy is kept frozenand a task policy is trained to select skills to maximize task reward.. we aggregate all local dependencies into a state-specific dependency graph (abbreviated in this work todependency graph). This overall dependency graph is represented with G(s, a, s) = {0, 1}N(N+1),and an edge Gij(s, a, s) denotes, during the transition (s, a, s), that state factor (si) (the Y = y)locally depends on sj (one of the Xj):. Gij := pCMI((xi); xj|{x/xj}) > 0(3). This graph is used to enhance skill representation, as explained in detail in Section 3.. 3Skill Discovery from Local Dependencies (SkiLD). In this section, we describe SkiLD, which enhances skills using local dependencies. SkiLD representslocal dependencies as state-specific dependency graphs, defined in Sec. 2.2, and learns to inducedifferent dependency graphs in the environment for different skills. To intelligently generate targetdependency graphs during training, SkiLD frames unsupervised skill discovery as a hierarchical RLproblem described in Fig. 2 and Alg. 1, where a high-level graph selection policy chooses target localdependencies to guide exploration and skill learning, and a graph-conditioned skill policy learns toinduce the specified local dependencies using primitive actions.. This framerwork requires formalizing three components: (1) the skill representation Z, presented inSec. 3.1, (2) the graph selection policy G(z|s) and its reward function RG, presented in Sec. 3.2, and(3) the skill policy skill(a|s, z) and its corresponding reward function Rskill, presented in Sec. 3.3.. 3.1Skill Representation Z. Prior unsupervised skill discovery methods usually focus skill learning on changing the state or eachfactor diversely, which is inefficient when there exist bottleneck states for explorations. Consequently,they are can be limited to learning simple skills, for example, only changing the easiest-to-controlfactor in the state (i.e., the agent itself). To address this problem, SkiLD not only focuses on changingthe state but also considers the interactions between state factors.. Skill Representation. SkiLD represents the skill space as the combination of two components:Z = G  B, where g  G is a state-specific dependency graph that specifies the desired localdependencies between state factors (e.g., hammering the nail), and b  B is a diversity indicator thesame as that used in Eysenbach et al. [19]. While the agent inducing particular local dependenciesg, we use b to further encourage it to visit distinguishable states (e.g., under different b values,training the agent to hammer the nail into different locations). Specifically, the dependency graph isrepresented as a binary matrix G = {0, 1}N(N+1). As described in Sec. 2.2, each edge Gij denotes,during the transition (s, a, s), whether the state factor (si) locally depends on sj. The diversityindicator B can be discrete or continuous. In this work, without loss of generality, we follow theprocedure of Eysenbach et al. [19] and use a discrete b sampled uniformly from {1, . . . , K}, whereK is a predefined number.. Given this skill space, SkiLD learns the skills as a skill-conditioned policy skill : S  Z  A, whereskill is trained to reach diverse states while ensuring that local dependencies specified by the graph g. 4. Algorithm 1 SkiLD Skill Discovery. 1: Initialize the high-level graph-selection policy G : S  G, the low-level skill policy skill :S  Z  A, the diversity indicator discriminator q : S  G  p(B), and the dynamics modelf : S  A  S, graph selection interval L.. 2: for each skill training timestep i do3:// data collection. 4:if i % L == 0 then. 5:Sample the target dependency graph g  G(s).. 6:Sample the diversity indicator from uniform distribution b  Uniform(B).. 7:Compose the skill variable z = (g, b).. 8:end if. 9:Collect state transitions (s, z, a, s) with actions from skill(a|s, z).. 10:Infer the induced dependency graph ginduced(s, a, s) using the dynamics model f (Sec. 3.1).. 11:Update the history of the seen graphs with ginduced(s, a, s).. 12:// training. 13:Sample a batch of (s, z, a, s) from the replay buffer.. 14:Update the dynamics model f(s, a) by minimize the prediction error w.r.t. s.. 15:Update the high-level policy with reward RG (Eq. 5) with the history of seen graphs.. 16:Update the discriminator q(b|s, g) with the discrimination (cross-entropy) loss.. 17:Infer the induced dependency graph ginduced(s, a, s) using the dynamics model f (Sec. 3.1).. 18:Infer the diversity reward Rdiversity = log q(b|s, g).. 19:Update skill with Rskill = 1[ginduced(s, a, s) = g]  (1 + Rdiversity) (Eq. 4).. 20: end for. are induced. Before we describe skill training in Sec. 3.3, we first discuss how to select the skill z forskill to follow during the skill learning stage.. 3.2High-Level Graph-Selection Policy G. To acquire skills that are useful for downstream tasks, the skill policy skill needs to learn to induce awide range of local dependencies sample-efficiently. To this end, we propose to learn a graph-selectionpolicy G : S  G to guide the training of skill. Specifically, training skill requires a wise selectionof graphs  as graph space G increases super-exponentially in the number of state factors N, manygraphs are not inducible. To this end, we only select target graphs for the skill policy from a historyof all seen graphs. As the agent learns to induce existing graphs in diverse ways, new graphs may beencountered, gradually expanding the set of seen graphs.. However, though this history guarantees graph inducibility, two challenges still remain: (1) How toefficiently explore novel local dependencies, especially hard-to-visit ones? (2) For all seen graphs,which one should skill learn next to maximize training efficiency? We address these challengesbased on the following insight  compared to well-learned skills, skill should focus its training onunderdeveloped skills. Meanwhile, learning new skills opens up the possibility of visiting novel localdependencies, e.g., learning to grasp the hammer makes it possible for the robot to hammer the nail.. According to this idea, we learn a graph-selection policy G that guides the exploration and trainingof the skill policy skill. Specifically, G : S  G selects a new dependency graph the skill policyshould induce for the next L time steps. To increase the likelihood of visiting hard graphs, G istrained to maximize the following graph novelty reward. RG =1. C(gvisited),(4). where C(gvisited) is the number of times that we have seen the graph in the collected transition.. 3.3Low-Level Skill Policy skill. Given the skill parameter z from the graph-selection policy, SkiLD learns skills as a skill-conditionedpolicy skill : S  Z  A, where skill learns to reach diverse states while ensuring that the localdependencies specified by g are induced. During skill learning, we select actions by iteratively calling. 5. the skill policy skill, and we denote ginduced(s, a, s) as the graph that describes the local dependenciesinduced in a transition (s, a, s) when executing a selected action a. We design the reward functionof the skill policy as:. Rskill = 1[ginduced(s, a, s) = g]  (1 + Rdiversity),(5). where 1[ginduced(s, a, s) = g] measures whether the induced dependency graph matches the desiredgraph, Rdiversity is the weighted diversity reward that further encourages visiting diverse states whenthe desired graph is induced, and  is the coefficient of diversity reward. In the following paragraphs,we describe how we infer ginduced(s, a, s) and estimate Rdiversity for each transition.. Inferring Induced Graphs. To infer the induced graph for a transition (S = s, A = a, S = s),we need to determine, for each (S)i, whether it locally depends on each factor Sj and the actionA. Following Sec. 2.2, we evaluate the conditional dependency (si)  sj|{s/sj, a} by examiningwhether their pointwise conditional mutual information (pCMI) is greater than a predefined threshold. If pCMIij =p((si)|s,a). p((si)|{s/sj,a})  , it suggests that sj is necessary to predict (si) and thus the localdependency exists. Meanwhile, as the transition probability p is unknown, we approximate it with alearned dynamics model that is trained to minimize prediction error.. Finally, after obtaining the induced dependency graph, we evaluate 1[ginduced(s, a, s) = g] byexamining whether each edge gijinduced matches the corresponding edge in the desired graph gij. AsRskill only provides sparse rewards to the skill policy when the desired graph is induced, we usehindsight experience replay [2] to enrich learning signals, by relabelling induced graphs as desiredgraphs in some episodes.. Diversity Rewards. When the skill policy induces the desired graph, Rdiversity further encouragesit to visit different distinguishable states under different diversity indicators b, e.g., hammeringthe nail to different locations. This diversity enhances the applicability of learned skills. To thisend, we design the diversity reward Rdiversity as the forward mutual information between visitedstates and the diversity indicator I(s; b), following DIAYN. To estimate the mutual information, weapproximate it with a variational lower bound I(s; b)  Eb,s log q(b|s), where q(b|s) is a neuralnetwork discriminator trained to predict the diversity indicator b from the visited state.. In practice, rather than learning a single low-level skill to handle all graphs, SkiLD utilizes a factorizedlower-level policy. When the target dependency graph is specified, SkiLD identifies which state factorshould be influenced and uses its corresponding policy to sample primitive actions. More detailsabout this subdivision can be found in Appendix A.. 3.4Downstream Task Learning. In SkiLD, after the skill learning stage, we utilize hierarchical RL to solve reward-superviseddownstream tasks with the discovered skills. The skill policy, skill acts as the low-level policy whilea task policy, task : S  Z, is learned to select which skill z = (g, b) to execute for L steps.Compared to diversity-based skills that are limited to simple behaviors, our local-dependency-basedskills enable a wide range of interactions between state factors, leading to more efficient explorationand superior performance of downstream task learning.. 4Experiments. In this section we aim to provide empirical evidence towards the following questions: Q1) Do theskills learned by SkiLD induce a diverse set of interactions among state factors? Q2) Do the skillslearned by SkiLD enable more efficient downstream task learning compared to other unsupervisedreinforcement learning methods? Our learned skills are visualized at https://sites.google.com/view/skild/.. 4.1Domains. In this work, we focus on addressing the challenge of vast state space brought by the number of statefactors. Hence, we evaluate our method on two challenging object-rich embodied AI benchmarks:Mini-behavior [32] and Interactive Gibson [42].. 6. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. (a) Thawing. <image: DeviceRGB, width: 299, height: 299, bpc: 8>. (b) Cleaning Car. <image: DeviceRGB, width: 731, height: 477, bpc: 8>. (c) Interactive Gibson. Figure 3: Evaluation domains: Mini-behavior: Installing Printer, Thawing and Cleaning Car, andiGibson.. The Mini-behavior (Mini-BH) domain [32] (Figure 3a) contains a set of gridworld environmentswhere an agent can move around and interact with a variety of objects to accomplish certain householdtasks. While conceptually simple, due to highly sequentially interdependent state factors (see detailsin the Appendix), this domain has been shown to be extremely challenging for the agents explorationability, especially under sparse reward [32]. Each Mini-BH environment contains different objectsand different success criteria. We tested on three particular environments in Mini-behavior, including:. Installing Printer: A relatively simple environment with three state factors: the agent, a table, anda printer that can be installed.. Cleaning Car: An environment where the objects have rich and complex interactions. The statefactors include the agent, a toggleable sink, a piece of rag that can be soaked in the sink, a car thatthe rag can clean, a soap and a bucket which can together be used to clean the rag.. Thawing: An environment with lots of movable objects. The state factors include the agent, a sink,a fridge that can be opened, and three objects that can be thawed in the sink: fish, olive, and a date.. The Interactive Gibson (iGibson) domain [43] (Figure 3b) contains a realistic simulated FetchRobot that operates in a kitchen environment with a refrigerator, sink, knife, and peach. The peachcan be washed or cut. This domain is very difficult especially when using low-level motor commandsbecause much of the domain is free space, meaning that only a minute fraction of action sequenceswill manipulate the objects meaningfully.. Both Mini-BH and iGibson require learning long-horizon policies spanning many low-level actionsfrom sparse reward, making these challenging domains (see details in Appendix).. 4.2Baselines. Before evaluating the empirical questions, we provide a brief description of the baselines. Thesebaselines include unsupervised skill learning, and causal and hierarchical methods.. Diversity is all you need (DIAYN [19]): This method learns unsupervised state-covering skills usinga mutual information objective. SkiLD utilizes a version of this for state-diversity skills modulated bya desired dependency graph. This baseline determines how incorporating graph information affectsthe algorithm.. Controllability-Aware Skill Discovery (CSD [50]): Extends DIAYN with a factorization based oncontrollability. This baseline is a comparable skill learning method that leverages state factorizationbut does not encode local dependencies.. Exploration via Local Dependencies (ELDEN [63]): This method utilizes gradient-based techniquesto infer local dependencies for exploration. However, without a skill learning component, it canstruggle to chain together complex behavior.. Chain of Interaction Skills (COInS [14]): This is a hierarchical algorithm that constructs a chainof skills using Granger-causality to identify local dependencies. Because it is restricted to pairwiseinteractions, it struggles to represent the rich policies necessary for these tasks.. Vanilla RL: This baseline uses PPO [57] to directly train an agent with the extrinsic reward. Unlikeother baselines, this method does not have a pertaining phase. Since all the task rewards are sparseand the tasks are often long horizon, vanilla RL often struggles.. 7. 4.3Interaction Graph Diversity. visitation likelihood (%). bucket, rag -> rag. car, rag -> rag. bucket, soap -> bucket. car, rag -> car. sink, rag -> rag. agent, sink, rag, action -> rag. agent, sink, action -> sink. agent, soap, action -> soap. agent, rag, action -> rag. agent, action -> agent. 0.5. 13.7. 16.6. 19.0. 25.5. 33.9. 64.8. 76.1. 92.5. 100.0. 0.0. 0.0. 0.1. 0.0. 0.4. 0.1. 47.9. 13.7. 32.4. 100.0. 0.0. 0.0. 0.0. 0.0. 0.0. 0.0. 2.5. 0.9. 9.0. 100.0. SkiLD (ours)CSDDIAYN. Figure 4: The percentage of episodes where a de-pendency graph is induced through random skillsampling. Standard deviation is calculated acrossfive random seeds.. We first evaluate whether SkiLD is indeed ca-pable of achieving complex interaction graphs(Q1), comparing against two strong skill discov-ery baselines introduced earlier: DIAYN andCSD.. Each of these methods is trained for 10 Millionsteps without having access to any reward. Thento evaluate their learned skills, we unroll each ofthem for 500 episodes with randomly sampledskills z and examine the diversity of the interac-tion graphs they can induce. Figure 4 illustratesthe percentages of episodes where some hardlocal dependencies have been induced at leastonce, in Mini-BH Cleaning Car (for simplic-ity of presentation, see Appendix for results onall inducible local dependency graphs and theirmeanings). We find that DIAYN and CSD arelimited to skills that only manipulate one ob-ject individually, for example, picking up therag (agent, rag, action  rag) or the soap (agent,soap, action  soap). By contrast, SkiLD learnsto induce more complicated causal interactions,such as soaking the rag in the sink (sink, rag rag) and cleaning the car with the soaked mug (car, rag  car).. 4.4Sample Efficiency and Performance. Next, we evaluate whether the local dependency coverage provided by SkiLD leads to a performanceboost in downstream task learning under the same number of environment interactions (Q2). Wefollow the evaluation setup in the unsupervised reinforcement learning benchmark [38], where fora given environment, an agent is first pre-trained without access to task reward for Kpt steps, andthen finetuned for Kft steps. Importantly, the same pre-trained skills are reused on multiple distinctdownstream tasks within the same environment, so that only the upper-level skill-selection policyis task-specific. We have Kpt = 2M, Kft = 1M for installing printer, Kpt = 10M, Kft = 5M forthawing and cleaning car, and Kpt = 4M, Kft = 2M for iGibson, and evaluate each method for eachtask across 5 random seeds. Hyperparameter details can be found in Appendix D. Specifically, weevaluate on the following downstream tasks:. Installing Printer: We have a single downstream task in this environment, where the agent needsto pick up the printer, put it on the table, and turn it on.. Thawing: We have three downstream tasks: thawing the fish or the olive or the date. Cleaning Car: We consider three downstream tasks, where each task is a pre-requisite of thefollowing one. The tasks are: soak the rag in the sink; clean the car with the rag; and clean the dirtyrag using the soap in the bucket.. IGibson: The tasks for this domain are: grasping the peach, washing the peach in the sink, andcutting the peach with a knife.. After skill learning, we train a new upper-level policy that uses z as actions and is trained with extrinsicreward, as described in Section 3.4. Figure 5 illustrates the improvement of SkiLD as compared toother methods. Without combining dependency graphs with skill learning, other methods strugglewith any but the simpler tasks. COInS performs poorly because of its chain structure, which restrictsthe agent controlling policy from picking up objects. ELDENs exploration reaches graphs, butwithout skills struggles to utilize that information in downstream tasks. DIAYN learns skills, but fewmanipulate the objects, so a downstream model struggles to utilize those skills to achieve meaningfulrewards. By comparison, SkiLD achieves superior performance on 9 of the 10 downstream tasksevaluated. In the two hardest tasks which require a very long sequence of precise controls, Clean Rag. 8. 00.5M1.0M0.0. 0.5. 1.0. Success Rate. (a) Install Printer. 02.5M5.0M0.0. 0.2. 0.4. (b) Thaw Olive. 02.5M5.0M0.0. 0.5. 1.0. (c) Thaw Fish. 02.5M5.0M0.0. 0.5. 1.0. (d) Thaw Date. 02.5M5.0M0.0. 0.5. 1.0. (e) Soak Rag. 02.5M5.0M0.0. 0.1. 0.2. Success Rate. (f) Clean Car. 02.5M5.0M0.00. 0.03. 0.06. (g) Clean Rag. 01.0M2.0M0.0. 0.5. 1.0. (h) Grasp Peach. 01.0M2.0M0.0. 0.5. 1.0. (i) Wash Peach. 01.0M2.0M0.0. 0.1. 0.2. (j) Cut Peach. Figure 5: Training curves of SkiLD and baselines on multiple downstream tasks (reward supervisedsecond phase). Each curve depicts the mean and standard deviation of the success rate over 5 randomseeds. SkiLD outperforms all baselines for most tasks, converging faster and to higher returns.. and Cut Peach, SkiLD is the only method that can achieve a non-zero success rate (although still farfrom fully mastering the tasks), showcasing the potential of local dependencies for skill learning.. 4.5Graph and Diversity Ablations. We also explore the functionality of the graph and diversity components of the skill parameter zby assessing the downstream performance of SkiLD without these components. This produces twoablative versions of SkiLD: SkiLD without diversity and SkiLD without dependency graphs. Toisolate learning from the effect of learned local dependencies, we use ground truth dependencygraphs for ablative evaluations where relevant. In Figure 6, learning without graphs results in zeroperformance, consistent with DIAYN results. In addition, removing diversity produces a notabledecline in performance, especially on more challenging tasks like clearning the rag. These evaluationsdemonstrate that SkiLD benefits from both the incorporation of dependency graphs and diversity.. 5Related Work. This work lies in the unsupervised skill learning framework [37], where the agent must discover aset of useful skills which are reward independent. It then extends these skills to construct a 2-layerhierarchical structure [60], where the upper policy receives reward both for achieving novel skills,and can then be tuned to utilize the learned skills to accomplish an end task. Finally, the skills areidentified using token causality, a specific problem identified in causal literature.. 5.1Unsupervised Skill Learning. This work describes a framework for utilizing local dependency graphs and diversity to discoverunsupervised skills. Diversity-based state coverage skills have been explored in literature [19]utilizing forward and backward mutual information techniques to learn a goal space Z, and a skillencoder q(z|) [11]. This unsupervised paradigm has been extended with Lipschitz constraints [49],contrastive objectives [39], information bottleneck [35], population based methods such as particleestimation [45], quality diversity [44] and mixture of experts [12]. These skills can then be used forhierarchical policies or planners [56, 67, 23], which mirrors the same structure as SkiLD. Unlikethese methods, SkiLD adds additional subdivision through dependency graphs, which mitigates thecombinatorial explosion of skills that can result from trying to cover a large factored space.. 5.2Hierarchical Reinforcement Learning. The hierarchical policy structure in SkiLD where a higher level policy passes a parameter to be inter-preted by low-level planners has been formalized in [60], and learned using deep networks utilizing. 9. 02.5M5.0M0.0. 0.5. 1.0. (a) Soak Rag. 02.5M5.0M0.0. 0.5. 1.0. (b) Clean Car. 02.5M5.0M0.00. 0.05. 0.10. (c) Clean Rag. Figure 6: A figure illustrating the ablative performance of SkiLD without diversity or without graphs.Each curve depicts the mean and standard deviation of the success rate over 5 random seeds. Withoutgraphs, the method collapses completely, while removing diversity results in a noticeable reductionin downstream performance.. extrinsic reward [3, 62], attention mechanisms [16], initiation critera [34, 4] and deliberation cost [26].Hierarchies of goal-based policies [40] has been extended with object-centric representations [66],offline data [48], empowerment [41] and goal counts [51]. In practice, SkiLD uses graph and diversityparameters similar to goal-based methods. However, the space of goals can often be intractablelarge, and methods to address this use graph laplacians [36] causal chains [13, 14] or general causalrelationships [29]. SkiLD is similar to these causal methods but utilizes local dependence along withgeneral two-layer architectures, thus showing increased generalizability.. 5.3Causality in Reinforcement Learning. This work investigates the application of local dependency to hierarchical reinforcement learning.This kind of reasoning has been described as local causality or interactions in prior RL workfor data augmentation [53, 54], learning skill chains [13, 14] and exploration [63]. This work isthe first synthesis of unsupervised skill learning and local dependencies applied to general 2-layerhierarchical reinforcement learning. Other general causality work investigates action-influencedetection [58, 28], affordance learning [10], model learning [30, 21], critical state identification [46],and disentanglement [17]. In the context of relating local dependency and causal inference, weprovide a discussion in Appendix C. SkiLD incorporates causality-inspired local dependence to skilllearning, resulting in a set of diverse skills.. 6Conclusion. Unsupervised skill discovery is a powerful tool for learning useful skills in long-horizon sparsereward tasks. However, many unsupervised skill-learning methods do not take advantage of factoredenvironments, resulting in poor performance in complex environments with several objects. SkillDiscovery from Local Dependencies utilizes state-specific dependency graphs, identified usinglearned pointwise conditional mutual information models, to guide skill discovery. The frameworkof defining skills according to a dependency graph and diversity goal, combined with a learnedsampling scheme, achieves difficult downstream tasks. In domains where hand-coded primitiveskills are typically given to the agent, like Mini-behavior and Interactive Gibson, SkiLD can achievehigh performance without requiring explicit domain knowledge. These impressive results ariseintuitively from incorporating local dependencies as skill targets, illuminating a meaningful directionfor unsupervised skill learning to be applied to a wider array of environments.. Limitations and Future Work An important assumption of SkiLD is its access to factored statespace. While factored state space can often be naturally obtained from existing RL benchmarksand many real-world environments, developments in disentangled representation learning [47, 31]will help with extending SkiLD to unfactored image domains. Secondly, SkiLD requires accuratedetection of local dependencies. While off-the-shelf methods [63, 58] work well for detecting localdependencies in our experiments, future works that can more accurately detect local dependencieswill be beneficial to the performance of SkiLD.. 10. 7Acknowledgement. This work has taken place in the Learning Agents Research Group (LARG) at the Artificial In-telligence Laboratory, The University of Texas at Austin. LARG research is supported in part bythe National Science Foundation (FAIN-2019844, NRT-2125858), the Office of Naval Research(N00014-18-2243), Army Research Office (W911NF-23-2-0004, W911NF-17-2-0181), LockheedMartin, and Good Systems, a research grand challenge at the University of Texas at Austin. Theviews and conclusions contained in this document are those of the authors alone. Peter Stone servesas the Executive Director of Sony AI America and receives financial compensation for this work. Theterms of this arrangement have been reviewed and approved by the University of Texas at Austin inaccordance with its policy on objectivity in research.. References. [1] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning withpolicy sketches. In International conference on machine learning, pages 166175. PMLR,2017.. [2] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experiencereplay. Advances in neural information processing systems, 30, 2017.. [3] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedingsof the AAAI conference on artificial intelligence, volume 31, 2017.. [4] Akhil Bagaria, Ben Abbatematteo, Omer Gottesman, Matt Corsaro, Sreehari Rammohan, andGeorge Konidaris. Effectively learning initiation sets in hierarchical reinforcement learning.Advances in Neural Information Processing Systems, 36, 2024.. [5] Sander Beckers, Hana Chockler, and Joseph Halpern. A causal analysis of harm. Advances inNeural Information Processing Systems, 35:23652376, 2022.. [6] Gianluca Bontempi and Maxime Flauder. From dependency to causality: a machine learningapproach. J. Mach. Learn. Res., 16(1):24372457, 2015.. [7] Serena Booth, W Bradley Knox, Julie Shah, Scott Niekum, Peter Stone, and Alessandro Allievi.The perils of trial-and-error reward design: misdesign through overfitting and invalid taskspecifications. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37,pages 59205929, 2023.. [8] Craig Boutilier, Thomas Dean, and Steve Hanks. Decision-theoretic planning: Structuralassumptions and computational leverage. Journal of Artificial Intelligence Research, 11:194,1999.. [9] Craig Boutilier, Nir Friedman, Moises Goldszmidt, and Daphne Koller. Context-specificindependence in bayesian networks. arXiv preprint arXiv:1302.3562, 2013.. [10] Jake Brawer, Meiying Qin, and Brian Scassellati. A causal approach to tool affordance learning.In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages83948399. IEEE, 2020.. [11] Vctor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Giro-i Nieto, andJordi Torres. Explore, discover and learn: Unsupervised discovery of state-covering skills. InInternational Conference on Machine Learning, pages 13171327. PMLR, 2020.. [12] Onur Celik, Dongzhuoran Zhou, Ge Li, Philipp Becker, and Gerhard Neumann. Specializingversatile skill libraries using local mixture of experts. In Conference on Robot Learning, pages14231433. PMLR, 2022.. [13] Caleb Chuck, Supawit Chockchowwat, and Scott Niekum. Hypothesis-driven skill discoveryfor hierarchical deep reinforcement learning. In 2020 IEEE/RSJ International Conference onIntelligent Robots and Systems (IROS), pages 55725579. IEEE, 2020.. 11. [14] Caleb Chuck, Kevin Black, Aditya Arjun, Yuke Zhu, and Scott Niekum. Granger-causalhierarchical skill discovery. arXiv preprint arXiv:2306.09509, 2023.. [15] Caleb Chuck, Sankaran Vaidyanathan, Stephen Giguere, Amy Zhang, David Jensen, and ScottNiekum. Automated discovery of functional actual causes in complex environments. arXivpreprint arXiv:2404.10883, 2024.. [16] Raviteja Chunduru and Doina Precup. Attention option-critic. arXiv preprint arXiv:2201.02628,2022.. [17] Oriol Corcoll and Raul Vicente. Disentangling controlled effects for hierarchical reinforcementlearning. In Bernhard Scholkopf, Caroline Uhler, and Kun Zhang, editors, Proceedings of theFirst Conference on Causal Learning and Reasoning, volume 177 of Proceedings of MachineLearning Research, pages 178200. PMLR, 1113 Apr 2022. URL https://proceedings.mlr.press/v177/corcoll22a.html.. [18] Yuqing Du, Eliza Kosoy, Alyssa Dayan, Maria Rufova, Pieter Abbeel, and Alison Gopnik. Whatcan ai learn from human exploration? intrinsically-motivated humans and agents in open-worldexploration. In NeurIPS 2023 workshop: Information-Theoretic Principles in Cognitive Systems,2023.. [19] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all youneed: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.. [20] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes,Mohammadamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser,Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcementlearning. Nature, 610(7930):4753, 2022.. [21] Fan Feng and Sara Magliacane. Learning dynamic attribute-factored world models for efficientmulti-object reinforcement learning. Advances in Neural Information Processing Systems, 36,2024.. [22] Pierre Fournier, Cedric Colas, Mohamed Chetouani, and Olivier Sigaud. Clic: Curriculumlearning and imitation for object control in nonrewarding environments. IEEE Transactions onCognitive and Developmental Systems, 13(2):239248, 2019.. [23] Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policiesfor hierarchical reinforcement learning. In International Conference on Machine Learning,pages 18511860. PMLR, 2018.. [24] Joseph Y Halpern. Actual causality. MIT Press, 2016.. [25] Joseph Y Halpern and Judea Pearl. Causes and explanations: A structural-model approach. parti: Causes. The British journal for the philosophy of science, 2005.. [26] Jean Harb, Pierre-Luc Bacon, Martin Klissarov, and Doina Precup. When waiting is not anoption: Learning options with a deliberation cost. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 32, 2018.. [27] Jiaheng Hu, Zizhao Wang, Peter Stone, and Roberto Martn-Martn. Disentangled unsupervisedskill discovery for efficient hierarchical reinforcement learning. In Workshop on ReinforcementLearning Beyond Rewards@ Reinforcement Learning Conference 2024.. [28] Jiaheng Hu, Peter Stone, and Roberto Martn-Martn. Causal policy gradient for whole-bodymobile manipulation. arXiv preprint arXiv:2305.04866, 2023.. [29] Xing Hu, Rui Zhang, Ke Tang, Jiaming Guo, Qi Yi, Ruizhi Chen, Zidong Du, Ling Li, Qi Guo,Yunji Chen, et al. Causality-driven hierarchical structure discovery for reinforcement learning.Advances in Neural Information Processing Systems, 35:2006420076, 2022.. [30] Yixuan Huang, Adam Conkey, and Tucker Hermans. Planning for multi-object manipulationwith graph neural network relational classifiers. In 2023 IEEE International Conference onRobotics and Automation (ICRA), pages 18221829. IEEE, 2023.. 12. [31] Jindong Jiang, Fei Deng, Gautam Singh, and Sungjin Ahn. Object-centric slot diffusion. arXivpreprint arXiv:2303.10834, 2023.. [32] Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and RobertoMartn-Martn. Mini-behavior: A procedurally generated benchmark for long-horizon decision-making in embodied ai. arXiv preprint arXiv:2310.01824, 2023.. [33] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Muller, Vladlen Koltun, andDavide Scaramuzza. Champion-level drone racing using deep reinforcement learning. Nature,620(7976):982987, 2023.. [34] Khimya Khetarpal, Martin Klissarov, Maxime Chevalier-Boisvert, Pierre-Luc Bacon, and DoinaPrecup. Options of interest: Temporal abstraction with interest functions. In Proceedings of theAAAI Conference on Artificial Intelligence, volume 34, pages 44444451, 2020.. [35] Jaekyeom Kim, Seohong Park, and Gunhee Kim. Unsupervised skill discovery with bottleneckoption learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th InternationalConference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,pages 55725582. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/v139/kim21j.html.. [36] Martin Klissarov and Marlos C. Machado. Deep Laplacian-based options for temporally-extended exploration. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara En-gelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th InternationalConference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,pages 1719817217. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/klissarov23a.html.. [37] Pawel Ladosz, Lilian Weng, Minwoo Kim, and Hyondong Oh.Exploration in deep re-inforcement learning: A survey.Information Fusion, 85:122, 2022.ISSN 1566-2535.doi: https://doi.org/10.1016/j.inffus.2022.03.003. URL https://www.sciencedirect.com/science/article/pii/S1566253522000288.. [38] Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang,Lerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark, 2021.. [39] Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and PieterAbbeel. Cic: Contrastive intrinsic control for unsupervised skill discovery. arXiv preprintarXiv:2202.00161, 2022.. [40] Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical reinforcement learning with hindsight.In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=ryzECoAcY7.. [41] Andrew Levy, Sreehari Rammohan, Alessandro Allievi, Scott Niekum, and George Konidaris.Hierarchical empowerment: Towards tractable empowerment-based skill-learning.arXivpreprint arXiv:2307.02728, 2023.. [42] Chengshu Li, Fei Xia, Roberto Martn-Martn, Michael Lingelbach, Sanjana Srivastava, BokuiShen, Kent Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, Andrey Kurenkov, C. KarenLiu, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, and Silvio Savarese. igibson 2.0: Object-centricsimulation for robot learning of everyday household tasks, 2021.. [43] Chengshu Li, Fei Xia, Roberto Martn-Martn, Michael Lingelbach, Sanjana Srivastava, BokuiShen, Kent Elliott Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, Andrey Kurenkov, KarenLiu, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, and Silvio Savarese. igibson 2.0: Object-centricsimulation for robot learning of everyday household tasks. In Aleksandra Faust, David Hsu,and Gerhard Neumann, editors, Proceedings of the 5th Conference on Robot Learning, volume164 of Proceedings of Machine Learning Research, pages 455465. PMLR, 0811 Nov 2022.URL https://proceedings.mlr.press/v164/li22b.html.. [44] Bryan Lim, Luca Grillotti, Lorenzo Bernasconi, and Antoine Cully. Dynamics-aware quality-diversity for efficient learning of skill repertoires. In 2022 International Conference on Roboticsand Automation (ICRA), pages 53605366. IEEE, 2022.. 13. [45] Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. Advancesin Neural Information Processing Systems, 34:1845918473, 2021.. [46] Haozhe Liu, Mingchen Zhuge, Bing Li, Yuhui Wang, Francesco Faccio, Bernard Ghanem,and Jurgen Schmidhuber. Learning to identify critical states for reinforcement learning fromvideos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages19551965, 2023.. [47] Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, andMichael Tschannen. Weakly-supervised disentanglement without compromises. In InternationalConference on Machine Learning, pages 63486359. PMLR, 2020.. [48] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchicalreinforcement learning. Advances in neural information processing systems, 31, 2018.. [49] Seohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, and Gunhee Kim. Lipschitz-constrained unsupervised skill discovery. In International Conference on Learning Representa-tions, 2021.. [50] Seohong Park, Kimin Lee, Youngwoon Lee, and Pieter Abbeel. Controllability-aware unsuper-vised skill discovery. arXiv preprint arXiv:2302.05103, 2023.. [51] Shubham Pateria, Budhitama Subagdja, Ah-Hwee Tan, and Chai Quek. End-to-end hierarchicalreinforcement learning with integrated subgoal discovery.IEEE Transactions on NeuralNetworks and Learning Systems, 33(12):77787790, 2021.. [52] Judea Pearl. Causality. Cambridge university press, 2009.. [53] Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locallyfactored dynamics. Advances in Neural Information Processing Systems, 33:39763990, 2020.. [54] Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. Mocoda: Model-basedcounterfactual data augmentation. Advances in Neural Information Processing Systems, 35:1814318156, 2022.. [55] David Poole and Nevin Lianwen Zhang. Exploiting contextual independence in probabilisticinference. Journal of Artificial Intelligence Research, 18:263313, 2003.. [56] Rafael Rodriguez-Sanchez and George Konidaris. Learning abstract world models for value-preserving planning with options. In NeurIPS 2023 Workshop on Generalization in Planning,2023.. [57] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximalpolicy optimization algorithms, 2017.. [58] Maximilian Seitzer, Bernhard Scholkopf, and Georg Martius. Causal influence detection forimproving efficiency in reinforcement learning. Advances in Neural Information ProcessingSystems, 34:2290522918, 2021.. [59] Wonil Song, Sangryul Jeon, Hyesong Choi, Kwanghoon Sohn, and Dongbo Min. Learningdisentangled skills for hierarchical reinforcement learning through trajectory autoencoder withweak labels. Expert Systems with Applications, page 120625, 2023.. [60] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: Aframework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181211, 1999.. [61] Chen Tang, Ben Abbatematteo, Jiaheng Hu, Rohan Chandra, Roberto Martn-Martn, and PeterStone. Deep reinforcement learning for robotics: A survey of real-world successes. arXivpreprint arXiv:2408.03539, 2024.. [62] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg,David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning.In International Conference on Machine Learning, pages 35403549. PMLR, 2017.. 14. [63] Zizhao Wang, Jiaheng Hu, Peter Stone, and Roberto Martn-Martn. Elden: Exploration vialocal dependencies. Advances in Neural Information Processing Systems, 36, 2024.. [64] Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su,Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library.Journal of Machine Learning Research, 23(267):16, 2022. URL http://jmlr.org/papers/v23/21-1127.html.. [65] Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian,Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al.Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):223228, 2022.. [66] Andrii Zadaianchuk, Maximilian Seitzer, and Georg Martius. Self-supervised visual reinforce-ment learning with object-centric representations. In International Conference on LearningRepresentations, 2021. URL https://openreview.net/forum?id=xppLmXCbOw1.. [67] Jesse Zhang, Haonan Yu, and Wei Xu. Hierarchical reinforcement learning by discoveringintrinsic options. arXiv preprint arXiv:2101.06521, 2021.. 15. AFactored Skills. Learning to reach both a desired graph g and a diversity parameter b through primitive actions ischallenging. First, different graphs often have substantially different characteristics, with somegraphs that are easy to achieve (eg. actionagent), and others that are quite challenging and rare (eg.agent, knife, fruitfruit). Not only would it be challenging for a single policy to encode all of thesebehaviors, the diversity parameter notwithstanding, but over-training the frequency at which certaingraphs are called might vary significantly. Rather than trying to learn a single monolithic policy, then,we instead structure the skill parameterized policy skill as a collection of factored skills: skill,i, foreach factor i  {1, . . . , N}.. This modification to the policy structure results in three changes: 1) The upper-level action spacepasses a single row of the graph G, denoted with gi, and the desired factor i. 2) Instead of achievingan entire graph use the achieved row 1[gachieved,i = gi]. 3) The history of seen graphs H is replacedwith a history of factored graph rows Hf.. Define the history of graph rows as Hf := {unique (i, gachieved,ii  1, . . . , Ngachieved  D)}.This takes the unique graph rows from all those seen in previous data. Then the upper policy uses thesame historical sampling procedure as with unfactorized graphs: the policy samples discretely fromthe new history, which will by default return i, gi, a graph row, and the desired factor. This resolvespoints 1,3. Point 2 is addressed by replacing Equation 5 with 1[gachieved,i = gi].. Empirically, we found that without this change, the lower policy rarely learns anything, even simplecontrol of the agent.. BEnvironment Details. In this section, we provide a detailed description of the environment, including its semantic stagesrepresenting internal progress toward task completion, state space, and action space. We also highlightthat while each task consists of multiple semantic stages, agents do not have access to this information.. <image: Indexed(20,DeviceRGB), width: 256, height: 256, bpc: 8>. (a) Installing Printer. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. (b) Thawing. <image: DeviceRGB, width: 323, height: 324, bpc: 8>. (c) Cleaning Car. <image: DeviceRGB, width: 411, height: 268, bpc: 8>. (d) iGibson. Figure 7: Environments.. Installing PrinterAs shown in Fig. 7(a), the Installing Printer environment is relatively simple,consisting of 3 factors: the agent, a printer, and a table. The task requires the agent to complete thefollowing stages: (1) pick up the printer, (2) bring the printer to and place it on the table, and (3)turn on the printer. The discrete state space consists of (i) the agents position and direction, (ii) thepositions of the printer and whether it is on or off, and (iii) the position of the table. The discreteaction space consists of (i) moving forward, turning left or right, (ii) picking up / placing down theprinter, and (iii) turning on / off the printer.. ThawingAs shown in Fig. 7(b) and Fig. 10(a), the Thawing environment consists of 6 factors:the agent, a sink, a refrigerator, and three frozen objects: fish, olive, and date. Thawing each objectrequires the agent to complete the following stages: (1) move to and open the refrigerator, (2) takethe frozen fish out of the refrigerator, (3) put the fish into the sink, and (4) turn on the sink to thawit. The discrete state space consists of (i) the agents position and direction, (ii) the positions of allenvironment entities, (iii) whether the sink door is turned on, (iv) whether the refrigerator door isopened, and (v) the thawing status of three objects. The discrete action space consists of (i) movingforward, turning left or right, (ii) opening / closing the refrigerator, (iii) turning on / off the sink, and(iv) picking up / placing down each object.. 16. Cleaning CarAs shown in Fig. 7(c), the Cleaning Car environment consists of 7 factors: the agent,a car, a sink, a bucket, a shelf, a rag, and a piece of soap. Cleaning both the car and the rag requiresthe agent to complete the following stages: (1) take the rag off the shelf, (2) put it in the sink, (3)toggle the sink to soak the rag up, (4) clean the car with the soaked rag, (5) take the soap off theself, and (6) clean the rag with the soap inside the bucket. The discrete state space consists of (i) theagents position and direction, (ii) the positions of all environment entities, (iii) whether the sink isturned on, (iv) the soak status of the rag, (v) the cleanness of the rag, and (vi) the cleanness of the car.The discrete action space consists of (i) moving forward, turning left or right, (ii) turning on / off thesink, and (iii) picking up / placing down the rag / soap.. iGibsonAs shown in Fig. 7(d), the iGibson environment consists of 4 factors: the robot, a knife, apeach, and a sink. The robot can do the following things: (1) grasp peach: move close to the peachand grasp it, (3) wash peach: grasp the peach and place it into the sink, (3) grasp knife: move close tothe knife and grasp it, (4) cut peach: grasp the knife and use it to cut the peach. The continuous statespace consists of (i) the robots proprioception, (ii) the poses of all environment entities, and (iii)whether the peach is cut. The continuous action space consists of (i) end-effector position change, (ii)base linear and angular velocity, and (iii) gripper torque (to open/close the gripper).. Though looking conceptually simple, we emphasize that these environments are challenging becauseof the following factors:. The state factors are highly sequentially interdependent, making skill learning and task learningchallenging: for example, in cleaning car environments, the agent cant clean the car until it picksup the rag, turns on the sink, and soaks the rag. These interdependencies between state factors posegreat challenges to the agents exploration ability.. During the skill-learning stage, we would like the agents to learn all possible skills (e.g., manipu-lating all objects) rather than learning to manipulate a single object.. During task learning, we use sparse rewards, and thus it is further challenging for agents toexplore.. In addition, we use primitive actions, and many actions have no effect if its not applicable inthe current state. So the task is especially challenging for exploration.. CLocal Dependencies and Causal Inference. In this work, we define local dependencies according to the state factors X = (X1, . . . , XN)and event of interest Y , which in the context of an MDP is a subset of the next state factorsX = (X1, . . . , XN). In the factored MDP formulation [8], we assume that p, the transitiondynamics, are represented by a dynamic Bayesian network (DBN) which is a time-directed bipartitegraph, with edges only from factors in X to factors in X. In this work, we assume that the underlyingground truth DBN, that is the transition function p, can be decomposed according to subsets of statefactors X, such there exists a p X(Y = y| X = x) for every state.. The factored transition dynamics analogizes with causal inference in the following way: If the statefactors and next state factors are each assigned a causal variable by adding the assumption that theycan be independently intervened on, and each next state variable carries an associated unobservednoise variable U i, which we assume is independent of any Xk not connected to Xj and any othernext state variable Xj, then we can represent the transition dynamics p with a structural causal model(SCM) [52], a graph connecting the causal variables in X to the causal variables in X.. For a particular outcome variable Y that is one of the next state causal variables X, we can describelocal dependence in the RL context according to assumptions about the structural causal model.Represent the non-noise parents of Y as pa(Y ), and the noise parents as paU(Y ). Under normalcausal assumptions, the structural causal model for Y is a function fY (pa(Y ), paU(Y )) = Y . DefineX as a subset of the endogenous parents of Y and U as an equivalent subset of the noise variables.Further define the values that pa(Y ), paU(Y ), X, U can take on as pa(y), paU(y), x, u respectively,and (pa(Y)), X, U as the set of states the parents of Y , the variables in X and variables in U can takeon respectively.. To formalize local invariance, we add the assumption that fY can be decomposed into a series of func-tions (fY 1( X1 = x1, U1 = u1), . . . , fY k( Xk = x, Uk = uk)) and gY (pa(Y ) = pa(y), paU(Y ) =. 17. paU(y)), where each fY i : X  U  Y and g : pa(Y)  {1, . . . , k}, a function mapping the parentsof Y to one of the functions. Then if f is represented as:. f(pa(x), paU(y)) :=. k. i=11(gY (pa(y), paU(y)) = i)fY i(xi, ui)(6). The local dependence of Y = y in a particular state (x, x) is then the set of variables in Xi for theparticular i where 1(gY (pa(y), paU(y)) = i) = 1, and the pCMI test is a way of uncovering theselocal dependencies from observational data.. Local dependence has been investigated in the field of context-specific independence [55, 9], whichseeks to find particular assignments of a subset of the causal variables under which an outcomeis independent of some subset of the inputs. In particular, context-set specific independence [9]determines if a variable is independent of other variables on a particular subset of states, described asthe partial context set. While our work uses the pCMI test described in Equation 2, context-specificindependence focuses on complete independence using knowledge of the structural model.. Alternatively, interactions can be viewed as the causes ( X) of particular effects (Y ), which havealso been investigated under the description of token or actual cause [25] (as opposed to generalcause). Actual cause utilizes a series of counterfactual tests to determine if a cause is necessary,sufficient, and minimal for an outcome. Actual cause has primarily been applied in simple, discreteexamples [5, 24], making it difficult to directly apply to RL. However, recent work has incorporatedthe notion of context-specific independence and extended actual cause to more complex domains [15].. DImplementation Details. The hyperparameters of skill learning and task learning can be found in Table 1. As it is challengingto identify local dependencies using learned dynamics models in Thawing and iGibson environments,we use ground truth local dependencies from simulator. The codebase is built on tianshou [64] forbackend RL, though with significant modifications.. The 5 seeds selected are 0 - 4. The experiments were conducted on machines of the followingconfigurations:. Nvidia A40 GPU; Intel(R) Xeon(R) Gold 6342 CPU @2.80GHz Nvidia A100 GPU; Intel(R) Xeon(R) Gold 6342 CPU @2.80GHz. EAdditional Results. E.1Interaction Graph Diversity. Figure 8 illustrates the percentages of episodes where all local dependencies have been induced atleast once, in Mini-BH Cleaning Car. Again, SkiLD (ours) induces all inducible dependency graphs,while baselines fail to induce hard graphs with challenging pre-conditions.. The meaning of the graphs are:. agent, action  agent: agent moving. agent, rag, action  rag: agent picking up the rag or moving it. agent, soap, action  soap: agent picking up the soap or moving it. agent, X  agent: X blocking the agents motion. agent, sink, action  sink: agent turning on or off the sink. agent, sink, rag, action  rag: agent soaking the rag in the sink (which requires that the sink isturned on).. sink, rag  rag: the same as above. car, rag  car: the rag cleaning the car and getting dirty (which requires that the rag is soaked). car, rag  rag: the same as above.. 18. Table 1: Parameters of Skill Learning and Task Learning. Parameters shared if not specified.. NameEnvironmentsPrinterThawingCleaning CariGibson. SkillPolicy. algorithmRainbowTD3n step35skill horizon30100exploration noise0.40.2MLP size[512, 512]optimizerAdamlearning rate3  104. batch size64. Graph SelectionPolicy. algorithmPPOoptimizerAdamlearning rate1  104. batch size1024clip ratio0.1MLP size[512, 512]GAE 0.95entropy coefficient0.1. LearnedDynamics Model. optimizerAdamlearning rate3  104. batch size128number of attention layers1attention embedding size128number of heads4. Task SkillSelection Policy. algorithmPPOMLP size[512, 512]optimizerAdamlearning rate1  104. batch size1024clip ratio0.1GAE 0.95entropy coefficient0.02. Training# of random seeds5diversity reward coefficient 0.5. bucket, soap  bucket: the water in the bucket getting soap in it. bucket, rag  rag: the rag getting cleaned in the bucket (which requires that the rag is dirty and thewater in the bucket gets soap in it).. E.22D Minecraft Results. In addtion to the environments shown in the paper, we further evaluating our method in larger-scalesettings in 2D Minecraft with 15 state factors following Andreas et al. [1].. The state space (15 state factors) consits of: the agent (location and direction), 10 environment entities(the positions of 3 wood, 1 grass, 1 stone, 1 gold, and 4 rocks surrounding the gold), and 4 inventorycells (i.e., the number of stick, rope, wood axe, and stone axe that the agent has).. The action space (9 discrete actions) consists of:. 4 navigation actions: moving up, down, left, right, picking up the environment entity in front, which has no effect if the agent does not have thenecessary tool for collecting it,. 4 crafting actions: crafting a stick/rope/wood axe/stone axe, no effect if the agent does not haveenough ingredients.. In the downstream Mine Gold task, the agent will receive a sparse reward after finishing all thefollowing steps. 19. visitation likelihood (%). bucket, rag -> rag. car, rag -> rag. bucket, soap -> bucket. car, rag -> car. sink, rag -> rag. agent, sink, rag, action -> rag. agent, soap -> agent. agent, bucket -> agent. agent, car -> agent. agent, rag -> agent. agent, sink, action -> sink. agent, sink -> agent. agent, soap, action -> soap. agent, rag, action -> rag. agent, action -> agent. 0.5. 13.7. 16.6. 19.0. 25.5. 33.9. 50.5. 54.7. 55.9. 61.6. 64.8. 72.7. 76.1. 92.5. 100.0. 0.0. 0.0. 0.1. 0.0. 0.4. 0.1. 12.5. 42.6. 52.4. 30.1. 47.9. 51.0. 13.7. 32.4. 100.0. 0.0. 0.0. 0.0. 0.0. 0.0. 0.0. 1.3. 14.9. 8.4. 4.6. 2.5. 4.2. 0.9. 9.0. 100.0. SkiLD (ours)CSDDIAYN. Figure 8: Among all inducible dependency graphs, the percentage of episodes where each graph isinduced through random skill sampling. Standard deviation is calculated across five random seeds.. collecting a unit of wood to craft a wood stick, collecting another unit of wood and combining it with the stick to craft a wood axe that is requiredfor collecting the stone and for removing the rock,. collecting a unit of wood and a unit of stone to craft a stick and then a stone axe that is required forcollecting the gold, remove the rock surrounding the gold and collect the gold with the stone axe.. As shown in Fig. 9, SkiLD still outperforms all baselines in this complex task, demonstrating theusefulness of its learned skills for downstream task solving.. FSkill Visualizations. In Figure 10 we visualize three challenging long-horizon skills learned by SkiLD: thawing the olive,cleaning the car, and cutting the peach. All of these skills require a sequence of interactions that isdifficult to recover without directed behavior. Thus, comparable baselines do not learn skills of similarcomplexity. More skill visualizations can be found at: https://wangzizhao.github.io/SkiLD/.. 20. 01M2M3M4M5M0.0. 0.5. 1.0. (a) Mine Gold. Figure 9: Training curves of SkiLD and baselines on the 2D Minecraft downstream task (rewardsupervised second phase). Each curve depicts the mean and standard deviation of the success rateover 5 random seeds. SkiLD outperforms all baselines, converging faster and to higher returns.. 21. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. (a) Thaw Olive Skill. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. <image: DeviceRGB, width: 320, height: 320, bpc: 8>. (b) Clean Car Skill. <image: DeviceRGB, width: 512, height: 512, bpc: 8>. <image: DeviceRGB, width: 512, height: 512, bpc: 8>. <image: DeviceRGB, width: 512, height: 512, bpc: 8>. <image: DeviceRGB, width: 512, height: 512, bpc: 8>. <image: DeviceRGB, width: 512, height: 512, bpc: 8>. <image: DeviceRGB, width: 512, height: 512, bpc: 8>. (c) Cut Fruit Skill. Figure 10: Policy rollouts for learned policies that achieve long horizon tasks (a) Mini-BH thaw olive,(b) Mini-BH clean car, (b) iGibson cut peach.. 22", "2410.18421v1.Atomistic_understanding_of_hydrogen_coverage_on_RuO2_110__surface_under_electrochemical_conditions_from_ab_initio_statistical_thermodynamics.pdf": "1. . Atomistic understanding of hydrogen coverage on RuO2(110) surface. under electrochemical conditions from ab initio statistical. thermodynamics. . Lei Zhang1, Jan Kloppenburg2, Chia-Yi Lin3, Luka Mitrovic4, Simon Gelin5, Ismaila Dabo5,. Darrell G. Schlom4, Jin Suntivich4, Geoffroy Hautier1,*. . 1 Thayer School of Engineering, Dartmouth College. 2 Department of Chemistry and Materials Science, Aalto University. 3 Smith School of Chemical and Biomolecular Engineering, Cornell University. 4 Department of Materials Science and Engineering, Cornell University. 5 Department of Materials Science and Engineering, The Pennsylvania State University. Abstract. Understanding the dehydrogenation of transition metal oxide surfaces under electrochemical. potential is critical to the control of important chemical processes such as the oxygen evolution reaction (OER). Using first principles computations, we model the thermodynamic dehydrogenation process on RuO2(110) and compare the results to experimental cyclic voltammetry (CV) on single crystal. We use a cluster expansion model trained on ab initio energy data coupled with Monte Carlo (MC) sampling to derive the macroscopic electrochemical observables, i.e., experimental CV, from the energetics of different hydrogen coverage microstates on well-defined RuO2(110). Our model reproduces the unique two-peaks cyclic voltammetry observed experimentally with current density peak positions and shapes in good qualitative agreement. We show that RuO2 (110) starts as a water-covered surface with hydrogen on bridge (BRG) and coordination-unsaturated sites (CUS) at low potential (<0.4 V vs. reversible hydrogen electrode, RHE).As the potential increases, the hydrogens on BRG desorb, becoming the main contributor to the first CV peak with smaller contributions from CUS. When all BRG hydrogens are desorbed (before 1.2V vs RHE), the remaining CUS hydrogens desorb abruptly in a very small potential window leading to the sharp second peak observed during CV. Our work shows that above 1.23V, the OER proceeds on a fully dehydrogenated RuO2(110) surface. We also demonstrate that the electrochemical dehydrogenation process on rutile involves multiple sites in a complex sequence of dehydrogenation. Our work highlights the use of first principles techniques coupled with statistical mechanics to model the electrochemical behavior of transition metal oxides surfaces.  Introduction. Electrochemical water-splitting is a viable way of producing sustainable, green hydrogen1,2.. However, catalysts that can efficiently drive the oxygen evolution reaction (OER), while maintaining reasonable stability are still under investigation and development3. OER catalysts that can sustain both acidic and anodic corrosions are very limited according to the Pourbaix. 2. . diagrams4,5. Only oxides from the precious-metal group6, e.g. RuO27, IrO28 and their derivatives9. 11, have met both the activity and stability requirements.. Aiming at improving the activity and stability of Ru and Ir-based catalysts, various catalyst-. modifying strategies, e.g. doping or alloying12, ligands engineering13,14, nano-engineering15,16, have been reported. However, the atomic-level reaction mechanism especially how the metal oxides exchange hydrogen with the solvent before and during the OER is still an open question. Recent experiments on well-defined crystals with clear surface terminations1720 using advanced thin film deposition, e.g. molecular beam epitaxy (MBE)21 and pulsed laser deposition (PLD)22,23 have provided insights into this fundamental question. Cyclic voltammetry (CV)21 provides a direct probe of the surface reaction, which can be coupled to physical characterizations based on optical24, or electronic14 response or atomic force or scanning tunneling microscopy17.. We have previously reported CV of rutile RuO2(110) films grown using MBE25,26. Before the. onset of the OER, two peaks were observed: one broad (0.8-1.0 V vs. reversible hydrogen electrode, RHE) and another narrow (1.3 V vs. RHE). The CV before the OER is indicative of the current coming from the surface dehydrogenation. Starting from the fully hydrogenated RuO2(110) surface, the hydrogen adsorbed species are removed as the voltage increases. The presence of the two peaks on RuO2(110) is different from the one peak water dehydrogenation feature to form *OH on Pt single crystal CV27. The two peaks were also observed on a rutile IrO2(110) film20. Using density functional theory (DFT) computations, the first peak was attributed to adsorbed water oxidation to hydroxide (*H2O  *OH + H+ + e) and the second peak to hydroxide dehydrogenation (*OH  *O + H+ + e). However, this previous theoretical analysis of the CV only considered hydrogen adsorbate patterns in small supercells as it is commonly performed in the field6. Realistic surfaces under electrochemical potential should consider much larger supercell for the adsorbates and include entropic effects. A better model of the surface dehydrogenation would require considering the complexity of the surface including enumerating possible microstates (hydrogen coverage configurations) and statistically sampling them at different electrochemical potential. One step in this direction would be to assume a mean-field type of interaction with interaction parameters fitted from experiments28 (e.g., Frumkin isotherm), or uses a simplified sampling using a partition function approach with limited configuration space29. A more elaborate technique to model the statistical mechanics of the different adsorbate configurations is the cluster expansion method30 which comprehensively capture the adsorbate-adsorbate interactions using effective interaction parameters are fitted directly from DFT calculations. Cluster expansion models have been widely used to understand the phase transformation in structural alloys31, Li-ion battery cathode materials32 and bimetallic alloy catalysts33. However, its application to surface modeling under electrochemical potential has been limited with the notable exception of a simple cluster expansion (pair-wise interactions) used to model the CV of a Pt single crystal surfaces27.. Here, we fit a cluster expansion model based on DFT data for different configurations of. hydrogen on RuO2(110) and sample the surface microstates using statistical mechanics coupled with a Monte Carlo (MC) algorithm at different potentials. Our model can qualitatively reproduce. 3. . the two peaks in the experimental CV with the right peak positions and broadness. This agreement allows us to assign the CV peaks to specific surface dehydrogenation processes and clarify the atomistic mechanisms at play on RuO2 (110) right before OER.  Methods. The density functional theory (DFT) calculations are performed using VASP, a plane-wave. basis code employing projector augmented wave (PAW) pseudopotentials with the implicit solvation model version, i.e. VASPsol34,35. Both dielectric and ionic responses upon the RuO2(110) surface are considered by specifying the dielectric constant (78.4) and Debye length (9.613 ) of the 0.1 M NaOH aqueous solution, where the experimental CV benchmark is based. Calculations are preformed using the generalized gradient approximation (GGA)-Perdew-Burke-Ernzheroff (PBE) functional and the PAW_PBE H (15Jun2001), PAW_PBE O (08Apr2002) and PAW_PBE Ru_pv (28Jan2005) pseudopotentials. A 4-Ru layer slab is constructed with an O-terminated BRG and CUS layer for *H adsorption. The bottom layer is with Ru termination to maintain the overall bulk RuO2 stoichiometry. The vacuum size perpendicular to the surface is set as 10 . A larger vacuum space, e.g. 15 , shift the total energy less than 40 meV and does not affect the statistical thermodynamics. An \"all band simultaneous update of orbitals\" (ALGO=All) method is used to find the electronic ground state. The bottom two O-Ru-O layers are frozen while the top layers are allowed to relax during the slab calculations. A relaxed 1*1 slab with in-plane dimension of 2 Ru atoms per layer is used as prototype for the primitive cell. For primitive cells, a 6*3*1 Gamma-centered k-points was used. K-points for larger supercells are adjusted accordingly to maintain a constant k-point density. 400 eV energy cutoff are used together with an energy convergence criterion of 10-4 eV/unit-cell. Cluster expansion is performed using CASM36. Cluster expansion is an energy Hamiltonian depending only on lattice occupations. The formula is shown in Equation 1:. Equation 1 () =. where  is a spin-like variable of up or down (occupation variable of -1 or +1 for an occupied or unoccupied atom specie) and is an element of vector   representing configuration space; Coefficient  is a fitted energy value for cluster  and called effective cluster interaction (ECI);  is the multiplicity of symmetrically equivalent cluster , the group of equivalent cluster  is denoted as . It can couple with MC sampling to evaluate thermodynamic quantities and predict phase transformations.. To train the cluster expansion model, over 500 supercells up to eight-times of the primitive. cell prototype with various lattice vectors and *H adsorption occupations are enumerated and calculated by DFT. The dataset is then used to train a cluster-based expansion model of total energy using selected interactions within a series of doublet, triplet and quadruplet clusters with diameters of 9, 7 and 6 . A Metropolis MC under the fixed chemical potential of hydrogen species (grand-canonical ensemble) is then performed using the trained cluster-expansion model to evaluate the. 4. . free energy of a 6464 supercell, yielding statistically robust thermodynamic quantities by taking into account both enthalpy and configuration entropy of microstates, serving a computational counterpart of experimental CV. Chemical potentials of hydrogen are scanned, which directly couples with the applied potential  using the computational hydrogen electrode (CHE) model37. The way of using 2 to represent the energy of a coupled proton and electron under the reversible condition (298.15 K, 1 atm, arbitrary pH) is well-established by J. Norskov et. al. in the CHE. Thus, any other electrode potentials can be referenced back to the CHEs absolute potential, i.e., 4.44 V:. Equation 2 (. .  ) = 4.44  0.5  2. where 4.44 is the absolute electrode potential of RHE, 2 is the Gibbs free energy of a H2 molecule including 0 K DFT energy (calibrated to the NIST JANAF table), zero-point vibrational energy, and free energy from 0 to 298.15 K with data from the NIST JANAF table38;  is the chemical potential of H within the chemical space of Ru8O16Ru8O16H3 pseudo-binary governed by the thermodynamics of *H on RuO2 (110). It is worth mentioning that the vibrational (including the zero-point energy, i.e. ZPE) contribution to the free energy of *H adsorption can be safely neglected as it is configuration and coverage independent, which cancels out itself when evaluating the free energy of mixing within the convex hull. Therefore, the systems mixing thermodynamics is governed mainly by 0 K enthalpies and configuration entropies of relevant microstates.. The electrochemical experiments were conducted with 47 formula-units thick molecular. beam epitaxy (MBE)-grown RuO2(110) films (~ 19 nm) in 0.1 M NaOH. The MBE synthesis was conducted in Veeco Gen 10 MBE. The ruthenium flux was obtained by evaporating Ru metal (99.99 % pure, ESPI Metals) using an electron beam into an ozone environment to form RuO2 on TiO2(110) substrates at 350 C. Ozone background pressure was maintained at 10-6 Torr during the heating up of substrates, growth, and cooling of the sample. The MBE-grown films were confirmed by X-ray diffraction, as shown in Figure S 1.. The MBE-grown RuO2(110) films were then prepared into a working electrode by attaching. Titanium wire on RuO2 films using Gallium liquid metal (99.99% pure, Amazon), and silver paint (Ted Pella, Leitsilber 200) and dried in air overnight. After that, the epoxy (Omegabond 101) was used to cover the silver paint, edges, and the backside of sample except for the RuO2(110) film. The RuO2(110) electrode was rinsed in DI water right before electrochemical experiments. A Pt wire was used as counter electrode. The reference electrode was reverse hydrogen electrode (RHE). The 0.1 M NaOH electrolyte was prepared by dissolving NaOH (99.99 % Suprapur from Millipore Sigma) in ultrapure water (18.2 M-cm). The CV was obtained in an Ar-saturated electrolyte with scan rate at 50 mV/s.. . Results. Experimental cyclic voltammetry and relation to hydrogen coverage. 5. . Figure 1 shows the experimental CV data on RuO2(110) single crystal thin film. The. measurement is performed at a potential scan rate of 50 mV/s. Current density is then measured through the potential with the sample submerged in a 0.1M NaOH aqueous electrolyte (pH = 13). Two peaks are clearly identified with a broad 1st peak spanning from 0.4 to 1.2 V and a sharp 2nd peak centered around 1.3V vs. RHE. No hysteresis is found in the cyclic measurement, indicating a spontaneous and reversible hydrogen desorption/adsorption behind these two peaks, consistent with our previous measurements on Ru and Ir-based rutile (110) surface20,21. We can interpret the CV data as a measure of the current related to dehydrogenation of the oxide surface. Mathematically, the current density   and voltage scan rate   are connected by. Equation 3 assuming a satisfied equilibrium condition:. Equation 3. . . =. . . . . . . (, ) = 0. where  is the *H coverage. The transferred charge   is linearly proportional to  as each hydrogen removed from the surface will release an electron. By integrating current density  w.r.t the applied potential , equilibrium surface *H coverage is obtained as a function of :. Equation 4   =. 1. . where   is the potential scan rate and   is the total charge transferred within the whole dehydrogenation process.. . Figure 1. Experimental cyclic voltammogram on single crystal thin film RuO2(110) with measured current density at a potential scan rate of 50mV/s under 0.1M NaOH with surface normalization.  Feature of two peaks: 1st broad peak located below 1.2 V with a width of ~1 V, 2nd sharp peak at ~1.3 V.. Cluster expansion. <image: DeviceRGB, width: 832, height: 622, bpc: 8>. 6. . First principles computations can be used to identify the atomistic mechanisms responsible for. the change in hydrogen coverage and thus the CV. In previous work, we have performed such analysis computing with DFT a series of simple ordered configuration for surface hydrogens within a small unit cell20,21. This simple analysis attributed the first peak to the *H2O  *OH + H+ + e reaction and the second peak to the *OH  *O + H+ + e reaction. However, this approach is severely limited by the unit cell size and number of hydrogen coverage configurations where more complex hydrogen interactions with longer distances cannot be captured. Moreover, this previous work focused on the CUS sites which is known to be the active site for OER but the BRG site could also be part of the dehydrogenation process. A more realistic model requires a much larger system modeled by a supercell to reach the thermodynamic limit and a comprehensive MC sampling of many different hydrogen configurations over multiple adsorption sites. These larger systems are typically not within reach of DFT because of computational cost. However, the cluster expansion technique can be used to fit a model Hamiltonian that will provide an energy for any configuration of interest. When this cluster expansion model is coupled with MC sampling, microstates can be efficiently sampled and experimental observables (e.g., hydrogen coverage versus potential) can be obtained by theory.. Our model RuO2 (110) surface has two types of O sites that can potentially bind with H, i.e.. CUS and BRG, where in particular O on CUS can bind with 2 H to form a water molecule-like motif and BRG O can only accommodate 1 H (left of Figure 2). One primitive cell of RuO2 (110) has 2 CUS and 1 BRG sites hosting 3 *H in total. The O-H bond on BRG can point to the lattice vector direction of b or -b. The two CUS O-H bonds have CUS1 pointing along with the BRG O-H and CUS2 O-H slanted towards lattice vector a or -a. To avoid the strong steric repulsion among *H, 1 ML (i.e. monolayer) *H have a rotationally rigid pattern with a restricted rotational degree of freedom. Different rotationally ordered patterns were tested and found energetically degenerate, hence are not considered explicitly in the statistical sampling. The way cluster expansion model works is to first enumerate a sufficient size of training set with various configurations based on the primitive slab. Figure 2 shows a few representative configurations of hydrogen in a 22 supercell. Lattice vectors and configurations are allowed to vary during the fit of the cluster expansion to ensure a diverse sampling and in total around 500 structures of different supercell sizes have been generated and computed with DFT.  For these 500 structures, we use DFT with an implicit solvation model to compute energies (see Methods). This data set is then used to train a cluster expansion model that links the configuration of hydrogens (i.e., what sites are occupied or not, through an occupation variable  that is -1 or 1, elements of occupation matrix  with lattice sites and clusters as row and column indices) and their corresponding energies (right of Figure 2). Quantities learned through this process are the effective cluster interactions  for a series of clusters (i.e., groups of sites) that can be doublet, triplet, quadruplet etc Mathematically, it is an inverse matrix problem where the interaction parameters  are solved numerically under the given the energy vector () as a function of the occupation matrix . Therefore, rank of matrix  needs to be as close to full as possible and the symmetrically distinct cluster basis sets (called orbit ) needs to be sufficient to describe the interactions. A robust cluster expansion model captures the. 7. . physics of the systems interactions by using a succinct set of interaction parameters with expanded clusters (i.e. doublet, triplet, quadruplet, etc.) while avoiding overfitting.. . . Figure 2. A schematic showing the random enumeration of DFT training sets (only 2*2 supercells with a few representative *H occupation states are shown for demonstration, while various sizes and shapes of supercells with *H coverages and configurations were enumerated) using the primitive cell on the left as the prototype. A vector-matrix projection is then constructed between the supercell energy vector () and the occupation matrix  (with cluster basis set and lattice site indices). Interaction parameters  within a symmetrically distinct cluster group (or called as orbit)  are then fitted using the. energy and occupation as input. A well-trained set of  constitutes a so-called cluster expansion model that is ready for efficient free energy sampling of a much larger supercell that is not accessible through quantum mechanics calculations like DFT.. Cluster Expansion and comparison to DFT Convex Hull. The zero K DFT mixing energies (blue circles) of the configurations studied in this work are. plotted versus the hydrogen fraction in Figure 3. Here, a convex hull is constructed by connecting ground states (blue dots) within the chemical space. DFT predicts three important ground states in the middle with *H site fraction of 0.833 (5/6 ML), 0.666 (4/6 ML) and 0.5 (3/6 ML). The formation energy is over -0.4 eV/primitive cell, a significantly larger value than  at room temperature (0.0259 eV), indicating a strong tendency of *H mixing over the surface. However, between 0.5 ML and 0 ML, no *H configuration formed more stable ground states than the convex line, indicating a possible sharp phase transition during dehydrogenation in this *H coverage range.. The formation energy from a trained cluster expansion is also plotted (red dots) showing a. good agreement with the data from DFT (blue circles). A combination of doublet, triplet and quadruplet clusters interactions within diameters of 9, 7 and 6  were found to give the best fitting quality, with root mean squared error (rmse) and cross-validation scores less than 2 meV/atom. More importantly, the overall shape of the convex hull and the key ground states are well-reproduced. Since microstates are sampled only at room temperature, data with formation energies close to the hull are most relevant in the following MC sampling.. . <image: DeviceRGB, width: 1247, height: 504, bpc: 8>. 8. . . Figure 3. The 0 K formation energy in eV per primitive cell (blue circle: DFT calculations, red dot: cluster expansion prediction). The ground state with a lowest formation energy of ~-0.47 eV is on 2/3 ML (0.666) coverage *H. The rest of deep ground states locate at 5/6 and 1/2 ML *H coverages. The root mean squared error (rmse) and cross-validation (cv) scores are both less than 2 meV/atom. The clex model is trained using a 9-7-6  interaction cutoff lengths with a doublet-triplet-quadruplet cluster combination.. Dehydrogenation profile from Monte-Carlo Simulations. The convex hull presented in Figure 3 is a zero Kelvin picture of the dehydrogenation process. on RuO2(110) surface. Introducing temperature effects and entropy requires to move to statistical mechanics and sample the equilibrium average coverage at different hydrogen chemical potentials (i.e., different electrochemical potentials in an electrochemical settings). This average requires the energy of many different configurations of hydrogen to be evaluated in large supercells, something impossible to achieve directly from DFT. However, we can use our cluster expansion fitted on DFT data to evaluate any configuration of hydrogen and use the laws of statistical mechanics.. The equilibrium *H coverage with the detailed *H site-fractions are computed w.r.t to the. hydrogen chemical potential (). The ensemble average, i.e., the systems grand potential at a given   and temperature   is evaluated through the partition function (, ) defined in Equation 5:. Equation 5 (, ) =  =   (. ,. ). where , and  are the number of *H and total energy of the microstate . Probability  of. microstate  is given as    with  the -th component of .. Grand potential  is related with the partition function by Equation 6:. Equation 6    =  =. where total energy , number of *H  and entropy  are averaged values by sampling sufficient microstates:. <image: DeviceRGB, width: 854, height: 624, bpc: 8>. 9. . Equation 7  =  =. 1. (. ,. ). Equation 8  =  ,=. 1. , (. ,. ). Equation 9  =. In practice, we have computed these quantities using our cluster expansion (E()) and. Metropolis MC importance sampling on a 6464 RuO2(110) surface containing multiple hydrogen sites. Our model is not limited by small unit cell size and includes the configurational entropy of hydrogen configuration. The MC sampling provides the coverage vs hydrogen chemical potential and thus electrochemical potentials as they are linked by the hydrogen reference electrode H+ + e-  H2 (see Methods).. To facilitate the comparison between theory and experiment, we have integrated the current. from the experimental CV to obtain the experimental hydrogen coverage vs. potential (blue curve) and compare it to the results of our MC simulations (red curve) in Figure 4. The 1st stage of the dehydrogenation process occurs in our model between 0.3~1.2V (~1 V potential window) where 50% of hydrogen is desorbed. This 1st stage of dehydrogenation corresponds to the 1st broad peak in the experimental CV (see Figure 1 as well). The remaining 50% hydrogen gets desorbed from ~1.2 to 1.3V within a small 0.1 V potential window corresponding to a much sharper 2nd peak (see Figure 1 as well). The agreement between theory and experiment is good overall.. Although the experiment shows a smooth transition during the 1st peak, the model shows some. staging. We attribute this discrepancy to the limitation of the implicit solvation model that neglects the entropic contribution of the explicit solvent water molecules within the Helmholtz layer. We expect the explicit solvent molecules to promote interfacial disorders and smear the simulated steps within the 1st peak. Nonetheless, the two-peak feature in the experimental CV with the 1st one broader and 2nd one sharper is very well reproduced in our model. The specific potential windows for the two peaks also agree reasonably well with the experiment. The peaks show less than 200 mV shift between simulation and experiment. This mismatch could be due to the used PBE functional. As we have previously discussed, errors up to 100 meV are not uncommon for oxidation processes in transition metal oxides within semilocal functionals such as GGA21. The neglect of the interfacial electric field could also explain the discrepancy. Moreover, rotational degrees of freedom (rotational configuration entropy) are higher for surfaces with lower *H coverage, as they have larger space necessary for OH rotations without steric hinderance from nearby hydroxyl groups. Water molecules near an inorganic surface are expected to be more disordered, rendering an entropically destabilized state at low *H coverage. Those missing factors can shift the dehydrogenation profile (together with the peaks) towards a higher applied potential.. . 10. . . Figure 4. Dehydrogenation profile on RuO2 (110) under the applied voltage (vs. a reversible hydrogen electrode) integrated from the experimental cyclic voltammetry (blue curve) and sampled from the Monte-Carlo simulation (red dotted line). A slow dehydrogenation process is revealed below ~1.2 V, after which the rest  ML *H undergoes a sharp desorption from ~1.2 to 1.4 V. All *H are desorbed further beyond, rendering a *H clean surface before the onset of OER.. . To further understand the atomistic details behind the dehydrogenation, we isolate the site-. specific *H on BRG and CUS shown in Figure 5 (see also Figure S 3). As the potential increases from 0.3 V to 0.5 V vs. RHE in the very beginning, *H on CUS (orange curve) get dehydrogenated first by 1/6 ML *H. However, after this point and until the end of the 1st stage, no further *H on CUS participates in additional dehydrogenation. Instead, BRG *H (green curve) contributes more to the dehydrogenation. This contribution from BRG *H continues to the end of 1st stage, where all BRG *H is desorbed, contributing to 1/3 ML of overall *H coverage. The 2nd stage starts at a potential of ~1.2 V vs. RHE. Here, the rest  ML *H on CUS is desorbed in a rather narrow potential window in stark contrast to the 1st dehydrogenation stage.. Our finding suggests that the two-stage dehydrogenation occurs via a solid-solution-like. behavior among *H on BRG during the 1st dehydrogenation stage and a two-phase-like behavior of *H on CUS at ~1.2V between  and 0 ML coverage phase regions. Interestingly, the sharp desorption on BRG and CUS at 0.5 V might be due to the large slope of the convex hull near the full *H coverage. This complex hull behavior from DFT and the cluster expansion model could give this small desorption an appearance of a kink. However, this does not alter our conclusion about the two-peak feature. *H redistribution between BRG and CUS is observed at 0.5 V.  The suspension of desorption on CUS after 0.5 V with  ML *H is due to a similar *H redistribution between CUS1 and CUS2 (See Figure S 3). This counter-intuitive *H redistribution was reported before, e.g. *H could hop from site to site facilitated by interfacial water layers through the. <image: DeviceRGB, width: 933, height: 717, bpc: 8>. 11. . Grotthuss mechanism39,40. This detailed site-fraction of *H unveils the possibility of adsorbate redistribution during a trivial desorption process where a monotonic *H coverage evolution is concerned.. We confirm that, before the onset of OER (above 1.3 or 1.4 V), there is no remaining *H on. the surface, indicating a decoupling of the first two steps (*OH2  *OH  *O) with the last two steps (*O  *OOH  *OO) in the 4-step 4e--involved OER. This finding might be of importance for modeling the kinetics of the OER process as it shows that the *O + H2O  *OOH + H+ + e- reaction occurs on an oxygen terminated, free of hydrogen (110) surface in RuO2. Therefore, a simple *OH2  *OH  *O reaction scheme should be revised to a more detailed one, i.e. *OH2CUS*OHBRG-*OH2CUS*OHBRG  *OH2CUS*OHBRG-*OHCUSOCUS*OHBRG. *OH2CUS*OBRG-*OHCUSOCUSOBRG  O2CUSOBRG-O2CUSOBRG, which will be further discussed next. Moreover, we emphasize that for non-trivial oxide surfaces, e.g., RuO2 (110), complex *H interactions among different adsorption sites greatly complicate the picture of a single active site on which intermediates for the OER (OH, O, OOH) are computed.. Our work shows that the BRG site plays an important role in the CV and will influence the. peaks positions. This is of importance as the 4-steps 4e- mechanism has linked catalyst activity to hydrogen adsorption energetics. The scaling relationship especially links activity to any of the free energy change in the catalytic steps (e.g., *OH to *O or *OH2 to *OH). This naturally leads to using the CV peak positions that are related to hydrogen adsorption energetics as descriptors of catalytic activity20,21. However, the BRG site is often considered inactive in the OER catalytic cycle and ignored in the typical 4 steps 4e- mechanism. This makes a direct link between hydrogen coverage measurements through CV (driven by CUS and BRG) and the energy change of the 4-step which is typically driven by CUS only less trivial than previously thought.. 12. . . Figure 5. Site-specific *H desorption in two stages. *H desorption profile over CUS and BRG, shown as orange and green . During the 1st peak voltage window, only 1/6 ML *H from CUS but all BRG *H desorb. CUS *H desorb first but suspend the process afterwards, leaving an unchanged  ML *H coverage. All BRG *H contributing 1/3 ML surface desorb gradually over the course of the 1st voltage window. During the 2nd peak voltage window, the remained  ML *H on CUS desorb quickly above a 1.2 V potential threshold. A slight *H redistribution is observed at ~0.5 V between CUS and BRG.. . To illustrate the evolution of *H configuration patterns, snapshots from MC during the. potential scan is presented at representative *H coverages in Figure 6. It is found that *H on CUS forms a 2a*1b superlattice with alternating *H and vacant * along a direction at 0.50 V with 5/6 ML coverage. At ~0.55 V, BRG *H starts to form a 3a*1b superlattice where 1 out of 3 BRG *H is missing in each BRG row. Some *H get re-adsorbed on CUS, forming a similar 3a*1b pattern. Higher applied potentials gradually strip off the rest of BRG *H while CUS *H remains a 2a*1b superlattice of  ML coverage till 1.15V. Starting at 1.2 V the left CUS *H strips start to peel off from the surface stripe-by-stripe rather than atom-by-atom, leaving fragmented CUS *H stripes with 2a periodicity. This happens within a narrow potential window of 0.1 V. At and above 1.25 V all CUS *H are gone, leaving a clean surface with O-termination ready for another round of adsorption. In fact, unlike TiO2, RuO2 or IrO2 have spontaneous dehydrogenation where overpotential is not needed in the first two OER steps41. We note that although *OOH is not included in the computational CV model, the CV is still well reproduced. This suggests that *OOH is likely not participating in the charge transfer process during the CV measurement below 1.23 V in our sample. This is different than previous results on PLD samples42.. . <image: DeviceRGB, width: 884, height: 660, bpc: 8>. 13. . . Figure 6. Representative snapshots of the Metropolis Monte-Carlo simulation w.r.t applied potentials (vs. RHE) starting with the ordered patterns of *H: *OH2CUS-*OHBRG-*OH2CUS-*OHBRG. 5/6 ML *H appears in *OH2CUS-*OHBRG-*OHCUS-*OHBRG at 0.50V where CUS *H are starting to desorb. With increasing potentials, BRG *H start to desorb as well and eventually are all gone at 1.15 V (OH2CUS-OHCUS). This belongs to the 1st stage of dehydrogenation with the potential window of almost 1 V.  After that, all the rest CUS *H with  ML coverage desorb quickly within 0.1 V (1.15 to 1.25 V), by stripping off CUS *H stripe-by-stripe rather than atom-by-atom. *H on BRG and CUS form green and orange lines in the 6464 MC supercell, with a dotted square encircling a 3 *H (2 on CUS and 1 on BRG) primitive cell (1st snapshot at 0.50 V).. . Conclusion. We use a cluster expansion trained on DFT energies and a statistical mechanics approach to. compute the CV and hydrogen coverage versus electrochemical potential for RuO2(110). The model reproduces the two peaks observed in the experimental CV and provides insight into the atomistic mechanism that governs the shape and position of the two peaks. The 1st broad peak is due to the gradual dehydrogenation on the BRG sites and the partial (1/6 ML) dehydrogenation on CUS. The 2nd sharp peak is due to quick desorption of  ML *H from CUS and happens when all BRG sites have been dehydrogenated. Our work shows that at an electrochemical potential relevant to the OER (>1.2V vs RHE), the RuO2(110) surface is fully dehydrogenated which can have important implications when modeling the kinetics of OER on realistic surfaces.. The application of a statistical thermodynamic model to connect the surface microscopic states. with macroscopic experimental data allows a direct investigation of the atomistic dehydrogenation process on RuO2(110). We conclude that this surface process follows an intricate process involving both sites on the rutile surface and complex interactions. While our model captures already much of the dehydrogenation behavior for RuO2(110), future work could include other effects such as explicit solvent or electric field at the interface.. . <image: DeviceRGB, width: 1248, height: 634, bpc: 8>. 14. . Acknowledgement. This work was supported as part of the Center for Electrochemical Dynamics and Reactions. at Surfaces (CEDARS), an Energy Frontier Research Center funded by the US Department of Energy, Office of Science, Basic Energy Sciences under award DE-SC0023415. The computational work used resources of the National Energy Research Scientific Computing Center (NERSC), a Department of Energy Office of Science User Facility using NERSC award BES-ERCAP0028800 and Andes8 clusters at Dartmouth College. The authors want to thank Dr. Wei Chen and Andrew Pike for helpful discussions.. *geoffroy.hautier@dartmouth.edu  References. (1) Guan, D.; Wang, B.; Zhang, J.; Shi, R.; Jiao, K.; Li, L.; Wang, Y.; Xie, B.; Zhang, Q.; Yu, J.;. Zhu, Y.; Shao, Z.; Ni, M. Hydrogen Society: From Present to Future. Energy Environ. Sci. 2023, 16 (11), 49264943. https://doi.org/10.1039/D3EE02695G.. (2) Stamenkovic, V. R.; Strmcnik, D.; Lopes, P. P.; Markovic, N. M. Energy and Fuels from. Electrochemical Interfaces. Nat. Mater. 2017, 16 (1), 5769. https://doi.org/10.1038/nmat4738.. (3) Kibsgaard, J.; Chorkendorff, I. Considerations for the Scaling-up of Water Splitting. Catalysts. Nat. Energy 2019, 4 (6), 430433. https://doi.org/10.1038/s41560-019-0407-1.. (4) Zagalskaya, A.; Chaudhary, P.; Alexandrov, V. Corrosion of Electrochemical Energy. Materials: Stability Analyses Beyond Pourbaix Diagrams. J. Phys. Chem. C 2023, 127 (30), 1458714598. https://doi.org/10.1021/acs.jpcc.3c01727.. (5) Wang, Z.; Guo, X.; Montoya, J.; Nrskov, J. K. Predicting Aqueous Stability of Solid with. Computed Pourbaix Diagram Using SCAN Functional. Npj Comput. Mater. 2020, 6 (1), 160. https://doi.org/10.1038/s41524-020-00430-3.. (6) Danilovic, N.; Subbaraman, R.; Chang, K.-C.; Chang, S. H.; Kang, Y. J.; Snyder, J.;. Paulikas, A. P.; Strmcnik, D.; Kim, Y.-T.; Myers, D.; Stamenkovic, V. R.; Markovic, N. M. ActivityStability Trends for the Oxygen Evolution Reaction on Monometallic Oxides in Acidic Environments. J. Phys. Chem. Lett. 2014, 5 (14), 24742478. https://doi.org/10.1021/jz501061n.. (7) Nguyen, M.-T.; Mu, R.; Cantu, D. C.; Lyubinetsky, I.; Glezakou, V.-A.; Dohnlek, Z.;. Rousseau, R. Dynamics, Stability, and Adsorption States of Water on Oxidized RuO2(110).. (8) Ha, M.-A.; Larsen, R. E. Multiple Reaction Pathways for the Oxygen Evolution Reaction. May Contribute to IrO 2 (110)s High Activity. J. Electrochem. Soc. 2021, 168 (2), 024506. https://doi.org/10.1149/1945-7111/abdeea.. (9) Akbashev, A. R.; Roddatis, V.; Baeumer, C.; Liu, T.; Mefford, J. T.; Chueh, W. C. Probing. the Stability of SrIrO 3 during Active Water Electrolysis via Operando Atomic Force Microscopy. Energy Environ. Sci. 2023, 16 (2), 513522. https://doi.org/10.1039/D2EE03704A.. (10) Wan, G.; Freeland, J. W.; Kloppenburg, J.; Petretto, G.; Nelson, J. N.; Kuo, D.-Y.; Sun,. C.-J.; Wen, J.; Diulus, J. T.; Herman, G. S.; Dong, Y.; Kou, R.; Sun, J.; Chen, S.; Shen, K. M.; Schlom, D. G.; Rignanese, G.-M.; Hautier, G.; Fong, D. D.; Feng, Z.; Zhou, H.;. 15. . Suntivich, J. Amorphization Mechanism of SrIrO 3 Electrocatalyst: How Oxygen Redox Initiates Ionic Diffusion and Structural Reorganization. Sci. Adv. 2021, 7 (2), eabc7323. https://doi.org/10.1126/sciadv.abc7323.. (11) BenNaim, M.; Liu, Y.; Stevens, M. B.; Lee, K.; Wette, M. R.; Boubnov, A.; Trofimov,. A. A.; Ievlev, A. V.; Belianinov, A.; Davis, R. C.; Clemens, B. M.; Bare, S. R.; Hikita, Y.; Hwang, H. Y.; Higgins, D. C.; Sinclair, R.; Jaramillo, T. F. Understanding Degradation Mechanisms in SrIrO 3 Oxygen Evolution Electrocatalysts: Chemical and Structural Microscopy at the Nanoscale. Adv. Funct. Mater. 2021, 31 (34), 2101542. https://doi.org/10.1002/adfm.202101542.. (12) Choubisa, H.; Abed, J.; Mendoza, D.; Matsumura, H.; Sugimura, M.; Yao, Z.; Wang, Z.;. Sutherland, B. R.; Aspuru-Guzik, A.; Sargent, E. H. Accelerated Chemical Space Search Using a Quantum-Inspired Cluster Expansion Approach. Matter 2023, 6 (2), 605625. https://doi.org/10.1016/j.matt.2022.11.031.. (13) Hao, S.; Liu, M.; Pan, J.; Liu, X.; Tan, X.; Xu, N.; He, Y.; Lei, L.; Zhang, X. Dopants. Fixation of Ruthenium for Boosting Acidic Oxygen Evolution Stability and Activity. Nat. Commun. 2020, 11 (1), 5368. https://doi.org/10.1038/s41467-020-19212-y.. (14) Lee, S.; Lee, Y.-J.; Lee, G.; Soon, A. Activated Chemical Bonds in Nanoporous and. Amorphous Iridium Oxides Favor Low Overpotential for Oxygen Evolution Reaction. Nat. Commun. 2022, 13 (1), 3171. https://doi.org/10.1038/s41467-022-30838-y.. (15) Du, K.; Zhang, L.; Shan, J.; Guo, J.; Mao, J.; Yang, C.-C.; Wang, C.-H.; Hu, Z.; Ling, T.. Interface Engineering Breaks Both Stability and Activity Limits of RuO2 for Sustainable Water Oxidation. Nat. Commun. 2022, 13 (1), 5448. https://doi.org/10.1038/s41467-022-33150-x.. (16) Ping, X.; Liu, Y.; Zheng, L.; Song, Y.; Guo, L.; Chen, S.; Wei, Z. Locking the Lattice. Oxygen in RuO2 to Stabilize Highly Active Ru Sites in Acidic Water Oxidation. Nat. Commun. 2024, 15 (1), 2501. https://doi.org/10.1038/s41467-024-46815-6.. (17) Chang, S. H.; Danilovic, N.; Chang, K.-C.; Subbaraman, R.; Paulikas, A. P.; Fong, D. D.;. Highland, M. J.; Baldo, P. M.; Stamenkovic, V. R.; Freeland, J. W.; Eastman, J. A.; Markovic, N. M. Functional Links between Stability and Reactivity of Strontium Ruthenate Single Crystals during Oxygen Evolution. Nat. Commun. 2014, 5 (1), 4191. https://doi.org/10.1038/ncomms5191.. (18) Over, H. Fundamental Studies of Planar Single-Crystalline Oxide Model Electrodes. (RuO 2 , IrO 2 ) for Acidic Water Splitting. ACS Catal. 2021, 11 (14), 88488871. https://doi.org/10.1021/acscatal.1c01973.. (19) Hu, B.; Kuo, D.-Y.; Paik, H.; Schlom, D. G.; Suntivich, J. Enthalpy and Entropy of. Oxygen Electroadsorption on RuO2(110) in Alkaline Media. J. Chem. Phys. 2020, 152 (9), 094704. https://doi.org/10.1063/1.5139049.. (20) Kuo, D.-Y.; Kawasaki, J. K.; Nelson, J. N.; Kloppenburg, J.; Hautier, G.; Shen, K. M.;. Schlom, D. G.; Suntivich, J. Influence of Surface Adsorption on the Oxygen Evolution Reaction on IrO 2 (110). J. Am. Chem. Soc. 2017, 139 (9), 34733479. https://doi.org/10.1021/jacs.6b11932.. (21) Kuo, D.-Y.; Paik, H.; Kloppenburg, J.; Faeth, B.; Shen, K. M.; Schlom, D. G.; Hautier,. G.; Suntivich, J. Measurements of Oxygen Electroadsorption Energies and Oxygen Evolution Reaction on RuO 2 (110): A Discussion of the Sabatier Principle and Its Role in Electrocatalysis. J. Am. Chem. Soc. 2018, 140 (50), 1759717605. https://doi.org/10.1021/jacs.8b09657.. 16. . (22) Rao, R. R.; Kolb, M. J.; Giordano, L.; Pedersen, A. F.; Katayama, Y.; Hwang, J.; Mehta,. A.; You, H.; Lunger, J. R.; Zhou, H.; Halck, N. B.; Vegge, T.; Chorkendorff, I.; Stephens, I. E. L.; Shao-Horn, Y. Operando Identification of Site-Dependent Water Oxidation Activity on Ruthenium Dioxide Single-Crystal Surfaces. Nat. Catal. 2020, 3 (6), 516525. https://doi.org/10.1038/s41929-020-0457-6.. (23) Kwon, S.; Stoerzinger, K. A.; Rao, R.; Qiao, L.; Goddard, W. A.; Shao-Horn, Y. Facet-. Dependent Oxygen Evolution Reaction Activity of IrO 2 from Quantum Mechanics and Experiments. J. Am. Chem. Soc. 2024, 146 (17), 1171911725. https://doi.org/10.1021/jacs.3c14271.. (24) Xu, P.; Von Rueden, A. D.; Schimmenti, R.; Mavrikakis, M.; Suntivich, J. Optical. Method for Quantifying the Potential of Zero Charge at the PlatinumWater Electrochemical Interface. Nat. Mater. 2023, 22 (4), 503510. https://doi.org/10.1038/s41563-023-01474-8.. (25) Bohnen, K.-P.; Heid, R.; De La Pea Seaman, O. Ab Initio Lattice Dynamics and. Thermodynamics of RuO 2 ( 110 ) Surfaces. Phys. Rev. B 2010, 81 (8), 081405. https://doi.org/10.1103/PhysRevB.81.081405.. (26) Feng, T.; Wang, Y.; Herklotz, A.; Chisholm, M. F.; Ward, T. Z.; Snijders, P. C.;. Pantelides, S. T. Determination of Rutile Transition Metal Oxide (110) Surface Terminations by Scanning Tunneling Microscopy Contrast Reversal. Phys. Rev. B 2021, 103 (3), 035409. https://doi.org/10.1103/PhysRevB.103.035409.. (27) Karlberg, G. S.; Jaramillo, T. F.; Sklason, E.; Rossmeisl, J.; Bligaard, T.; Nrskov, J. K.. Cyclic Voltammograms for H on Pt(111) and Pt(100) from First Principles. Phys. Rev. Lett. 2007, 99 (12), 126101. https://doi.org/10.1103/PhysRevLett.99.126101.. (28) Hrmann, N. G.; Reuter, K. Thermodynamic Cyclic Voltammograms Based on Ab Initio. Calculations: Ag(111) in Halide-Containing Solutions. J. Chem. Theory Comput. 2021, 17 (3), 17821794. https://doi.org/10.1021/acs.jctc.0c01166.. (29) Rossmeisl, J.; Jensen, K. D.; Petersen, A. S.; Arnarson, L.; Bagger, A.; Escudero-. Escribano, M. Realistic Cyclic Voltammograms from Ab Initio Simulations in Alkaline and Acidic Electrolytes. J. Phys. Chem. C 2020, 124 (37), 2005520065. https://doi.org/10.1021/acs.jpcc.0c04367.. (30) Van De Walle, A. Methods for First-Principles Alloy Thermodynamics. JOM 2013, 65. (11), 15231532. https://doi.org/10.1007/s11837-013-0764-3.. (31) Van De Walle, A.; Asta, M. High-Throughput Calculations in the Context of Alloy. Design. MRS Bull. 2019, 44 (4), 252256. https://doi.org/10.1557/mrs.2019.71.. (32) Van Der Ven, A.; Bhattacharya, J.; Belak, A. A. Understanding Li Diffusion in Li-. Intercalation Compounds. Acc. Chem. Res. 2013, 46 (5), 12161225. https://doi.org/10.1021/ar200329r.. (33) Cao, L.; Li, C.; Mueller, T. The Use of Cluster Expansions To Predict the Structures and. Properties of Surfaces and Nanostructured Materials. J. Chem. Inf. Model. 2018, 58 (12), 24012413. https://doi.org/10.1021/acs.jcim.8b00413.. (34) Mathew, K.; Kolluru, V. S. C.; Mula, S.; Steinmann, S. N.; Hennig, R. G. Implicit Self-. Consistent Electrolyte Model in Plane-Wave Density-Functional Theory. J. Chem. Phys. 2019, 151 (23), 234101. https://doi.org/10.1063/1.5132354.. (35) Mathew, K.; Sundararaman, R.; Letchworth-Weaver, K.; Arias, T. A.; Hennig, R. G.. Implicit Solvation Model for Density-Functional Study of Nanocrystal Surfaces and Reaction Pathways. J. Chem. Phys. 2014, 140 (8), 084106. https://doi.org/10.1063/1.4865107.. 17. . (36) Puchala, B.; Thomas, J. C.; Van der Ven, A. CASM Monte Carlo: Calculations of the. Thermodynamic and Kinetic Properties of Complex Multicomponent Crystals. arXiv September 20, 2023. http://arxiv.org/abs/2309.11761 (accessed 2024-09-11).. (37) Nrskov, J. K.; Rossmeisl, J.; Logadottir, A.; Lindqvist, L.; Kitchin, J. R.; Bligaard, T.;. Jnsson, H. Origin of the Overpotential for Oxygen Reduction at a Fuel-Cell Cathode. J. Phys. Chem. B 2004, 108 (46), 1788617892. https://doi.org/10.1021/jp047349j.. (38) NIST-JANAF Thermochemical Tables. https://janaf.nist.gov.. (39) Marx, D. Proton Transfer 200 Years after von Grotthuss: Insights from Ab Initio. Simulations. ChemPhysChem 2006, 7 (9), 18481870. https://doi.org/10.1002/cphc.200600128.. (40) Sato, R.; Ohkuma, S.; Shibuta, Y.; Shimojo, F.; Yamaguchi, S. Proton Migration on. Hydrated Surface of Cubic ZrO 2: Ab Initio Molecular Dynamics Simulation. J. Phys. Chem. C 2015, 119 (52), 2892528933. https://doi.org/10.1021/acs.jpcc.5b09026.. (41) Suntivich, J.; Hautier, G.; Dabo, I.; Crumlin, E. J.; Kumar, D.; Cuk, T. Probing. Intermediate Configurations of Oxygen Evolution Catalysis across the Light Spectrum. Nat. Energy 2024. https://doi.org/10.1038/s41560-024-01583-x.. (42) Rao, R. R.; Kolb, M. J.; Halck, N. B.; Pedersen, A. F.; Mehta, A.; You, H.; Stoerzinger,. K. A.; Feng, Z.; Hansen, H. A.; Zhou, H.; Giordano, L.; Rossmeisl, J.; Vegge, T.; Chorkendorff, I.; Stephens, I. E. L.; Shao-Horn, Y. Towards Identifying the Active Sites on RuO 2 (110) in Catalyzing Oxygen Evolution. Energy Environ. Sci. 2017, 10 (12), 26262637. https://doi.org/10.1039/C7EE02307C.. . 18. . Supplementary Information. . . Figure S 1. X-ray diffraction of an MBE-grown 19 nm RuO2(110) film (~47 formula-units thick) on a TiO2(110) substrate.. . . Figure S 2. (Left) Top view of a 2*2 super-slab of RuO2 (110) surface with fully covered hydrogen atoms (The balls in red, pink are O and H. Oxygen octahedrons are centered with Ru). Two H atoms are on CUS sites with H on CUS Type 1 site pointing towards the BRG site line. Only one H atom can bind on the BRG site with the same pointed direction as CUS1. Some important atomic distances between H atoms are labeled. (Right) Side view of a 1*1 slab of RuO2 (110) with 4-Ru layers. The asymmetric slab with Ru-termination at the bottom is fixed atomic positions as the bulk geometry and O-termination at the top is allowed to relax and adsorbs H atoms.. . <image: DeviceRGB, width: 1340, height: 1028, bpc: 8>. <image: DeviceRGB, width: 908, height: 505, bpc: 8>. 19. . . Figure S 3. *H desorption profile over the three kinds of *H adsorption sites (CUS1, CUS2 and BRG). The two kinds of CUS sites are in orange (with CUS1 in , and CUS2 in  ), BRG is in green . During the 1st stage, half of CUS1 *H are desorbing, *H on CUS2 desorbs first and fast but get re-adsorbed from *H redistributed from CUS1, which maintains a constant H* site-fraction on all CUS; BRG *H are desorbing gradually over the whole course of 1st stage, spanning a potential window of ~1 V and are all gone at ~1.2 V, marking the boundary between 1st and 2nd peak. The 2nd stage starts right after all BRG H* are gone, where a sharp desorption of *H over both CUS sites occurs within a narrow ~0.1 V potential window.. . <image: DeviceRGB, width: 824, height: 618, bpc: 8>. 20. . Supplementary File. 1. Dehydrogenation Movie. ", "2410.18424v1.A_Causal_Graph_Enhanced_Gaussian_Process_Regression_for_Modeling_Engine_out_NOx.pdf": "A Causal Graph-Enhanced GaussianProcess Regression for ModelingEngine-out NOx. Shrenik Zinage1, Ilias Bilionis1, Peter Meckl1. AbstractThe stringent regulatory requirements on nitrogen oxides (NOx) emissions from diesel compression ignition enginesrequire accurate and reliable models for real-time monitoring and diagnostics. Although traditional methods suchas physical sensors and virtual engine control module (ECM) sensors provide essential data, they are only usedfor estimation. Ubiquitous literature primarily focuses on deterministic models with little emphasis on capturing theuncertainties due to sensors. The lack of probabilistic frameworks restricts the applicability of these models for robustdiagnostics. The objective of this paper is to develop and validate a probabilistic model to predict engine-out NOxemissions using Gaussian process regression. Our approach is as follows. We employ three variants of Gaussianprocess models: the first with a standard radial basis function kernel with input window, the second incorporating adeep kernel using convolutional neural networks to capture temporal dependencies, and the third enriching the deepkernel with a causal graph derived via graph convolutional networks. The causal graph embeds physics knowledge intothe learning process. All models are compared against a virtual ECM sensor using both quantitative and qualitativemetrics. We conclude that our model provides an improvement in predictive performance when using an input windowand a deep kernel structure. Even more compelling is the further enhancement achieved by the incorporation of acausal graph into the deep kernel. These findings are corroborated across different validation datasets.. KeywordsGaussian process regression, causal graph, graph neural networks, convolutional neural networks, deep kernel, engine-out NOx, diesel compression ignition engine. Introduction. Given the detrimental impact of nitrogen oxides (NOx) onenvironmental and human health, stringent regulations (EPA2021) are essential to mitigate these effects and ensuresustainable urban air quality. In this context, developingrobustpredictivemodelsiscrucialforguidingtheevolution of internal combustion engine technology, enablingmanufacturers to comply with evolving global environmentallegislation and effectively reducing air pollutants, includingthose contributing to greenhouse gas emissions. Researchonpredictivemodelsforengine-outNOxemissionsremains a dynamic and ongoing area of focus withinthedieselpowertrainresearchcommunity.Althoughextensive research in this field is documented in the openliterature (Aliramezani et al. 2022), more studies need tobe conducted to address the uncertainty analysis of thesemodels. These models are typically designed for real-time implementation, and the model prediction is oftenused for control or diagnostic purposes. Consequently, itis essential to develop probabilistic models that not onlyforecast emissions but also specify expected error marginsor define an error probability distribution.Given the extensive research on modeling engine-outNOx, various methods have been explored to address thecomplexities and uncertainties inherent in these systems.Traditional physics-based models have been developed tocapture the underlying mechanisms of NOx formationin diesel engines. For instance Asprion et al. (2013). proposed a fast and accurate physics-based model for NOx.Similarly Aithal (2010) used finite-rate chemical kineticsto model NOx formation in diesel engines, highlightingthe role of combustion processes. These models, whileinsightful, often require extensive computational resourcesand detailed knowledge of engine parameters. In contrast,data-driven approaches have gained popularity due totheir ability to learn from empirical data and predictNOx emissions with high accuracy.Fang et al. (2022)used artificial neural networks to predict transient NOxemissions from high-speed direct injection diesel engines.SimilarlyShin et al. (2020) developed a deep neuralnetwork model with Bayesian hyperparameter optimizationto predict NOx emissions under transient conditions.Other studies have also used support vector machinesand particle swarm optimization for emission modeling inhomogeneous charge compression ignition engines (Gordonet al. 2023). Hybrid approaches that combine physicalinsights with machine learning techniques have also beenexplored. For instance, Shahpouri et al. (2021) investigatedhybrid ML methods for soot emission prediction. However,. 1School of Mechanical Engineering, Purdue University, West Lafayette,IN, USA. Corresponding author:Shrenik ZinageEmail: szinage@purdue.edu. Prepared using sagej.cls [Version: 2017/01/17 v1.20]. arXiv:2410.18424v1  [cs.LG]  24 Oct 2024. 2International Journal of Engine Research XX(X). these deterministic models often overlook the uncertaintiesassociated with sensor data and engine dynamics. Toaddress this limitation, probabilistic frameworks have beenproposed. Yousefian et al. (2021) applied Bayesian inferenceand uncertainty quantification to hydrogen-enriched andlean-premixed combustion systems. Cho et al. (2018)presented a structured approach to uncertainty analysis ofpredictive models for engine-out NOx, highlighting the needfor robust uncertainty quantification.Gaussian processes (GPs) (Williams and Rasmussen. 2006) are widely used in Bayesian modeling due to theirinterpretability and robust ability to quantify uncertainty.Typically,GPsrelyonasmallnumberofkernelhyperparameters, which are optimized based on the marginallikelihood. However, in most common scenarios, the kernelis fixed, which limits the GPs ability to adaptivelylearn from data in a way that could improve predictiveperformance. This leads to GP primarily acting as smoothingmechanisms, which can be a constraint when dealingwith complex, high-dimensional data. On the other hand,deep neural networks (LeCun et al. 2015) are known fortheir powerful representation learning capabilities, enablingthem to make predictions on unseen test data. Despitetheir success, standard deterministic neural networks oftenproduce overconfident predictions (Guo et al. 2017) andstruggle with providing reliable uncertainty estimates.Bayesian neural networks (BNNs) have been proposed toaddress these limitations, but inference in BNNs remainschallenging due to the complex posterior distributions andthe large number of parameters involved. Furthermore,BNNs often require multiple forward passes to obtain anestimate of the predictive posterior, increasing computationalcosts. Given these issues, it is appealing to combinethe uncertainty quantification strengths of GPs with therepresentation learning power of neural networks. This led tothe development of deep kernel learning (DKL) (Wilson et al.2016), which uses a neural network to learn representationsfrom data and then inputs these representations into a GP.This approach allows for joint end-to-end training of boththe kernel and network parameters, either through variationalinference or marginal likelihood optimization. In Wilsonet al. (2016), DKL has demonstrated superior performancecompared to traditional kernel methods, such as the radialbasis function (RBF) kernel, as well as standard neuralnetworks, across a variety of tasks.Graph neural networks (GNNs) have emerged as apowerful tool for learning from graph-structured data,evolving through several stages to address increasinglycomplexdatarepresentationandlearningtasks.Theinception of GNNs (Scarselli et al. 2008) marked asignificant shift in how data structured in graphs couldbe processed by learning algorithms. Graph convolutionalnetworks (GCNs) (Kipf and Welling 2016a) introduced theconcept of applying convolutional operations directly ongraphs. These networks extend the convolutional paradigmto graph data by considering the graphs structure inthe convolution operation, allowing for the aggregation ofneighbor features through a form of weighted average. Thisapproach effectively captures local graph structures and nodefeatures, leading to improved performance in tasks such asnode classification and graph classification. Building on the. success of GCNs, graph attention networks (Velickovic et al.2017) introduced an attention mechanism that allows nodesto weigh the importance of their neighbors informationdynamically. This mechanism allows the model to focusmore on relevant parts of the graph, improving the modelsability to learn from complex graph structures. GATs haveshown significant improvements in various tasks by allowingmore nuanced feature aggregation from neighbors, comparedto the more uniform aggregation in GCNs. Gated graphsequence networks (GGSNs) (Li et al. 2015) represent afurther evolution in the processing of graph-structured data,incorporating elements of sequence modeling into graphnetworks. By using gated recurrent units (Zinage et al.2024b) or similar mechanisms, GGSNs can model graphdynamics and temporal changes, making them particularlysuited for tasks involving sequences of graphs or graphswith evolving structures. This approach has expanded theapplicability of GNNs to a broader range of tasks, includingthose that involve time-series data on graphs. Variationalgraph autoencoders (VGAEs) (Kipf and Welling 2016b) alsorepresent a key development in unsupervised learning forgraph-structured data, extending the idea of autoencoderswith a probabilistic framework. VGAEs use an encoder-decoder structure where the encoder maps nodes into a latentspace, learning distributions over latent variables insteadof fixed embeddings. This probabilistic approach allowsVGAEs to capture uncertainty in the node representations,making the model more robust in learning from sparse ornoisy graph data. After generating the latent distributions,the decoder reconstructs the graphs adjacency matrixby sampling from these distributions and calculating thelikelihood of edges between nodes. VGAEs have beenparticularly effective in tasks like link prediction and graphgeneration, where understanding the inherent uncertainty ingraph structures is important. Graph isomorphism networks(GINs) (Xu et al. 2018) further push the boundaries of GNNsby improving their ability to differentiate between distinctgraph structures. GINs are as powerful as the Weisfeiler-Lehman test for graph isomorphism, which determineswhether two graphs are structurally identical. By using asum operation to aggregate neighbor features, GINs ensurethat even subtle differences in graph topology are captured.This makes GINs particularly effective in graph classificationtasks, where minor structural variations can significantlyaffect outcomes.. The primary contribution of this work is the incorporationof causal information within the deep kernel structure usingGCNs. This causal information derived from the causalgraph serves to embed physics-informed knowledge into thelearning process.. We have organized our paper as follows. We begin bydelving into the physical principles underlying the formationof NOx. Next, we discuss Gaussian process regression(GPR). Following this, we detail the findings of our study.Finally, the conclusions are succinctly presented.. This research has been conducted in collaboration withCummins Inc with the data from a Cummins medium-dutydiesel engine. In compliance with Cummins policies, allplots in this study have been normalized.. Prepared using sagej.cls. Zinage, et al.3. Engine-out NOx formation. The formation of NOx, consisting of nitric oxide (NO)and nitrogen dioxide (NO2), in diesel engines involvesseveral mechanisms. Notably, thermal NO, fuel NO, andprompt NO play crucial roles (Heywood 2018). Prompt NOpredominantly forms under fuel-rich conditions and exhibitsminimal temperature dependency. Conversely, fuel NO relieson nitrogenous compounds in the fuel. The dominant processin diesel engines for NO generation within the combustionchamber is thermal NO, as described by the extendedZeldovich mechanism (Lavoie et al. 1970). This processresults from the oxidation of atmospheric nitrogen. The keyreactions for thermal NO are as follows:. N2 + O  NO + N,. N + O2  NO + O,. N + OH  NO + H.. These reactions are very sensitive to combustion temperature(exceeding 2000 K), in-cylinder oxygen (O2) levels, andthe duration at peak temperatures within lean air-fuelmixtures (Bowman 1975). In contrast, NO2 formationprimarily occurs downstream of the cylinder throughincomplete oxidation of NO, explained by the reaction:. NO + HO2  NO2 + OH.. Engine-out NOxlevels are largely governed by thetemperature of the burnt gases and O2 concentration. Thesefactors are influenced by various engine parameters, suchas intake air mass flow rate, fuel flow rate along withengine speed and load. Consequently, these engine variablesare chosen for modeling engine-out NOx. An effectivemethod for controlling engine-out NOx in diesel enginesis exhaust gas recirculation (EGR). EGR predominantlyconsists of nitrogen (N2), carbon dioxide (CO2), andwater vapor (H2O), which displaces air in the cylinderand leads to reduced NOx formation. The decrease inNOx is attributed to the lowered oxygen concentration andcombustion temperatures due to the higher specific heatcapacity of the triatomic molecules present in EGR.. Gaussian Process Regression. GP is a probabilistic non-parametric approach which definesa prior over functions and can be used for regression tasks. AGP is exhaustively described by its mean function, m(x),and a covariance function known as the kernel function,k(x, x). Importantly, for any finite set of inputs, a GP willoutput a multivariate Gaussian distribution. In the context ofGPR, the underlying assumption is that the function to beapproximated is a sample from a GP.The mean function, m(x), and covariance functionk(x, x), are represented by the equations:. m(x) = E[f(x)],. k(x, x) = E[(f(x)  m(x))(f(x)  m(x))].. In practice, it is common to use a zero mean function,i.e., m(x) = 0, and focus on defining the covariancefunction. The covariance function, measures the similaritybetween data points. One common choice is the radial. basis function (RBF) kernel with automatic relevancedetermination (ARD). Assuming ld represents the lengthscale for each dimension, and D indicates the number ofdimensions, we have:. krbf (x, x) = 2 exp. . . D. d=1. (xd  xd)2. 2l2d. . ,. where 2 is the signal variance and  = (l1, l2, . . . ld, 2).The length scales ld control the relevance of differentdimensions. When ld is small, changes in the d-th dimensionhave a large effect on the covariance, making that dimensionmore relevant.Given a dataset D of n input vectors, X = (x1, . . . , xn),each having dimension D, these index an n  1 vector oftargets y = (y1, . . . , yn). Assuming the presence of additiveGaussian noise, the relationship between the likelihoodfunctions is:. yi | f(xi)  Nyi; f(xi), 2y,. where 2y is the variance of the noise. The observations areassumed to be independent. The predictive distribution of theGP at n test points indexed by X can then be expressed as:. f | X, X, y, , 2y  N (E [f] , C (f)) ,. E [f] = m(X) + k(X, X)k(X, X) + 2yI1 (y  m(X)). C [f, f] = k(X, X)  k(X, X)k(X, X) + 2yI1 k(X, X),. where,. f = f(X) = [f (x1) , . . . , f (xn)] .. Here, k(X, X) is an n  n matrix of covariances betweenthe GP evaluated at X and X. Additionally, m(X)designates the n  1 mean vector, and k(X, X) representsthe n  n covariance matrix evaluated at training inputs X.The conditional probability density of the target variable y,given the parameters  and input data X, can be expressedas:. log p(y | , X). model fity k(X, X) + 2yI1 y +. logk(X, X) + 2yIcomplexity penalty. . .. The parameters  are found by maximizing this quantity.. Deep kernel. A deep kernel (Wilson et al. 2016; Zinage et al. 2024a)integrates a neural network with a traditional kernel function,improving the models ability to capture complex patternsin data. The neural network effectively compresses anddistills the input space, mapping it to a lower-dimensionalrepresentation that is more informative for the GP model.The transformation induced by the neural network canallow the model to fit more complicated, non-smooth, andanisotropic functions, unlike the standard kernels like RBF,which model smooth and isotropic functions (Calandra et al.2016). Also, by mapping a high-dimensional input space. Prepared using sagej.cls. 4International Journal of Engine Research XX(X). to a lower-dimensional one using a neural network, wecan mitigate the curse of dimensionality. This makes theGPR more tractable and can improve its generalization byfocusing on the most informative dimensions of the data.. Input: Nsamples  Ninputs  Ws. Conv1D: Nsamples  256  Ws. ReLU: Nsamples  256  Ws. MaxPool1D: Nsamples  256  (Ws//2). Conv1D: Nsamples  128  (Ws//2). Flatten: Nsamples  128  (Ws//2). Linear: Nsamples  100. Linear: Nsamples  50. Linear: Nsamples  25. Linear: Nsamples  10. Linear: Nsamples  3. RBF Kernelk((xi, w), (xj, w)|, w). Figure 1. Deep kernel using CNN. Assuming (x; w) is a nonlinear mapping given by aneural network parameterized by weights w, and k(xi, xj)represents the RBF kernel, we transform the inputs as:. kdeep(xi, xj)  krbf ((xi; w), (xj; w)).. We simultaneously optimize all deep kernel hyperparametersby maximizing the log marginal likelihood of the exact GP.To capture temporal or sequential dependencies effec-tively, especially when dealing with time-series data, a slid-ing window approach is often used. The sliding windowtechnique involves using a fixed window to extract overlap-ping segments from the input data, allowing the model tocapture both recent and historical patterns in the sequence.If we consider a sliding input window, we would have theinput space dimension to be mapped from a number of inputfeatures (Ninputs)  window size (Ws) to a user-defined inputspace before feeding into the RBF kernel.In this study, we used convolutional neural networks(CNNs) to capture these temporal dependencies as depictedin Fig.1. CNNs are particularly well-suited to capturetemporal dependencies due to their translation invariance,local receptive fields, and efficient parameter sharing.. Translation invariance allows the model to detect patternsregardless of their position in the sequence, while localreceptive fields enable learning of localized temporaldependencies.Sharedweightsreducethenumberofparameters, improving computational efficiency.. Deep kernel while incorporating causalinformation. GCNs (Kipf and Welling 2016a) are a type of neural networkthat aims to generalize convolutional neural networks towork with graph data. In our approach, we use GCNs toincorporate causal information from the causal graph intothe deep kernel learning framework. Let us denote a graphas G = (V, E), where V is the set of vertices (or nodes)and E  V  V is the set of edges connecting the vertices.Each node v  V can have associated features, representedas a vector xv, which in our case are the time series data ofthe variables. The entire graph can thus be represented by afeature matrix M  RNF , where N is the number of nodesand F is the number of features per node. Additionally, thestructure of the graph is represented by an adjacency matrixA  {0, 1}NN that encodes the causal structure, whereAij = 1 if there is a causal effect from node i to node j, and0 otherwise. Assuming H(l) represents the hidden state inthe l-th layer with H(0) = M, W (l) being the weight matrixfor the l-th layer, A = A + I being the adjacency matrix ofthe graph G with added self-connections (I is the identitymatrix), D being the diagonal node degree matrix of A, and() representing a nonlinear activation function, the graphconvolution operation in its simplest form can be expressedas:H(l+1) = D 1. 2 A D 1. 2 H(l)W (l).. This operation effectively updates each nodes representationby considering its own features and those of its causalpredecessors, as defined by the graph structure. Thenormalization withD 1. 2 A D 1. 2ensures that the scaleof the feature representations is maintained, preventingvanishing or exploding gradients during training. Bystacking multiple GCN layers, the model can capturehigher-order dependencies, allowing it to learn rich featurerepresentations that incorporate causal information.The output of the GCN serves as the input to the GPskernel function. By transforming the input features usingthe GCN, we provide the GP with rich representations thatincorporate both the features and the causal informationbetween the input variables. The RBF kernel is then appliedto these transformed features. Fig. 2 shows the schematic of adeep kernel learning while encoding causal information. Thismeans that the GCN learns not just the static structure of thegraph but also how changes in one part of the graph (e.g.,one node or a set of nodes) might influence other parts. Onceits trained, even though the output graph may structurallyresemble the input, it contains a deeper understandingof the causal relationship within the graph. Please referto Zecevic et al. (2021) for further insights into connectionsbetween GNNs and structural causal models (SCMs). SCMsare frameworks that combine causal graphs with structuralequations to model the relationships between variables.Incorporating these structural equations into predictivemodels via SCMs offers several advantages over traditional. Prepared using sagej.cls. Zinage, et al.5. A. B. E. D. F. G. CA. B. E. D. F. G. C. A. B. E. D. F. G. C. Directed acyclic causal graph. Hidden layer. A. B. E. D. F. G. C. A. B. E. D. F. G. C. A. B. E. D. F. G. C. Hidden layer. A. B. E. D. F. G. C. ReLUReLUA. B. E. D. F. G. COutput. Directed acyclic causal graph. Causal information not encodedCausal information encoded. <image: DeviceGray, width: 93, height: 108, bpc: 1>. layer. Gaussian Processes. Figure 2. Deep kernel learning while incorporating causal information. Input: Nsamples  Ninputs  Ws. GCNConv: Nsamples  Ninputs  32. ReLU: Nsamples  Ninputs  32. GCNConv: Nsamples  Ninputs  16. ReLU: Nsamples  Ninputs  16. GCNConv: Nsamples  Ninputs  8. ReLU: Nsamples  Ninputs  8. GCNConv: Nsamples  Ninputs  4. Mean: Nsamples  Ninputs. Subsampling: Nsamples  3. Linear: Nsamples  8. Linear: Nsamples  4. Linear: Nsamples  3. RBF Kernelk((xi, w), (xj, w)|, w). Figure 3. Deep kernel using GCN. statistical or purely correlational models. Traditional modelsoften rely on correlation between variables, which may notreflect true causal relationships. SCMs on the other hand. explicitly model causation, ensuring that the influence ofeach variable on others is accurately represented. Due tothis, models based on causation are more likely to generalizewell to new unseen data, especially under interventionsor distribution shifts. The predictions remain reliable evenwhen the underlying data distribution changes, as causalrelationships are invariant to such changes.The causal graph for engine-out NOx was constructedbased on expert guidance from Cummins, reflecting aphysical understanding of how engine-out NOx is caused dueto other variables in a diesel compression ignition engine.However, due to intellectual property (IP) protection, wehave not shared the graph in this paper. It is important tonote that while GCNs incorporate causal graph informationinto feature representations, they do not perform explicitcausal inference or enforce causal constraints in predictions.Instead, it improves the models ability to learn complexdependencies by embedding structural information from thecausal graph (Thost and Chen 2021).. Verifying the approach on an illustrativeexample. To validate our proposed method, we apply it to a syntheticillustrative example based on a predefined SCM. Considera system with k input variables and one output variable.We denote the input variables as x1, x2, . . . , xk, and theoutput variable as y. Specifically, let yt represent the targetvariable at time step t. The SCM for this example defines thefollowing causal relationships:. x5 = x21,. x6 = x2 log(1 + |x4|) + x3,. x7 = sin(x5) cos(x5),. x8 = x26 +. |x5|,. y = x7 + x8.. In this model, each equation represents how a child variableis causally influenced by its parent variables. For example,x5 is directly influenced by x1, while y is directly influencedby x7 and x8. This hierarchical and nonlinear structureallows us to assess how well our causal graph-enhanced GP. Prepared using sagej.cls. 6International Journal of Engine Research XX(X). x1x2x3x4. x6. x8. x5. x7. y. Figure 4. Structural causal model (illustrative example). captures the underlying causal dependencies and nonlinearrelationships within the data, as depicted in Fig 4. To evaluateour approach, we generate synthetic data that adheresto the defined SCM, ensuring the preservation of causalrelationships. We begin by sampling the input variablesx1,t, x2,t, x3,t, x4,t for each time step t from predefineddistributions. Using these sampled inputs, we compute theintermediate variables x5,t, x6,t, x7,t, x8,t according to theSCM equations. The output variable yt is then defined asthe sum of x7,t and x8,t, with the additive Gaussian noiset  N(0, 2) to account for measurement uncertainty:. yt = x7,t + x8,t + t.. Furthermore, we model the measured input variables xi,t,measwith uncertainty by introducing measurement noise:. xi,t,meas | xi,t  Nxi,t,  2,i = 1, 2, . . . , k,. where  denotes the standard deviation. The GP model isthen trained to predict yt using the inputs while incorporatingthe information of the causal graph through the deepkernel. We evaluate the performance of our model bycomparing it with a standard GP and a GP equipped with aconventional deep kernel using multilayer perceptron (MLP).Additionally, we assess the predictive accuracy of our modelagainst scenarios where the causal information is eitherpartially correct or entirely incorrect, in order to validate theeffectiveness of our approach.. Results. Data Generation. Illustrative example To generate synthetic data, we set  =0.01 and  = 0.01. A total of 10,000 samples are generated,with the first 9000 samples used for training and the last 1000samples reserved for testing.. Engine-out NOx Experimental data from a multi-pulsefueling diesel compression ignition engine was provided byCummins Inc. For the construction of probabilistic models,. we elected to utilize the dataset that includes EGR asit covered the entire operating region of the engine. Theperformance of the trained models is then tested on sixdifferent validation datasets that were collected on variousduty cycles by intentionally running at varying engine-outNOx levels. In our discourse, we consider the experimentaldata to be the ground truth and the modeling results arecompared with it.. Input Selection. The input variables for modeling engine-out NOx werechosen primarily on the basis of the physical understandingof how NOx is produced in an engine. Using ARD offersus a certain level of flexibility with our input selections.Essentially, if a chosen input turns out not to be directlyrelevant, the ARD mechanism will assign a lower weight,minimizing its impact on the model. Therefore, we can besomewhat less stringent in ensuring that every single inputis precisely the right one. However, a crucial aspect tomonitor when using ARD is the correlation between the inputvariables. It is important to ensure that the inputs we considerare not correlated to each other which was indeed the case.Due to IP considerations, we have not disclosed the completelist of input variables used in this study. However, wecan confirm that the selected inputs include various engineoperating parameters related to the air handling system, fuelsystem, and combustion system, that are known to influenceNOx formation.. Data Normalization. Prior to training the GP model, we normalized all datasetsusing the empirical cumulative distribution function (ECDF)method. This normalization converts the data into a uniformdistribution, which is especially beneficial when the originaldistribution is unknown or does not conform to the Gaussiandistribution often assumed by many machine learningalgorithms. Additionally, the ECDF approach improves therobustness to outliers by ranking data points instead ofdirectly scaling their values.. Metrics. Illustrativeexample Tocomparetheefficacyofourproposed model in comparison to a standard GP and a GPwith a conventional deep kernel, we use the root meansquared error (RMSE) as the primary performance metric.Additionally, we assess the models performance in scenarioswith partially correct or entirely incorrect causal informationby using RMSE, mean absolute error (MAE), and thecoefficient of determination (R2) as evaluation metrics.. Engine-out NOx In order to assess the ability of the GPmodels to accurately model NOx, we employed severalquantitative metrics. These include the RMSE, the 90th,95th, and 98th percentiles of the absolute errors in NOxemissions, which provide a comprehensive view of theerror distribution. These metrics were selected for theirrobustness in assessing the model accuracy, as they allowus to capture not only the average model performance butalso the behavior of the model under various conditions,particularly focusing on extreme values. Additionally, to. Prepared using sagej.cls. Zinage, et al.7. examine the consistency of NOx emissions under identicalinput conditions, we used the Quantile-Quantile (Q-Q) plot,which provides a graphical representation of the empiricaldistribution of the models output compared to a theoreticaldistribution, thereby allowing a thorough assessment of themodels ability to replicate the observed NOx distributionunder repeated experiments.. Modeling Results. The GP models and deep kernel using GCN are programmedusing GPyTorch (Gardner et al. 2018) and PyTorchGeometric (Fey and Lenssen 2019) library respectively, witha PyTorch backend. We set the loss function as negativeof exact marginal log likelihood and the optimizer usedis Adam (Kingma 2014) with tuned hyperparameters. Toprevent the models from overfitting, we consider using earlystopping with a patience of 50 epochs.. Illustrative example We train three GP models: the firstwith a standard RBF kernel (GP (RBF)), the second witha conventional deep kernel using MLP (GP (DRBF-MLP)),and the third with a deep kernel based on GCN (GP(DRBF-GCN)). Table 1 presents the accuracy of the medianpredictions of these models, evaluated using RMSE as thenumber of training samples increases. Notably, the GP(DRBF-GCN) outperforms the other two models, especiallyin low-data regimes (i.e., when N = 1000) due to theinductive bias introduced by the causal information encodedin the deep kernel. As the number of training samplesincreases, the performance of all three models improves, andthey converge to similar levels of accuracy.. Table 1. Accuracy of the GP models (illustrative example). ModelsRMSE (using N training samples)N = 1000N = 5000N = 9000. GP (RBF)0.450.200.16. GP (DRBF-MLP)0.540.210.15. GP (DRBF-GCN)0.180.160.15. To further demonstrate that the model effectively encodescausal information, we train three additional GP models:one that encodes the correct causal relationships, one thatencodes partially correct causal relationships, and one thatencodes incorrect causal relationships. Table 2 shows theaccuracy of these models median predictions on the testdataset, evaluated using RMSE, MAE, and R2. We cansee that the best performance is achieved by the modelwhich encoded the correct causal information with theperformance degrading progressively as the encoding ofcausal information becomes partially correct to completelyincorrect. This verifies our proposed approach.. Table 2. Accuracy of the GP models with different causalinformation (illustrative example). ModelsRMSEMAER2. Correct causal information0.150.110.95. Partially correct causal information0.520.310.54. Incorrect causal information2.311.98-9.68. Engine-out NOx In our comprehensive analysis of variousGP models built for predicting engine-out NOx, we were. provided with the readings from an ECM virtual sensorprovided by Cummins for the purpose of comparison. Thiscomparative study revealed distinct trends and performancenuances in our models. Firstly, there is a clear indicationthat increasing the complexity of GP models improvestheir predictive performance. This is particularly noticeablewhen comparing the performance of standard GP (RBF)models with different input window sizes against relativelycomplex models such as GP (Deep RBF) with CNN orGCN. These complex models often outperform both thestandard GP models and the ECM virtual sensor providedby Cummins, especially in scenarios with cumulative NOxlevel 2 (as seen in Tables 3, 4 and 8). However, this trendis not universal across all test cases, suggesting that whilecomplexity contributes to performance, it is not the soledeterminant. We can see that GP (DRBF-GCN) [Ws = 5s]performs relatively better than all other models for validation1, 2, and 6 (see Tables 3, 4 and 8). However, we observea relatively better performance of more simpler models forcumulative NOx level 1 scenarios (as seen in Tables 6 and7).It is important to differentiate between epistemic andaleatory uncertainties in this context. Epistemic uncertaintyoriginates from insufficient knowledge or data within themodel, while aleatory uncertainty is due to the inherentvariability in the system under study. In the cumulativeNOx level 3 scenario of validation 6, all GP modelsincluding the more complex ones, underperform significantlycompared to the ECM sensor. The ECM sensors relativelybetter performance in this scenario can be linked to itstraining on a different dataset, likely encompassing a widerrepresentation of NOx conditions with cumulative NOx level3. This highlights a potential gap in our training dataset forscenarios with cumulative NOx level 3, particularly in termsof epistemic uncertainty. Fig. 5 further supports this, showingconsiderable epistemic uncertainty as compared to aleatoryuncertainty in the GP (DRBF-CNN) [Ws = 5s] modelpredictions for FTP with cumulative NOx level 3, suggestingencounters with scenarios not adequately captured in thetraining phase. Due to encoding of causal relationships in thedeep kernel for GP (DRBF-GCN), we can observe a relativedecrease in the offset between the median prediction and themeasured data (Fig. 6) as compared to GP (DRBF-CNN).Although incorporating physical laws did not help us removethe offset completely, it was at least able to decrease thisoffset compared to pure regression models. Please note thatall the values presented in the tables are expressed in partsper million (ppm).. Table 3. Validation 1 (FTP cumulative NOx level 2). ModelsRMSENOx Error Percentiles90th95th98th. GP (RBF) [Ws = 1s]61.3975.86111.10174.29. GP (RBF) [Ws = 2s]63.3068.81118.17193.81. GP (RBF) [Ws = 3s]61.6873.78113.68187.26. GP (RBF) [Ws = 4s]63.0880.54117.73174.86. GP (RBF) [Ws = 5s]66.5877.56120.20203.28. GP (DRBF-CNN) [Ws = 5s]61.4680.18120.83176.45. GP (DRBF-GCN) [Ws = 5s]60.7374.58110.33170.36. ECM virtual sensor102.53142.88189.14361.77. To analyze the variability of NOx conditioned on thesame input, Fig. 7 shows the Q-Q plots between the sample. Prepared using sagej.cls. 8International Journal of Engine Research XX(X). Table 4. Validation 2 (RMCSET cumulative NOx level 2). ModelsRMSENOx Error Percentiles90th95th98th. GP (RBF) [Ws = 1s]46.2081.7897.09105.62. GP (RBF) [Ws = 2s]46.5564.2569.6291.48. GP (RBF) [Ws = 3s]42.8774.0982.3286.66. GP (RBF) [Ws = 4s]44.4474.9083.6986.88. GP (RBF) [Ws = 5s]45.7875.0782.5585.91. GP (DRBF-CNN) [Ws = 5s]50.1683.6692.3597.72. GP (DRBF-GCN) [Ws = 5s]42.8674.1882.4586.95. ECM virtual sensor58.59105.38111.58140.42. Table 5. Validation 3 (Step cycle cumulative NOx level 2). ModelsRMSENOx Error Percentiles90th95th98th. GP (RBF) [Ws = 1s]117.62126.26185.99313.48. GP (RBF) [Ws = 2s]73.7098.19141.58196.38. GP (RBF) [Ws = 3s]82.45132.19175.77250.93. GP (RBF) [Ws = 4s]90.24132.87183.10268.93. GP (RBF) [Ws = 5s]101.43147.34197.26294.08. GP (DRBF-CNN) [Ws = 5s]81.33101.54153.63254.07. GP (DRBF-GCN) [Ws = 5s]83.54103.82158.26241.45. ECM virtual sensor216.60349.67561.70758.07. Table 6. Validation 4 (FTP cumulative NOx level 1). ModelsRMSENOx Error Percentiles90th95th98th. GP (RBF) [Ws = 1s]89.56100.05132.69199.25. GP (RBF) [Ws = 2s]80.38103.20134.09169.11. GP (RBF) [Ws = 3s]77.9494.85124.80171.09. GP (RBF) [Ws = 4s]77.3788.78116.60148.06. GP (RBF) [Ws = 5s]83.3798.78125.67152.97. GP (DRBF-CNN) [Ws = 5s]126.98209.13291.76334.32. GP (DRBF-GCN) [Ws = 5s]93.71110.48143.31205.34. ECM virtual sensor152.79225.43287.62400.98. Table 7. Validation 5 (Step cycle cumulative NOx level 1). ModelsRMSENOx Error Percentiles90th95th98th. GP (RBF) [Ws = 1s]123.90190.46247.38392.68. GP (RBF) [Ws = 2s]85.35132.38177.28256.68. GP (RBF) [Ws = 3s]84.83130.67189.10277.71. GP (RBF) [Ws = 4s]92.76147.26212.70313.76. GP (RBF) [Ws = 5s]96.39151.64229.76313.40. GP (DRBF-CNN) [Ws = 5s]89.66138.24186.94265.52. GP (DRBF-GCN) [Ws = 5s]91.58143.65188.63265.20. ECM virtual sensor213.85324.20510.12711.17. Table 8. Validation 6 (FTP cumulative NOx level 3). ModelsRMSENOx Error Percentiles90th95th98th. GP (RBF) [Ws = 1s]438.44691.64703.77731.37. GP (RBF) [Ws = 2s]473.84791.03819.27848.78. GP (RBF) [Ws = 3s]345.26569.79584.68616.90. GP (RBF) [Ws = 4s]289.50468.79481.60515.96. GP (RBF) [Ws = 5s]285.97467.26484.28527.75. GP (DRBF-CNN) [Ws = 5s]257.10399.96427.23455.54. GP (DRBF-GCN) [Ws = 5s]138.18123.43183.87380.44. ECM virtual sensor107.55108.35169.16349.40. quantiles of the GP (DRBF-GCN) [Ws = 5s] model (inthe scaled version) and the theoretical quantiles of standardnormal for all validation datasets. In an ideal case thesedistributions should be the same i.e., all the points must lie onthe 45 degree dotted red line. We can see that except for FTPwith cumulative NOx level 3, the intermediate NOx valuesclosely follow the standard normal. There is a reasonable. 1. 1200. Time (s). Engine-out NOx conc. [Normalized]. MeasuredECM virtual sensorGP(DRBF- CNN)[Ws = 5s] (median). 95% epistemic uncertainty95% epistemic + aleatory uncertainty. 8. 0.14. 0. 0.29. 0.43. 0.57. 0.72. 0.86. 02004006008001000. Figure 5. GP(DRBF-CNN)[Ws = 5s] predictions on FTP withcumulative NOx level 3. Engine-out NOx conc. [Normalized]. Time (s). 0200400600800100012000. 0.14. 0.29. 0.43. 0.57. 0.72. 0.86. 1. MeasuredECM virtual sensorGP(DRBF- GCN)[Ws = 5s] (median). Figure 6. GP(DRBF-GCN)[Ws = 5s] predictions on FTP withcumulative NOx level 3. difference observed between the sample and theoreticalquantiles for extreme low and high NOx values. This isindicative that our model is very sensitive to the changes ininput values for predicting extreme low or high NOx values.Due to the gap between validation 6 and the training datasets,we can observe reasonable deviations between these twoquantiles for FTP with cumulative NOx level 3. We couldnot compare these results with the ECM virtual sensor as itwas not a probabilistic model.. Conclusions. This paper developed and validated a set of probabilisticmodels for predicting engine-out NOx using GPR. Thesemodels were compared against a virtual ECM sensor,providing a robust framework for assessing their predictiveperformance under different operating conditions. Wesystematically increased the complexity of the model inthree stages: first, by incorporating memory through varying. Prepared using sagej.cls. Zinage, et al.9. Theoretical quantiles. Theoretical quantilesTheoretical quantiles. Sample quantiles. Theoretical quantiles. Sample quantiles. Sample quantilesSample quantiles. Sample quantiles. Sample quantiles. Theoretical quantiles. RMCSET cumulative NOx level 2Step cycle cumulative NOx level 2. FTP cumulative NOx level 1. FTP cumulative NOx level 2. Step cycle cumulative NOx level 1FTP cumulative NOx level 3. Theoretical quantiles. Figure 7. Quantile-Quantile plots (GP(DRBF-GCN)[Ws = 5s]). input window sizes; second, by using CNN within thedeep kernel framework to improve the models ability tolearn complex temporal patterns; and finally, by using GCNto incorporate causal information, embedding knowledgeinformed by physics into the learning process. The keytakeaways from the results indicate that increasing modelcomplexity led to improved performance in NOx scenarioswith cumulative NOx level 2, with the GP(DRBF-GCN)model consistently outperforming simpler models and theECM sensor. The incorporation of causal information inthe GP(DRBF-GCN) model reduced the offset betweenpredictions and actual measurements for the FTP cycle withcumulative NOx level 3, although performance in extremeNOx cases varied. Simpler models performed better inscenarios with cumulative NOx level 1, and NOx predictions(more specifically, the FTP cycle with cumulative NOxlevel 3) suffered from high epistemic uncertainty due toinsufficient training data.AlthoughtheGP(DRBF-GCN)modeldemonstratessuperior performance in encoding and leveraging causalinformation for NOx prediction, a significant limitationmust be considered. The efficacy of GP(DRBF-GCN) isheavily based on the accuracy of the encoded causalrelationships. As illustrated in Table 2, the performance ofthe model significantly degrades when causal informationis partially correct or incorrect, highlighting its sensitivityto the quality of causal encoding. This dependence requiresprecise identification and integration of causal factors, whichmay not always be feasible in complex engine systems.. Authors contributionsShrenik Zinage: Methodology, Software, Validation, Visu-alization, Writing - original draft. Ilias Bilionis: Fundingacquisition, Methodology, Validation, Writing - review andediting. Peter Meckl: Funding acquisition, Validation, Writ-ing - review and editing.. AcknowledgementThe authors extend sincere gratitude to Akash Desai fromCummins Inc. for his invaluable feedback and guidancethroughout this research. Additionally, heartfelt thanks aredue to Dr. Lisa Farrell and Clay Arnett from CumminsInc., not only for sponsoring this research but also for theircrucial technical input and the provision of experimental dataessential for conducting the simulations.. Declaration of conflicting interestsThe author(s) declared no potential conflicts of interest with. respect to the research, authorship, and/or publication of thisarticle.. FundingThis work has been funded by Cummins Inc under grantnumber 00099056.. References. Aithal S (2010) Modeling of nox formation in diesel engines usingfinite-rate chemical kinetics.Applied Energy 87(7): 22562265.. Aliramezani M, Koch CR and Shahbakhti M (2022) Modeling,diagnostics, optimization, and control of internal combustionengines via modern machine learning techniques: A review andfuture directions. Progress in Energy and Combustion Science88: 100967.. Asprion J, Chinellato O and Guzzella L (2013) A fast and accuratephysics-based model for the nox emissions of diesel engines.Applied energy 103: 221233.. Bowman CT (1975) Kinetics of pollutant formation and destructionin combustion.Progress in energy and combustion science1(1): 3345.. Calandra R, Peters J, Rasmussen CE and Deisenroth MP (2016)Manifold gaussian processes for regression.In: 2016International joint conference on neural networks (IJCNN).IEEE, pp. 33383345.. Cho H, Brewbaker T, Upadhyay D, Fulton B and Van NieuwstadtM (2018) A structured approach to uncertainty analysis ofpredictive models of engine-out nox emissions. InternationalJournal of Engine Research 19(4): 423433.. EPA(2021)Regulationsforemissionsfromvehiclesandengines.URLhttps://www.epa.gov/regulations-emissions-vehicles-and-engines/cleaner-trucks-initiative.. Fang X, Zhong F, Papaioannou N, Davy MH and Leach FC (2022)Artificial neural network (ann) assisted prediction of transientnox emissions from a high-speed direct injection (hsdi) dieselengine. International Journal of Engine Research 23(7): 12011212.. Fey M and Lenssen JE (2019) Fast graph representation learningwith pytorch geometric. arXiv preprint arXiv:1903.02428 .. Gardner J, Pleiss G, Weinberger KQ, Bindel D and Wilson AG(2018) Gpytorch: Blackbox matrix-matrix gaussian processinference with gpu acceleration.Advances in neuralinformation processing systems 31.. Gordon D, Norouzi A, Blomeyer G, Bedei J, Aliramezani M,Andert J and Koch CR (2023) Support vector machine basedemissions modeling using particle swarm optimization forhomogeneous charge compression ignition engine.Interna-tional Journal of Engine Research 24(2): 536551.. Guo C, Pleiss G, Sun Y and Weinberger KQ (2017) On calibrationof modern neural networks. In: International conference onmachine learning. PMLR, pp. 13211330.. Heywood JB (2018) Internal combustion engine fundamentals.McGraw-Hill Education.. Kingma DP (2014) Adam: A method for stochastic optimization.arXiv preprint arXiv:1412.6980 .. Kipf TN and Welling M (2016a) Semi-supervised classifica-tion with graph convolutional networks.arXiv preprintarXiv:1609.02907 .. Prepared using sagej.cls. 10International Journal of Engine Research XX(X). Kipf TN and Welling M (2016b) Variational graph auto-encoders.arXiv preprint arXiv:1611.07308 .. Lavoie GA, Heywood JB and Keck JC (1970) Experimentaland theoretical study of nitric oxide formation in internalcombustion engines. Combustion science and technology 1(4):313326.. LeCun Y, Bengio Y and Hinton G (2015) Deep learning. nature521(7553): 436444.. Li Y, Tarlow D, Brockschmidt M and Zemel R (2015) Gated graphsequence neural networks. arXiv preprint arXiv:1511.05493 .. Scarselli F, Gori M, Tsoi AC, Hagenbuchner M and Monfardini G(2008) The graph neural network model. IEEE transactions onneural networks 20(1): 6180.. Shahpouri S, Norouzi A, Hayduk C, Rezaei R, Shahbakhti M andKoch CR (2021) Soot emission modeling of a compressionignition engine using machine learning. IFAC-PapersOnLine54(20): 826833.. Shin S, Lee Y, Kim M, Park J, Lee S and Min K (2020) Deep neuralnetwork model with bayesian hyperparameter optimization forprediction of nox at transient conditions in a diesel engine.Engineering Applications of Artificial Intelligence 94: 103761.. Thost V and Chen J (2021) Directed acyclic graph neural networks.arXiv preprint arXiv:2101.07965 .. Velickovic P, Cucurull G, Casanova A, Romero A, Lio P andBengio Y (2017) Graph attention networks.arXiv preprintarXiv:1710.10903 .. Williams CK and Rasmussen CE (2006) Gaussian processes formachine learning, volume 2. MIT press Cambridge, MA.. Wilson AG, Hu Z, Salakhutdinov R and Xing EP (2016) Deepkernel learning. In: Artificial intelligence and statistics. PMLR,pp. 370378.. Xu K, Hu W, Leskovec J and Jegelka S (2018) How powerful aregraph neural networks? arXiv preprint arXiv:1810.00826 .. Yousefian S, Bourque G and Monaghan RF (2021) Bayesianinference and uncertainty quantification for hydrogen-enrichedand lean-premixed combustion systems. international journalof hydrogen energy 46(46): 2392723942.. Zecevic M, Dhami DS, Velickovic P and Kersting K (2021)Relating graph neural networks to structural causal models.arXiv preprint arXiv:2109.04173 .. Zinage S, Mondal S and Sarkar S (2024a) Dkl-kan: Scalable deepkernel learning using kolmogorov-arnold networks.arXivpreprint arXiv:2407.21176 .. Zinage V, Zinage S, Bettadpur S and Bakolas E (2024b) Leveraginggated recurrent units for iterative online precise attitude controlfor geodetic missions. arXiv preprint arXiv:2405.15159 .. Prepared using sagej.cls", "2410.18443v1.ELECTRE_TRI_nB__pseudo_disjunctive__axiomatic_and_combinatorial_results.pdf": "arXiv:2410.18443v1  [cs.DM]  24 Oct 2024. ELECTRE TRI-nB, pseudo-disjunctive:axiomatic and combinatorial results a. Denis Bouyssou bThierry Marchant cMarc Pirlot d. October 16, 2024. Abstract. ELECTRE TRI-nB is a method designed to sort alternatives evaluatedon several attributes into ordered categories. It is an extension of ELECTRETRI-B, using several limiting proles, instead of just one, to delimit eachcategory.ELECTRE TRI-nB comes in two avours: pseudo-conjunctiveand pseudo-disjunctive. In a previous paper we have characterized the or-dered partitions that can be obtained with ELECTRE TRI-nB, pseudo-conjunctive, using a simple axiom called linearity. The present paper is ded-icated to the axiomatic analysis of ELECTRE TRI-nB, pseudo-disjunctive.It also provides some combinatorial results.Keywords: Multiple criteria analysis, Sorting models, ELECTRE TRI-nB.. 1Introduction. ELECTRE TRI (or ETRI for short) is a family of methods for sorting alterna-tives evaluated on several attributes into ordered categories. The rst method inthis family was ETRI-B (Roy and Bouyssou, 1993, Yu, 1992). Then came severalvariants, that we do not detail.1 Recently, Fernandez, Figueira, Navarro, and Roy(2017) proposed a new variant (named ELECTRE TRI-nB or ETRI-nB for short)that uses several limiting proles instead of merely one as in the original ETRI-B.. <image: None, width: 1, height: 1, bpc: 1>. aAuthors are listed alphabetically. They have contributed equally.bFormer Senior Researcher, CNRS, Paris, France, e-mail: dbouyssou@gmail.com.cGhent University, Department of Data Analysis, H. Dunantlaan, 1, B-9000 Gent, Belgium,e-mail: thierry.marchant@UGent.be.dUniversitedeMons,ruedeHoudain9,7000Mons,Belgium,e-mail:marc.pirlot@umons.ac.be.1For an overview of ELECTRE methods, we refer to Roy and Bouyssou (1993, Ch. 5 & 6),Figueira, Greco, Roy, and Slowinski (2013), and Figueira, Mousseau, and Roy (2016).. 1. Like ETRI-B, the new ETRI-nB has two versions: pseudo-conjunctive (pc) andpseudo-disjunctive (pd).A simplied version of ETRI-B-pc received a detailed axiomatic analysis inBouyssou and Marchant (2007a,b), Greco, Matarazzo, and Slowinski (2001), Slowinski, Greco, and Matarazzo(2002).Later, Bouyssou and Marchant (2015) have shown that ETRI-B-pd ismuch more dicult to analyze than ETRI-B-pc, although their denitions mayseem dual to each other at rst sight.Bouyssou, Marchant, and Pirlot (2023)hereafter referred to as BMP23havecharacterized the pseudo-conjunctive version of ETRI-nB making auxiliary use of asimplied version thereof. This characterizations uses a single axiomLinearitythat was rst proposed by Goldstein (1991).Bouyssou, Marchant, and Pirlot(2022) have characterized the particular case of ETRI-nB using at most 2 lim-iting proles.The present paper intends to axiomatically analyze ETRI-nB-pd or a simpliedversion thereof. Our main ndings are twofold. The rst one is similar to thatin Bouyssou and Marchant (2015): ETRI-nB-pd is much more dicult to analyzethan ETRI-nB-pc, although their denitions may seem dual to each other. Thesecond one is a characterization of a special case of ETRI-nB-pd, involving Linear-ity and a new condition, raising some interesting combinatorial questions aboutmaximal antichains in direct products of chains.. 2Framework and notation. We use the framework of conjoint measurement (Krantz, Luce, Suppes, and Tversky,1971). As in BMP23, we will restrict our attention to the case of two categories.This allows us to use a simple framework while not concealing any important dif-culty.2 For the same reasons, we suppose throughout that the set of objects tobe sorted is nite.The nite set of alternatives is X = X1  . . .  Xn, with n  2. The setof attributes is N = {1, . . . , n}.For x, y  X, i  N and J  N, we useXJ, XJ, Xi, Xi, (xJ, yJ) and (xi, yi) as usual.Our primitives consist of atwofold partition A, U of the set X, where A (resp. U) contains the sA. <image: None, width: 1, height: 1, bpc: 1>. tisfactory(resp. U. <image: None, width: 1, height: 1, bpc: 1>. nsatisfactory) alternatives.An attribute i is inuential for A, U if there exist xi, yi  Xi and ai  Xisuch that (xi, ai)  A and (yi, ai)  U. If an attribute is not inuential, it doesnot play any role and can be suppressed. We therefore suppose without loss ofgenerality that all attributes are inuential.. <image: None, width: 1, height: 1, bpc: 1>. 2Bouyssou and Marchant (2007b) have shown how to extend the axiomatic analysis to the caseof more than two categories, in the case of ETRI-B. Their technique applies mutatis mutandisto ETRI-nB.. 2. 3Axiomatic analysis of ETRI-nB-pc: a digest. In this section, we recall some denitions and results presented in BMP23. AllETRI methods start with a preference modelling step during which a preferencerelation is built for each attribute. This valued preference relation depends ona number of parameters that we do not detail here. In a second step, these nvalued preference relations are aggregated into a single valued preference relationthat is afterwards cut to dene a crisp outranking relation S. The assignment ofalternatives to categories occurs in a third step. In order to save space, we do notpresent the exact denition of ETRI-nB-pc, but an idealization thereof: Model E.It mostly simplies steps 1 and 2 and we will later see that this does not entailany loss of generality. See Fernandez et al. (2017) for a complete description ofETRI-nB-pc and BMP23 for the relationship between Model E and ETRI-nB-pc.. Denition 1 (Models E, Ec, Eu)We say that a partition A, U has a representation in Model E if:. for all i  N, there is a semiorder Si on Xi (with asymmetric part Pi andsymmetric part Ii),. for all i  N, there is a strict semiorder Vi on Xi that is included in Pi andis the asymmetric part of a semiorder Ui,. (Si, Ui) is a homogeneous nested chain of semiorders and Wi is a weak orderthat is compatible with both Si and Ui, 3. there is a set of subsets of attributes F  2N such that, for all I, J  2N,[I  F and I  J]  J  F,. there is a binary relation S on X (with symmetric part I and asymmetricpart P) dened by. x S y  [S(x, y)  F and V (y, x) = ] ,. where S(x, y) = {i  N : xi Si yi} and V (x, y) = {i  N : xi Vi yi},. there is a set P = {p1, . . . , pk}  X of k limiting proles, such that for allp, q  P, Not[p P q],. such that. x  A. x S pfor some p  PandNot[q P x]for all q  P.(1). <image: None, width: 1, height: 1, bpc: 1>. 3Wi is the intersection of the weak orders Swoiand U woi, respectively induced by Si and Ui.See Appendix A of the supplementary material of BMP23.. 3. We then say that (Si, Vi)iN, F, P is a representation of A, U in Model E.Model Ec is the particular case of Model E, in which there is a representation thatshows no discordance eects, i.e. in which all relations Vi are empty. Model Eu isthe particular case of Model Ec, in which there is a representation that requiresunanimity, i.e. such that F = {N}.. In this denition, Si is the idealization of the preference relation on attribute i,Vi represents all pairs of levels on attribute i for which a discordance could occur(step 1).4 S is the idealization of the outranking relation (step 2). The third step(the assignment of alternatives to categories) is described by (1).Goldstein (1991) has proposed a simple condition that may be satised by somepartitions:. Denition 2 (Linearity)The partition A, U is linear on attribute i if, for all xi, yi  Xi and all ai, bi Xi,(xi, ai)  Aand(yi, bi)  A. . . . . (yi, ai)  Aor(xi, bi)  A.(2). The partition A, U is linear if it is linear on all attributes. If all partitions thatcan be represented in some Model M are linear, we say that Model M satisesLinearity.. Replacing A by U in (2) yields an equivalent denition of Linearity.On eachattribute Xi, we dene the relation i letting, for all xi, yi  Xi,. xi i yi if [for all ai  Xi, (yi, ai)  A  (xi, ai)  A].. By construction, i is transitive and reexive; it is complete if and only if thepartition is linear on attribute i. The symmetric part of i is denoted by i. It isnot useful to keep in Xi elements that are equivalent w.r.t. the equivalence relationi. Indeed, if xi  yi then (xi, ai)  A i (yi, ai)  A. In order to simplifynotation, we suppose throughout the paper that we are dealing with partitionson ni=1 Xi for which all relations i are trivial5. This non-restrictive conventionimplies that each relation i is antisymmetric.Let  be the relation on X dened by x  y i xi i yi for all i  N. Thisrelation is a partial order (reexive, transitive and antisymmetric).Let A =. <image: None, width: 1, height: 1, bpc: 1>. 4In Denition 1, Si and Vi are supposed to be semiorders. The reason of this assumptionis that the notion of semiorder is related to the existence of thresholds, as they appear in themodelling of preference and veto in the classical ELECTRE methods.5If i is not trivial, we can work without loss of generality with the quotient Xi/ i.. 4. min(A, ) be the set of minimal elements in A for . By construction, for anyx  A and yi i xi, we have (yi, xi)  U.We say Model M is nested inor is a special case ofModel M (denotedM  M) if all partitions that can be represented in M can also be represented inM. Models M and M are equivalent (denoted M  M) if M  M and M  M.We note M  M if M  M and M is not equivalent to M. By construction, wehave Eu  Ec  E. The main results in BMP23 can now be summarized in thefollowing theorem.. Theorem 11. ETRI-nB-pc  E  Ec  Eu.. 2. A partition A, U has a representation in any of these models i it is linear.. 3. This representation can always be taken to be (i, Vi = )iN, F = {N}, P =A, that is a representation in Model Eu.. We like to stress point 1: although model E and the nested models Ec and Eu. seem to be simplications of ETRI-nB-pc, they are not: all four models are fullyequivalent.. 4ETRI-nB-pd: denition and diculties. The pseudo-disjunctive version of ETRI-nB consists of three steps. The rst andthe second one are identical to steps 1 and 2 in ETRI-nB-pc. The only dierenceis the third step: the assignment of alternatives to categories. With ETRI-nB-pc,an alternative x is assigned to A i it is weakly preferred (in terms of S) to alimiting prole and no limiting prole is strictly preferred to x (in terms of P),as in (1). With ETRI-nB-pd, an alternative x is assigned to U i (i) there is alimiting prole strictly preferred (in terms of P) to x and (ii) x is not strictlypreferred to any limiting prole. As in Section 3, in order to save space, we do notpresent the exact denition of ETRI-nB-pd, but an idealization thereof: Model F.We dene Model F that is to ETRI-nB-pd what Model E is to ETRI-nB-pc.. Denition 3Model F is dened exactly as Model E, except that we now replace (1) by:. x  U. p P xfor some p  PandNot[x P q]for all q  P.(3). The denition of Models F c and F u parallels that of Ec and Eu.. All pseudo-disjunctive models mentioned so far satisfy linearity.. 5. Lemma 1If A, U has a representation in Model F, then it is linear. The same holds forETRI-nB-pd, F u and F c.. ProofConsider rst Model F. Suppose that we have (xi, ai)  U and (yi, bi)  U.We have either xi Wi yi or yi Wi xi since Wi is a weak order. Suppose wlog thatyi Wi xi. Because (yi, bi)  U, we know that p P (yi, bi), for some p  P, andNot[(yi, bi) P q] for all q  P. Using Lemma 3 in BMP23, we obtain p P (xi, bi)and Not[(xi, bi) P q] for all q  P.6 Hence, (xi, bi)  U and linearity holds forModel F. By construction, F u  F c  F and linearity thus also holds for thesemodels.Since we did not formally dene ETRI-nB-pd, we cannot provide the proofthat linearity holds for partitions generated by ETRI-nB-pd. For the interestedreader, this proof closely follows that of Corollary 1 in BMP23.. Hence, combining Lemma 1 with Theorem 1, we obtain the next proposition.. Proposition 1F u  F c  F  E and ETRI-nB-pd  E  ETRI-nB-pc.. At this stage, given the apparent duality between the denitions of the pseudo-conjunctive and pseudo-disjunctive models, we can suspect that F u  F c  F ETRI-nB-pd  E, but the next result shows that it does not hold.. Proposition 2F u  F c and F  E.. ProofPart 1: F u  F c. Let N = {1, 2, 3} and Xi = {0, 1} for all i  N, so that X has 23 = 8 ele-ments.Consider the partition A, U such that A = {111, 101, 011} and U ={110, 100, 010, 001, 000}, abusing notation in an obvious way. It is simple to checkthat all attributes are inuential for A, U and that, for all i  N, we have 1i i 0i.Notice that we have A = Min(, A) = {101, 011} and U = Max(, U) ={110, 010, 001}.Let us show that this partition cannot be obtained with Model F u. Observerst that, here, since all attributes are inuential and can only take two values, wemust have that Si = i, for all i  N.. <image: None, width: 1, height: 1, bpc: 1>. 6Lemma 3 in BMP23 is established under the hypothesis that the partition [A, U] is repre-sentable in Model E. Since the proof only uses the properties of relation S, which are commonto Models E and F, the result also holds for partitions representable in Model F.. 6. Since 110  U, there must be p  P such that p P 110. Since we are lookingfor a representation in Model F u and we know that Si = i, for all i  N, we mustnd a prole p  P such that p  110. The only candidate is 111. But takingP = {111} together with F = {N} does not lead to the desired partition. Indeed,we have 111  101, so that 101 should be in U.This partition can be obtained with Model F c, taking Si = i, for all i  N,P = {111} and F = {{1, 3}, {2, 3}}.. Part 2: F  E. Let n = 4 and X1 = X2 = X3 = {2, 1, 0} and X4 = {0, 1}, so that X has 54elements. Consider the partition A, U such that A = {2221, 2211, 2121, 1221,2111, 1211, 1121, 1111, 2220}. Notice that A = {1111, 2220}. It is easy to checkthat all attributes are inuential for A, U and that, for all i  {1, 2, 3}, we have2i i 1i i 0i, while 14 4 04. Hence, the partition is linear and, by Theorem 1, itcan be represented in Model E.In order to show that this partition cannot be obtained in Model F, wehave to examine, all cases of indierence thresholds (associated with the strictsemiorders Si), combined with all cases of veto thresholds (associated with thestrict semiorders Vi), and combined with all choices for F.Notice that if an attribute in i  {1, 2, 3} has thresholds (i.e. Si is not a weakorder), this means that 2i Ii 1i and 1i Ii 0i. But veto eects can only occur amongthe elements that are strictly preferred. Hence, in this case, the only possibility isto take 2i Vi 0i.If {1, 2, 4}  F, then, without veto, 2201  U outranks all elements in A, acontradiction. This will remain true unless, there is a veto eect on attribute 3.If 23 V3 03, the only elements in A that are not strictly beaten by antherelement in A are 2220, 1111, and 1121. It is easy to check that taking all of themor any subset of them as the set of proles does not lead to the desired partition(consider 2201  U). If, furthermore, 23 V3 13, the only elements in A that are notstrictly beaten by anther element in A are 2220 and 1111. It is easy to check thattaking all of them or any subset of them as the set of proles does not lead to thedesired partition (consider 2201  U).The analysis of the cases {1, 3, 4}  F and {2, 3, 4}  F is entirely similar.. Suppose now that F = {{1, 2, 3}, N}. Suppose that only attribute 1 has thresh-olds. Without veto, it is easy to check that 1220  U outranks all elements in A.This remains true, whatever the choice of veto thresholds on attributes 2 and 3.This also remains true if 14 V4 04. But veto eects on attribute 1 are immaterialsince 11 is indierent to both 21 and 01.The situation is entirely similar if 2 (resp. 3) is the only attribute to havethresholds.. 7. Suppose that only attributes 1 and 2 have thresholds. Without veto, it is easyto check that 1120  U outranks all elements in A. This remains true, whateverthe choice of veto thresholds on attributes 1 and 2 since 11 (resp. 12) is indierentto 21 and 01 (resp. 22 and 02). This also remains true if 14 V4 04. Clearly, the vetothreshold on attribute 3 is immaterial.The analysis of the cases in which 1 and 3 or 2 and 3 have thresholds is entirelysimilar.. It remains to tackle the case F = {N}.Suppose that only attribute 1 has thresholds. Without veto, there are only 3elements in A that are not strictly beaten by another element in A: 2220, 1111and 2111. It is easy to check that taking all of them or any subset of them asthe set of proles does not lead to the desired partition. It is simple to check thatwhatever the choice of veto we make on attributes 2, 3 and 4, the situation remainsthe same.There is only one possibility to put a veto on attribute 1, i.e. 21 V1 01. In thiscase there are only 2 elements in A that are not strictly beaten by another elementin A: 2220 and 1111. In any case, it is impossible to recover the desired partition.The situation is entirely symmetric in the case only attribute 2 or only attribute3 has thresholds.Suppose that both attributes 1 and 2 have thresholds. Without veto, thereare only 5 elements in A that are not strictly beaten by another element in A:2220, 1111, 2211, 2111, and 1211. It is easy to check that taking all of them orany subset of them as the set of proles does not lead to the desired partition. Itis simple to check that whatever the choice of veto we make on attributes 3 and4, the situation remains the same. There is only one possibility to put a veto onattribute 1 (resp. 2), i.e. 21 V1 01 (resp. 22 V2 02). It is simple to check that anyof the three possible choices for the veto on these attributes does not alter thesituation.The situation is entirely symmetric in the case only attributes 1 and 3 or onlyattributes 2 and 3 have thresholds.Finally, if all attributes have thresholds, there is only one element in A that isnot strictly beaten by another element in A: 2220. It is easy to check that takingthis element to be the unique prole, does not lead to the desired partition. Now,the choice of veto thresholds (they must be of the type 2i Vi 0i) on attributes 1, 2,and 3 is immaterial. But it is also simple to check that adding a veto on attribute4 does not change the situation.. Given Propositions 1 and 2, it would be highly desirable to know whetherETRI-nB-pd  E or ETRI-nB-pd  F. Unfortunately, we are presently unableto prove or disprove these equivalences. This shows that the relations between. 8. the pseudo-disjunctive models are more complex than between the correspondingpseudo-conjunctive models.. 5Two characterizations. In view of the above-metioned diculties, we devote this section to two simplerproblems: (1) the characterization of Models F c, F, E and ETRI-nB-pc when allattributes are binary and (2) the characterization of a special case of Model F u.. 5.1The case of binary attributes. Suppose the partition A, U is linear on attribute i. We say attribute i is binary ifthe weak order i has exactly two equivalence classes. Such attributes are commonin many applications. The case in which all attributes are binary corresponds to thewell-developed theory of monotone Boolean functions (see Crama and Hammer,2011).. Proposition 3F c  F  E  ETRI-nB-pc whenever all attributes are binary.. ProofWhen all attributes are binary, each Xi contains only two elements that we candenote by 1i and 0i with 1i i 0i . Each element in X corresponds to a uniquecoalition C(x) = {i  N : xi = 1i}  2N. Hence, all linear partitions have arepresentation in F c with Si = i, for all i  N, P = {111} and F = {C(x) : x A}.. Since the proof uses a set P containing only one limiting prole, the readermay have the impression that Proposition 3 only applies to ETRI-B and not toETRI-nB. What the result actually says is that any partition generated by amodel F c, F, E or ETRI-nB-pc (irrespective of the number of limiting proles)can be represented in the other three models. The proof further shows that therepresentation can be chosen so that P is a singleton.. 5.2A special case of Model F u. In order to reduce the complexity of the models, let us assume that the data areof good qualityin the sense of Roy (1996, Section 8.2)meaning that there isno imprecision, uncertainty, or inaccurate determination. In that case, there isno need to use preference or indierence thresholds and the relation Si is a weakorder. Since i is also a weak order and we cannot have xi i yi while yi Si xi, it. 9. must be the case that Si is a renement of i (i.e. Si  i). But since we haveassumed that the relation i is trivial, the equality Si = i must hold.So, in this section, we restrict our attention to partitions having a represen-tation in Model F u such that Si = i is a weak order for all i  N. Model F u. together with this additional constraint will be denoted by F u. <image: None, width: 1, height: 1, bpc: 1>. . In such a model,S = , P =  and Condition 3 reduces to x  U i p  x, for some p  P.Indeed, we may not have p  x  q for q  P, otherwise p  q, a contradiction.Notice that F u. <image: None, width: 1, height: 1, bpc: 1>. F u  F c  F  E.By construction, the set A = Min(, A) is an antichain in the poset (X, ),remembering our convention that each relation i is trivial. Observe that in therst part of the proof of Proposition 2, the antichain A = {101, 011} is not amaximal antichain, i.e. it is strictly included in the antichain {110, 101, 011}. Asshown below, a characteristic feature of partitions that can be represented in ModelF u. <image: None, width: 1, height: 1, bpc: 1>. is that A is a maximal antichain in the poset (X, ).. Theorem 2Let X = ni=1 Xi be a nite set and A, U be a twofold linear partition of X.The partition A, U has a representation in Model F u. <image: None, width: 1, height: 1, bpc: 1>. i the antichain A, in theposet (X, ), is maximal.. ProofNecessity. Suppose that A is not a maximal antichain. Hence there is x  X suchthat x is incomparable, using , w.r.t. all elements in A. In view of the denitionof A, it is impossible that x  A (since this would imply that x  z, for somez  A). Hence, we must have x  U, so that there must be a prole p  P suchthat p  x. This prole must be in A. But, by hypothesis, this prole cannotbelong to A. Hence, by construction, we know that p  y, for some y  A  A,which implies y  U, a contradiction.Suciency. Since A, U is linear, we know that it has a representation in ModelEu using the representation (i, Vi = )iN, F = {N}, P = A. Since A is amaximal antichain, it is easy to see that this representation is also a representationin Model F u. <image: None, width: 1, height: 1, bpc: 1>. . Indeed, by construction, it is impossible that x  U is incomparable,using , to all p  P = A. Let q  P be such that x and q are comparable using. It is impossible that x  q since this would imply that x  A, in view of thedenition of A = P. Hence, we must have that q  x.. The next result shows that F u. <image: None, width: 1, height: 1, bpc: 1>. F u, thereby showing that the hypothesis thatthe representation is such that Si = i, for all i  N, is not innocuous.. Proposition 4F u. <image: None, width: 1, height: 1, bpc: 1>. F u.. 10. ProofLet N = {1, 2, 3, 4} and Xi = {0, 1, 2} for all i  N, so that X has 34 = 81elements. Consider the partition A, U such that A = {2222, 2221, 2220, 2212,2211, 2210, 2202, 2201, 2200, 2122, 2121, 2120, 2112, 2111, 2110, 2102, 2101, 2022,2021, 2020, 2012, 2011, 2010, 2002, 2001, 1222, 1221, 1220, 1212, 1211, 1210, 1202,1201, 1200, 1122, 1121, 1120, 1112, 1111, 1110, 1102, 1101, 1022, 0222, 0221, 0220,0212, 0211, 0210, 0202, 0201, 0122, 0121, 0120, 0112, 0111, 0110, 0102, 0101, 0022}.The set A has 60 elements. It is easy to check that we have A = {2010, 2001,1200, 0110, 0101, 0022}.We have 2012  A, 1012  U, 1200  A, 0200  U, so that 21 1 11 1 01.Similarly, we have: 2200  A, 2100  U, 1101  A, 1001  U, so that 22 212 2 02. We also have: 0022  A, 0012  U, 2110  A, 2100  U, so that23 3 13 3 03. Finally, we have: 0022  A, 0021  U, 2101  A, 2100  U, sothat 24 4 14 4 04 (notice that the role of attributes 3 and 4 is entirely symmetric,in this example).Hence, using Theorem 2, this partition cannot be represented in Model F u. <image: None, width: 1, height: 1, bpc: 1>. .Indeed, the antichain A is not maximal: the element 2100 is incomparable, using, to all elements in A.Yet it is cumbersome but easy to check that this partition can be obtainedin Model F u, taking P = {2200, 0022}, F = {N}, Si = i, for i = 2, 3, 4, and21 P1 01, 21 I1 11, and 11 I1 01.. Let us dene Eu. <image: None, width: 1, height: 1, bpc: 1>. in the same way as F u. <image: None, width: 1, height: 1, bpc: 1>. . By Theorem 1, Eu. <image: None, width: 1, height: 1, bpc: 1>. is equivalent toEu. Summarizing Proposition 4 and previous results, we have that F u. <image: None, width: 1, height: 1, bpc: 1>. F u F c  F  E  Ec  Eu  Eu. <image: None, width: 1, height: 1, bpc: 1>. . This long chain of inclusions and equivalencesillustrates the strong asymmetry between the families of pseudo-conjunctive andpseudo-disjunctive models.In order to explore the gap between both families,we devote the rest of the paper to comparing the numbers of partitions that canbe represented in models F u. <image: None, width: 1, height: 1, bpc: 1>. and Eu. <image: None, width: 1, height: 1, bpc: 1>. (or any of the pseudo-conjunctive modelsdiscussed in this paper). This will help us quantify how restrictive F u. <image: None, width: 1, height: 1, bpc: 1>. is comparedto Eu. <image: None, width: 1, height: 1, bpc: 1>. .. 6Counting maximal antichains. The number of partitions that can be represented in model F u. <image: None, width: 1, height: 1, bpc: 1>. (resp. model Eu. <image: None, width: 1, height: 1, bpc: 1>. )is the number of maximal antichains (resp. antichains) in the poset (X, ). Thisposet can be seen as a direct product of n chains, where n is the number of at-tributes and the ith chain (i  {1, . . . , n}) is the set [mi] = {1, . . . , mi} orderedby  (the natural order on the integer interval [mi]), with mi being the num-ber of equivalence classes of the weak order i.Notice that antichains in the. 11. direct product of n chains also plays an important role in the analysis of mul-tichoice cooperative games, as shown by Grabisch (2016a). More generally, theimportance of studying discrete mathematics structures in decision theory waspowerfully stressed in Grabisch (2016b).The number of antichains (maximal antichains) in [m1]  . . .  [mn] will bedenoted by dE(m1, . . . , mn) (resp. dF(m1, . . . , mn)). When m1 = . . . = mn =m, the numbers dE(m1, . . . , mn) and dF(m1, . . . , mn) are respectively denoted byDE(m, n) and DF(m, n). We rst tackle two special cases (n = 2 and m = 2) andthen the general case, for which we have few results.. 6.1The case n = 2. Let N denote the set of positive integers. The next result, due to Covington (2004),presents a recurrence relation for dF(m1, m2).. Theorem 3For all m1, m2  N, dF(m1, m2) is equal to. dF(m1  1, m2  1) +. m12. i=0dF(i, m2  1) +. m22. i=0dF(m1  1, i).(4). A detailed proof of this result can be found in Bouyssou, Marchant, and Pirlot(2024). For dE(m1, m2), the following result easily follows from Berman and Kohler(1976).. Corollary 1For all m1, m2  N, we have. dE(m1, m2) =m1 + m2m1. .. ProofAccording to Berman and Kohler (1976), the number of antichains in [m1][m2][m3] is equal tom31. i=0. m1+m2+im1. <image: None, width: 1, height: 1, bpc: 1>. m1+im1.(5). Setting m3 = 1 in this expression yields the desired result.. For illustration purpose, we computed some numerical results under the con-straint that m1 = m2 (to save space). Some terms of the sequences DE(m, 2) andDF(m, 2) can be found in Table 1, with the corresponding ratios DF(m, 2)/DE(m, 2).. 12. <image: None, width: 1, height: 1, bpc: 1>. mDF(m, 2)DE(m, 2)DF(m, 2)/DE(m, 2). <image: None, width: 1, height: 1, bpc: 1>. 1120.52360.539200.45427700.3857142865832520.32936507962599240.28030303781734320.23805361382599128700.20194250298323486200.17118469810267971847560.14503994511866597054320.1228452921228128727041560.10402025613915907104006000.088062900142990383401166000.0745422841597863691551175200.063090031003.76527E+519.05485E+584.15829E-08. <image: None, width: 1, height: 1, bpc: 1>. Table 1: Number DF(m, 2) of maximal antichains, number DE(m, 2) of antichainsand ratio of these numbers in [m]2 for m  [15] and m = 100. Values of DF(m, 2)are computed by means of (4).. 13. For small values of m, the dierence of expressivity between models F u. <image: None, width: 1, height: 1, bpc: 1>. and Eu. <image: None, width: 1, height: 1, bpc: 1>. isnot very large, but it grows for large values of m, since the ratio seems to convergeto 0.DF(m, 2) and DE(m, 2) are respectively sequences A171155 and A000984 inthe On-line Encyclopedia of Integer Sequences OEIS (2023). A recurrence relationis mentioned by Alois P. Heinz (without proof) for DF(m, 2) in OEIS (2023):DF(m, 2) is equal to. (4m  3)DF(m  1, 2)  (2m  5)DF(m  2, 2) + DF(m  3, 2)  (m  3)DF(m  4, 2). <image: None, width: 1, height: 1, bpc: 1>. m.. Some other results (old and new) about the case n = 2 are presented inBouyssou et al. (2024). Therein, in addition to enumeration results, correspon-dences (bijections) between (maximal) antichains in products of chains and othermathematical structures are established.. 6.2The case m = 2. DF(2, n) is sequence A326358 in OEIS (2023). No expression seems to be knownfor this sequence and the highest known value corresponds to n = 7. Some termscan be found in Table 2.DE(2, n) corresponds to the Dedekind numbers (sequence A000372 in OEIS(2023)), for which no expression is known. The highest known value correspondsto n = 9. Some terms can be found in Table 2 with the corresponding ratiosDF(2, n)/DE(2, n). Here again, for small values of n, the dierence of expressivitybetween models F u. <image: None, width: 1, height: 1, bpc: 1>. and Eu. <image: None, width: 1, height: 1, bpc: 1>. is not very large, but for large values of n, the ratioseems to converge to 0.. <image: None, width: 1, height: 1, bpc: 1>. nDF(2, n)DE(2, n)DF(2, n)/DE(2, n). <image: None, width: 1, height: 1, bpc: 1>. 1230.66666672360.537200.354291680.172619537675810.0495976863174678283540.004055259712380591424146820409980.00005127214. <image: None, width: 1, height: 1, bpc: 1>. Table 2: Number DF(2, n) of maximal antichains, number DE(2, n) of antichainsand ratio of these numbers in [2]n for n  [7].. 14. 6.3The general case. In the general case, analytic expressions for DF(m, n) and DE(m, n) are dicultto obtain and we therefore only provide a lower bound for DF(m, n) and somenumerical results.. 6.3.1A lower bound for DF(m, n). Proposition 5The number of maximal antichains in [m]n is at least the number of antichains of[m]n1, that is DF(m, n)  DE(m, n  1).. ProofThe set {x  [m]n : xi = m} is the set of elements x  X having their ithcoordinate equal to m. We shall prove that any antichain, not necessarily maximal,in {x  [m]n : xi = m} can be extended into a maximal antichain of X, whichhas no other element with its ith coordinate xi equal to m. This will establishProposition 5 since any antichain of Xi is in one-to-one correspondence with anantichain of {x  [m]n : xi = m}.We take wlog i = 1. If the antichain in {x  [m]n : x1 = m} is maximal,the result is obvious. Otherwise, let A be any non-maximal antichain in {x [m]n : x1 = m}. Since A is not maximal in {x  [m]n : x1 = m}, there is atleast one element x = (m, x2, . . . , xn) that is incomparable to all elements in A.Let x = (m  1, x2, . . . , xn). We have that x  x and x is incomparable to anyelement in A. Indeed, for no y  A, we have x  y (otherwise x  y wouldhold too) and, for no y  A, we have y  x (otherwise y  x would also hold).Consider the set of all elements in {x  [m]n : x1 = m} that are incomparableto all elements in A. Select the minimal elements from this set. Change the rstcoordinate of each minimal element x into x1 = m  1, yielding an element x.Let A be the set obtained by adding all such elements x to the antichain A.These elements are incomparable to all elements in A and incomparable to oneanother. Therefore, A is an antichain. It is easy to see that it is maximal in X.Furthermore, the intersection of A with the set {x  [m]n : x1 = m} is exactlyA.. Since DF(m, n) is nondecreasing with m and n, we may conclude in particularthat the number of maximal antichains in X is at least the number of antichains in[2]n1, which is Dedekind number DE(2, n  1). Table 2 suggests that this boundis very weak. It also suggests that DF(m, n) grows extremely fast with n even form = 2.. 15. 6.3.2Some numerical results. Table 3 presents some values of DF(m, n) for small values of m and n, computedwith the help of the software system Macaulay2 (Grayson and Stillman, 2021). For[3]3, we used the function maximalAntichains provided by the package Posetsin the software system Macaulay2 (Grayson and Stillman, 2021) and manuallychecked the result. For [4]3 and [3]4, we also used the function maximalAntichains,but without manual check. For larger values (except when m = 2 or n = 2), thecalculations are prohibitively long (indicated by question marks in Table 3).. <image: None, width: 1, height: 1, bpc: 1>. DF(m, n). <image: None, width: 1, height: 1, bpc: 1>. n = 1234. <image: None, width: 1, height: 1, bpc: 1>. m = 1. <image: None, width: 1, height: 1, bpc: 1>. 11112. <image: None, width: 1, height: 1, bpc: 1>. 237293. <image: None, width: 1, height: 1, bpc: 1>. 391441165474. <image: None, width: 1, height: 1, bpc: 1>. 42710631?5. <image: None, width: 1, height: 1, bpc: 1>. 583??. <image: None, width: 1, height: 1, bpc: 1>. Table 3: Number of maximal antichains (DF(m, n)) for small values of m and n.Boldface entries are new.. For [3]3, using (5), we nd DE(3, 3) = 980 so that the ratio DF(3, 3)/DE(3, 3)is equal to 0.14693878. Similarly, for [4]3, we obtain DE(4, 3) = 232848 so that theratio DF(4, 3)/DE(4, 3) is equal to 0.04565639, which implies a huge dierence ofexpressivity between F u. <image: None, width: 1, height: 1, bpc: 1>. and Eu. <image: None, width: 1, height: 1, bpc: 1>. .. 7Conclusion. Although our results about ETRI-nB-pd and its special cases are very partial, wehave axiomatic and combinatorial results showing that. 1. the analysis of the pseudo-disjunctive models is far more complex than thatof the pseudo-conjunctive models;. 2. there is a whole variety of pseudo-disjunctive models that are not all equiv-alent, contrary to what we observed for pseudo-conjunctive models;. 3. most pseudo-disjunctive models are strict special cases of the correspondingpseudo-conjunctive models;. 4. the pseudo-disjunctive model F u. <image: None, width: 1, height: 1, bpc: 1>. is much more restrictive than the corre-sponding pseudo-conjunctive model.. 16. The strong asymmetry between the pseudo-conjunctive and pseudo-disjunctivemodels can be ascribed to the central role played by the relation P in the denitionof ETRI-nB-pd while S is central in ETRI-nB-pc. Indeed, Bouyssou and Pirlot(2015a,b) have shown that the nature of the relation P is rather dierent fromthat of the relation S in the ELECTRE methods.Hence, paralleling Bouyssou and Marchant (2015), we suggest to dene thedual of ETRI-nB-pc not by means of (3), but rather by. x  U. p S xfor some p  PandNot[x P q]for all q  P.(6). It is easy to see that ETRI-nB-pc and its dual now correspond via the transpositionoperation consisting in inverting the direction of preference on all criteria and per-muting A and U (see Almeida-Dias, Figueira, and Roy, 2010, Bouyssou and Marchant,2015, Roy, 2002).Mimicking Bouyssou et al. (2023, Th. 15), it is clear this dual model is char-acterized by Linearity. Instead of taking A as the set of proles to delimit A,we now take U = Max(, U) to delimit the category U, still using Si = i andF = {N}.If we replace (3) by (6) in the denition of Models F, F c and F u, it is alsosimple to see that they are all equivalent to the dual of ETRI-nB-pc.. References. J. Almeida-Dias, J. R. Figueira, and B. Roy. ELECTRE TRI-C: A multiple criteria sort-ing method based on characteristic reference actions. European Journal of OperationalResearch, 204(3):565580, 2010.J. Berman and P. Kohler. Cardinalities of nite distributive lattices. Mitt. Math. Sem.Giessen, 121(103124), 1976.D. Bouyssou and T. Marchant.An axiomatic approach to noncompensatory sortingmethods in MCDM, I: The case of two categories. European Journal of OperationalResearch, 178(1):217245, 2007a.D. Bouyssou and T. Marchant.An axiomatic approach to noncompensatory sortingmethods in MCDM, II: More than two categories. European Journal of OperationalResearch, 178(1):246276, 2007b.D. Bouyssou and T. Marchant. On the relation between ELECTRE TRI-B and ELEC-TRE TRI-C and on a new variant of ELECTRE TRI-B. European Journal of Opera-tional Research, 242:201211, 2015.D. Bouyssou and M. Pirlot. A consolidated approach to the axiomatization of outrankingrelations: A survey and new results. Annals of Operations Research, 229(1):159212,2015a. doi: 10.1007/s10479-015-1803-y.. 17. D. Bouyssou and M. Pirlot. A note on the asymmetric part of an outranking relation.International Transactions in Operational Research, 22(5):883912, 2015b.doi: 10.1111/itor.12135.D. Bouyssou, T. Marchant, and M. Pirlot. A note on ELECTRE TRI-nB with few limit-ing proles. 4OR, 20:443463, 2022. doi: https://doi.org/10.1007/s10288-021-00485-y.Published online on June 17, 2021.D. Bouyssou, T. Marchant, and M. Pirlot. A theoretical look at ELECTRE TRI-nB andrelated sorting models. 4OR, A Quarterly Journal of Operations Research, 21(73103),2023.D. Bouyssou, T. Marchant, and M. Pirlot. About maximal antichains in a product oftwo chains: A catch-all note, 2024. URL https://arxiv.org/abs/2410.16243.Michael A. Covington. The number of distinct alignments of two strings. Journal ofQuantitative Linguistics, 11(3):173182, 2004.Y. Crama and P.L. Hammer. Boolean Functions: Theory, Algorithms, and Applications.Encyclopedia of Mathematics and its Applications. Cambridge University Press, 2011.E. Fernandez, J. R. Figueira, J. Navarro, and B. Roy. ELECTRE TRI-nB: A new mul-tiple criteria ordinal classication method. European Journal of Operational Research,263(1):214224, 2017. doi: 10.1016/j.ejor.2017.04.048.J. R. Figueira, S. Greco, B. Roy, and R. Slowinski. An overview of ELECTRE methodsand their recent extensions. Journal of Multi-Criteria Decision Analysis, 20(12):6185,2013.J. R. Figueira, V. Mousseau, and B. Roy.ELECTRE methods.In J. R. Figueira,S. Greco, and M. Ehrgott, editors, Multiple Criteria Decision Analysis: State of the ArtSurveys, number 233 in International Series in Operations Research & ManagementScience, pages 155185. Springer, New York, NY, 2016. ISBN 978-1-4939-3094-4. doi:10.1007/978-1-4939-3094-4. <image: None, width: 1, height: 1, bpc: 1>. 5. First Edition: 2005.W. M. Goldstein. Decomposable threshold models. Journal of Mathematical Psychology,35(1):6479, 1991.M. Grabisch. Remarkable polyhedra related to set functions, games and capacities. TOP,24:301326, 2016a. doi: 10.1007/s11750-016-0421-4.Michel Grabisch.Set Functions, Games and Capacities in Decision Making, vol-ume 46. Springer, Cham, 2016b. ISBN 978-3-319-30690-2.doi: https://doi.org/10.1007/978-3-319-30690-2. <image: None, width: 1, height: 1, bpc: 1>. 2.Daniel R. Grayson and Michael E. Stillman. Macaulay2, a software system for researchin algebraic geometry. Available at https://math.uiuc.edu/Macaulay2/, 2021.S. Greco, B. Matarazzo, and R. Slowinski. Conjoint measurement and rough set ap-proach for multicriteria sorting problems in presence of ordinal criteria. In A. Colorni,M. Paruccini, and B. Roy, editors, A-MCD-A, Aide Multicrit`ere `a la Decision / MultipleCriteria Decision Aid, pages 117144. European Commission, Joint Research Centre,Luxembourg, 2001.D. H. Krantz, R. D. Luce, P. Suppes, and A. Tversky. Foundations of measurement,volume 1: Additive and polynomial representations. Academic Press, New York, 1971.OEIS. The On-line Encyclopaedia of Integer Sequences, Sloane, N. J. A. (Ed.). 2023.. 18. URL https://oeis.org.B. Roy. Multicriteria methodology for decision aiding. Kluwer, Dordrecht, 1996. Originalversion in French: Methodologie multicrit`ere daide `a la decision, Economica, Paris,1985.B. Roy.Presentation et interpretation de la methode ELECTRE TRI pour aecterdes zones dans des categories de risque. Document du LAMSADE 124, LAMSADE,Universite Paris Dauphine, Paris, France, March 2002. 25 pages.B. Roy and D. Bouyssou. Aide multicrit`ere `a la decision : methodes et cas. Economica,Paris, 1993. ISBN 2-7178-2473-1.R. Slowinski, S. Greco, and B. Matarazzo. Axiomatization of utility, outranking anddecision-rule preference models for multiple-criteria classication problems under partialinconsistency with the dominance principle. Control and Cybernetics, 31(4):10051035,2002.W. Yu. Aide multicrit`ere `a la decision dans le cadre de la problematique du tri : concepts,methodes et applications. Th`ese de doctorat, Universite Paris Dauphine, Paris, France,1992. (in French).. 19", "2410.18453v1.Density_of_states_of_the_Hubbard_model_supplemented_with_the_quantizing_magnetic_field.pdf": "arXiv:2410.18453v1  [cond-mat.str-el]  24 Oct 2024. Density of states of the Hubbard modelsupplemented with the quantizing magnetic eld. Alexei Sherman. Institute of Physics, University of Tartu, W. Ostwaldi Str 1, Tartu,50411, Estonia.. Contributing authors: alekseis@ut.ee;. Abstract. Using the strong coupling diagram technique, we calculate the zero-temperaturedensity of states  of electrons on a square lattice immersed in a perpendicularuniform magnetic eld. The electrons are described by Hubbard Hamiltonian. Formoderate doping, Landau subbands are observed for small Hubbard repulsions Uonly. For larger U, the subbands are blurred. Instead, small peaks varying withthe eld induction B arise by opening the Mott gap in its vicinity. The relatedvariation of  with 1/B may be connected with the low-frequency quantum oscil-lations in lightly doped cuprates. For all considered repulsions,  has gaps neartransfer frequencies of the Hubbard atom,  and U  , with  the chemicalpotential. In the heavily underdoped case  < 0, Landau subbands are groupedinto the lower and upper Hubbard subbands for moderate and large repulsions.The intensity of the upper Hubbard subband decreases with approaching theFermi level to the lower edge of the spectrum and nally vanishes.. Keywords: two-dimensional Hubbard model, quantizing magnetic eld, density ofelectron states, Landau subbands, strong coupling diagram technique. 1 Introduction. External magnetic elds are known to aect the transport properties of crystals.Experimental manifestations of this inuence were observed in metals by Shubnikov,de Haas, and van Alphen as oscillations of resistivity and magnetization caused byvarying magnetic elds [1]. The oscillations are connected with the magnetic splittingof the electron spectrum into Landau levels [2] transformed into subbands in crystals. 1. [3, 4]. Closely related to these eects are plateaux in the Hall resistivity of the two-dimensional (2D) electron gas [5, 6], which are manifestations of the integer quantumHall eect. The Coulomb interaction between electrons results in further splitting ofthe Landau subbands. This event was termed the fractional quantum Hall eect [7, 8].The Coulomb repulsion between electrons is also responsible for the anomalously lowfrequency of the Shubnikov-de Haas oscillations in lightly doped cuprate perovskites[9]. One of the possible explanations of these anomalous frequencies relates them tosmall Fermi-surface pockets located in the nodal region of the Brillouin zone [9]. Suchsmall pockets arise with the opening of the Mott gap in its vicinity [10, 11].Presumably, some of the above eects may be understood in the framework ofthe Hubbard model supplemented with a uniform magnetic eld. Calculations usingthis model were already performed for explaining low-frequency quantum oscillationsin lightly doped cuprates in Refs. [10, 11] and for reproducing Shubnikov-de Haasconductivity oscillations in crystals with large Fermi surfaces [12, 13]. The aboveeects reveal themselves in the model density of states (DOS) (). This work aims tostudy the DOS in dierent parameter regions. Since the eects are usually observedat extremely low temperatures T , we consider the case T = 0. We use the strongcoupling diagram technique (SCDT) [1417]. In this approach, Greens functions arecalculated by applying series expansions in kinetic energy powers. The method wasdeveloped for the case U  t, where U and t are the repulsion between electronsresiding on the same lattice site and intersite hopping constant, respectively. Self-consistent calculations [17] show that SCDT gives qualitatively correct results also forU  t. A numerical continuation of obtained results to real frequencies will blur nestructures in DOS. To perform this procedure analytically, we limit the irreduciblepart to two lowest-order terms of the SCDT expansion.We nd that at half-lling,  = U/2, and for moderate doping, Landau subbandsare well resolved only for small Hubbard repulsions. Here  is the chemical potential.For larger U, subbands are blurred and merge together. Instead, small peaks appearnear the Mott gap with its opening. Their location depends on the eld inductionB. We relate the peaks to states satisfying the Onsager quantization condition [1].When the Fermi level (FL) falls into the region of the peaks, their dependence on themagnetic eld causes oscillations in ( = 0). The frequency of these oscillations willbe anomalously low due to small Fermi-surface pockets for such electron concentrations[10, 11]. Hence, the peaks can be related to the low-frequency quantum oscillationsobserved in lightly doped cuprates [9]. For all doping and Hubbard repulsions, the zero-temperature DOSs have gaps at the Hubbard atom transfer frequencies  =  andU  . For B = 0, nite-temperature DOSs have dips at these frequencies [18]. Bothzero-temperature gaps and nite-temperature dips stem from multiple reabsorptionsof electrons by Hubbard atoms [17]. The gap widths grow with doping. The DOSis cardinally changed in the heavily underdoped case when  < 0. Here, Landausubbands are well resolved not only for U  t, but for moderate and large repulsions aswell. In the later cases, lower and upper Hubbard subbands are composed of an equalnumber of Landau subbands. The intensity of the upper Hubbard subband decreaseswith approaching the FL to the lower spectral edge. This spectral modication followsfrom the fact that the local interaction between electrons becomes increasingly rarewith the reduction of their concentration.. 2. The article is organized as follows: The model Hamiltonian and the main formulasare given in the next section. The results of calculations and their discussion arebrought up in Sect. 3. The last section is devoted to concluding remarks.. 2 Model and main formulas. In the presence of the uniform magnetic eld B perpendicular to a square crys-tal lattice, electrons with the on-site Coulomb interaction are described by theHamiltonian. H = t. laexp. . i e. <image: None, width: 1, height: 1, bpc: 1>. . l. laA(r)dr. . ala,al + 1. <image: None, width: 1, height: 1, bpc: 1>. 2gBB. lnl. +U. lnlnl,,(1). where the 2D vector l labels sites of the lattice, a are four vectors connecting nearestneighbor sites,  =,  is the spin projection, e and  are the modulus of the electroncharge and Planck constant, respectively, A(r) is the vector potential, al and alare electron creation and annihilation operators, g and B are the g-factor and Bohrmagneton, respectively, and nl = alal is the electron number operator.In Eq. (1), the inuence of the magnetic eld on the kinetic term is described inthe Peierls approximation [19, 20]. It is valid until the magnetic length lB =. <image: None, width: 1, height: 1, bpc: 1>. /(eB)is larger than the size of the Wannier function lW . Below, we shall use strong elds todecrease computation time and promote the convergence of the used iteration proce-dure. For such elds, lB is only slightly larger than a = |a|. Nevertheless, we supposethat the condition lW < lB is fullled. We also assume that the Zeeman splittingdescribed by the next to the last term is much smaller than the energy parameters ofthe Hamiltonian and widths of spectral features. Therefore, this term will be omittedbelow.Using the Landau gauge [2] A(l) = Blyx, the exponent in the kinetic term in (1)can be written as ieBlyax/. We suppose that the lattice is located in the x-y plane,B is directed along the z axis, x is the unit vector along the x axis, ly and ax are yand x components of the respective vectors. We restrict ourselves to elds satisfyingthe conditione. <image: None, width: 1, height: 1, bpc: 1>. Ba2 = 2. <image: None, width: 1, height: 1, bpc: 1>. ,(2). where  and  are coprime integers. With these notations and approximations, theHamiltonian reads. H = t. laexpiax. <image: None, width: 1, height: 1, bpc: 1>. a lala,al + U. lnlnl,,(3). where  = 2/(a)y. As seen from Eq. (3), the Hamiltonian is invariant with respectto translations by the lattice period along the x axis and by  lattice periods alongthe y axis. Below, we use a lattice containing Nx sites along the x axis and Ny sites. 3. along the y axis with the periodic boundary conditions. In such a lattice, the followingFourier transformation over the spacial variables can be dened:. akm =1. <image: None, width: 1, height: 1, bpc: 1>. . <image: None, width: 1, height: 1, bpc: 1>. N. . lei(k+m)lal,(4). where the wave vector k = (2nx/(Nxa), 2ny/(Nya)), nx = 0, 1, . . . Nx  1, ny =0, 1, . . . Ny  1 is dened in the reduced, magnetic Brillouin zone, m = 0, 1, . . .  1,and N = NxNy. m is a cyclic variable with the period .Below, we calculate the DOS. () =  1. <image: None, width: 1, height: 1, bpc: 1>. N. . kmIm Gmm(k, ),(5). where Gmm(k, ) is the Fourier transform of Greens function G(l , l)=T al( )al() after the analytic continuation to the real frequency axis. Here anglebrackets denote the statistical averaging with the operator H = H   l nl,which denes also time dependencies of operators in G(l , l), T is the chronolog-ical operator. To calculate this Greens function, we use the SCDT series expansionover the powers of the kinetic energy in the Hamiltonian (3). Terms of the expan-sion are products of the hopping constants with the Peierls exponential tll=t a exp(iaxl/a)l,la and on-site cumulants of electron creation and annihi-lation operators. The terms of the expansion can be visualized by depicting tll asdirected lines and cumulants as circles with the number of outgoing and incoming linesequal to the number of creation and annihilation operators in them. The linked-clustertheorem is valid, and partial summations are allowed in SCDT. A two-leg diagram isirreducible if it cannot be divided into two disconnected parts by cutting a hoppingline tll. Denoting the sum of all such diagrams by K, the Fourier transform of theelectron Greens function is written as. G(k, ) =[K(k, )]1  tk1= [1  K(k, )tk]1 K(k, ),(6). where tk is the Fourier transform of tll,. tmm(k) = teikxam,m+ + eikxam,m + 2 coskya + 2m. <image: None, width: 1, height: 1, bpc: 1>. . m,m.(7). Notice that after Fourier transformations, G, K, and t depend on two variables m andm, which are considered as matrix indices in Eq. (6).Diagrams of the rst three orders in K(k, ) are shown in Fig. 1. Thanks to thepossibility of carrying out partial summations, internal lines tk in these diagrams canbe substituted by the renormalized hopping. (k, ) = tk + tkG(k, )tk.(8). 4. =-+ 12-12(a)(b)(c)+...K. (d). Fig. 1 Diagrams of the rst three orders in K(k, ).. In the below calculations, the irreducible part K(k, ) is approximated by the sumof diagrams (a) and (b) in Fig. 1. As seen below, this approximation correctly describesthe main peculiarities of DOS shapes in the absence of the magnetic eld. It givesthe critical repulsion for the Mott gap opening, Uc  6t, agreeing with results froman innite series of SCDT ladder diagrams [17]. The main dierence between DOSsin these two approximations, neglected in the present work, is the narrow Slater gapat half-lling near  = 0 for moderate U [21]. The gap is a consequence of the spinantiferromagnetic ordering. In the used approximation, K is local. Therefore, it doesnot describe any spin ordering, and the Slater gap does not appear in DOSs. Never-theless, we use this approximation because it allows us to perform the continuation toreal frequencies analytically without recourse to the maximum entropy method. Theirreducible part reads. Kmm(j) = C(1)(j)m,m  T. <image: None, width: 1, height: 1, bpc: 1>. N. . j. . kmC(2)(j, j, j, j)m,mm+m(k, j). = C(1)(j)m,m  T. jC(2)(j, j, j, j)mm(j),(9). where C(1) and C(2) are cumulants of the rst and second orders, respectively, j is aninteger dening the fermion Matsubara frequency j = (2j  1)T ,. m(j) = 1. <image: None, width: 1, height: 1, bpc: 1>. N. . km1. . m2m3tm1m2(k)Gm2m3(k, j)tm3,m1m(k). = 1. <image: None, width: 1, height: 1, bpc: 1>. N. . km1m2[1  t(k)K(j)]1. <image: None, width: 1, height: 1, bpc: 1>. m1,m2tm2,m1m(k).(10). In moving from the rst line to the second in (9), we used Eq. (8) and took intoaccount that tll = 0. Notice that in the used approximation, Kmm(j) depends onlyon the dierence m  m.For the analytic continuation to the real frequency axis, we need to nd a relationbetween the quantity m(j) on the upper half of the imaginary axis and its valuesinnitesimally above the real axis. From the spectral representation, one can see that. Gmm(k, j) =1. <image: None, width: 1, height: 1, bpc: 1>. 2i. . . Gmm(k,  + i)  Gmm(k,  + i). <image: None, width: 1, height: 1, bpc: 1>. ijd,  +0.. 5. Taking into account that tmm(k) is a Hermitian matrix, tmm(k) = tmm(k), we ndfrom this equation and (10). m(j) =1. <image: None, width: 1, height: 1, bpc: 1>. 2i. . . m( + i)  m( + i). <image: None, width: 1, height: 1, bpc: 1>. ijd.(11). Expressions for cumulants appearing in (9) read [1417]. C(1)(j) = Z1 eE0 + eE1g1(ij) +eE1 + eE2g2(ij),. C(2)(j, j, j, j) =Z1eE1 (1  2j,j). +Z2 e(E0+E2)  e2E1(2  j,j)F(ij)F(ij). Z1eE0Ug1(ij)g1(ij)g3(ij + ij) [g1(ij) + g1(ij)](12). Z1eE2Ug2(ij)g2(ij)g3(ij + ij) [g2(ij) + g2(ij)]. +Z1eE1F(ij)g21(ij)  F(ij)g2(ij). +F(ij)g21(ij)  F(ij)g2(ij) ,. where E0 = 0, E1 = , and E2 = U  2 are eigenenergies of the Hubbard atomHamiltonian, Hl = Unlnl  nl, corresponding to the empty, singly, and doublyoccupied states,  = 1/T , Z = exp(E0) + 2 exp(E1) + exp(E2),. g1(ij) = (ij + )1,g2(ij) = (ij +   U)1,. g3(ij + ij) = (ij + ij + 2  U)1,F(ij) = g1(ij)  g2(ij).. Substituting cumulants (12) and Eq. (11) into (9), performing the summation over j,replacing ij with  + i, and setting T = 0, we nd. Km() = H()g1( + i)m,0  U. <image: None, width: 1, height: 1, bpc: 1>. 2i. 0. . m()  m(). g1()g1()g3( + ) [g1() + g1()]d. + H()H(U  )1. <image: None, width: 1, height: 1, bpc: 1>. 2 [g1() + g2()] m,0 + 3. <image: None, width: 1, height: 1, bpc: 1>. 4F 2()m(). s(1)m. <image: None, width: 1, height: 1, bpc: 1>. 2g21()  F()g2() s(2)m. <image: None, width: 1, height: 1, bpc: 1>. 2 F(). + H(  U)g2( + i)m,0 + U. <image: None, width: 1, height: 1, bpc: 1>. 2i. . 0. m()  m(). g2()g2()g3( + ) [g2() + g2()]d,(13). 6. where H() is the Heaviside step function,. s(1)m =1. <image: None, width: 1, height: 1, bpc: 1>. 2i. . . m()  m()[H()g1() + H()g2()] d,. s(2)m =1. <image: None, width: 1, height: 1, bpc: 1>. 2Ui. . . m()  m() H()( +  + U)g21()(14). +H()( +   2U)g22()d.. Notice that the irreducible part Km() is dierent in the region of half-lling andmoderate doping 0 <  < U and for the heavily underdoped  < 0 and overdoped  >U cases. It is a consequence of the fact that in these three regions, the ground state ofthe Hubbard atom Hamiltonian Hl is dierent. With growing , it is changed from theempty to two singly occupied and nally to the doubly occupied state. Various groundstates lead to dierences in the cumulants and irreducible parts in the mentioned regions. This discontinuous change is a consequence of zero temperature and electroncorrelations. It is not a property inherent solely in SCDT. For example, similar abruptchanges in Greens function with  [11] occur in the cluster perturbation theory (CPA)[22]. In this approach, the crystal Greens function is calculated from the cluster ones.The latter have many ground states corresponding to dierent . We are interestedin three regions of  and two abrupt changes in Greens functions in these regions.We are talking about the region with the electron concentration x = 1 and two of itsneighbors with the number of electrons one less or more than the number of clustersites Nc. These three regions are analogous to the above  regions of the Hubbardatom. The dierence between the three cluster regions is retained even for Nc  when the CPA becomes exact. It occurs because the regions are qualitatively dissimilar one corresponds to an insulator and two other metals. We suppose that the dierencepersists even for U < Uc until U  t. Accordingly, the abrupt change of Greensfunction is also maintained at T = 0 in this case.The Hamiltonian (3) has the particle-hole symmetry. To see this, we change theelectron operators as follows:. al = eiQlal,al = eiQlal,Q =. <image: None, width: 1, height: 1, bpc: 1>. a ,. <image: None, width: 1, height: 1, bpc: 1>. a. ,. reverse the eld direction, and take into account that axay = 0. Therefore, in thefollowing consideration, we only consider chemical potentials   U/2.Equations (7), (10), (13), and (14) form a closed system, which can be solved byiteration for given values of U/t, /t, , and . In this procedure, as the starting valuesof Km(), we used the rst terms in braces in Eq. (13). To achieve the proper analyticproperties of the retarded Greens function, a small imaginary constant was added tothe frequency in the functions g1() and g2() in the starting Km(),    + i,  0.01t. For the case 0 <   U/2, no other articial broadening was used in thefollowing iteration steps. In the case  < 0, we had to add such a broadening in therst term in braces for all iteration steps to achieve convergence. After the iterationconvergence, the obtained irreducible part was applied for calculating Greens function(6) and the DOS (5).. 7. -4-2024. -6-4-20246. -8-6-4-202468. -0.6-0.30.00.30.6. (a). r. (b). w/t. (c). Fig. 2 Densities of states for half-lling, U = 0.5t,  = 7 (a), U = 4t,  = 5 (b), and U = 8t,  = 3(c). For all panels  = 1. The inset in panel (c) shows the vicinity of the Mott gap on an increasedscale.. 3 Results and discussion. Densities of states calculated at half-lling, x = 2 0 ()d = 1, for several valuesof the Hubbard repulsion U and strengths of the magnetic eld are shown in Fig. 2.For U = 0.5t, all seven (for  = 7) Landau subbands are well resolved, though threecentral ones merge. However, at U = 4t, gaps between Landau subbands disappear,and only weak remnant DOS variations near  = 3t remind of them.. 8. -4-2024. -6-4-20246. -8-6-4-202468. (a). r. (b). w/t. (c). Fig. 3 Densities of states for half-lling, B = 0, U = 0.5t (a), U = 4t (b), and U = 8t (c).. For U = 4t and 8t, the DOS shapes are close to those calculated with the sameapproximation for B = 0. For comparison, these later DOSs are shown in Fig. 3.Equations for calculating these spectra are similar to those in the previous section withthe following dierences: Matrix equations (6) and (8) transform to the scalar formwith k the wave vector in the usual Brillouin zone. Only the m = 0 components areretained in Eqs. (5), (10), (13), (14), and the hopping function (7) transforms to tk =2t[cos(kxa) + cos(kya)]. As follows from the comparison of zero-temperature spectrain Fig. 3 with nite-temperature results of Ref. [17] obtained with an innite series of. 9. -0.6-0.30.00.30.6. r. w/t. Fig. 4 Densities of states near the Mott gap for half-lling, U = 8t, and  = 3 (black solid line), 5(red dashed line), 9 (green dotted line), 20 (blue dash-dotted line). For all curves  = 1.. SCDT diagrams, the used approximation qualitatively correctly reproduces the mainfeatures of DOSs in the square-lattice Hubbard model. The main dierences betweenthe two approximations are located near frequencies  = , U, and 0. In the nite-temperature calculations, the DOS is suppressed near the transfer frequencies of theHubbard atom due to the electron reabsorption, leading to the spectrums four-bandstructure. It is observed also in Monte Carlo simulations [18]. We obtained narrowgaps at these frequencies instead of the intensity suppressions in the zero-temperaturecalculations. As the above formulas show, the irreducible part does not depend on thewave vector. As a consequence, the used approximation does not describe any spinordering neither for itinerant electrons for small U (the Slater mechanism [23]) nor forlocalized electrons at larger repulsions (the Heisenberg mechanism). Therefore, thereis no conical Slater gap at  = 0 in Figs. 3(a) and (b), which is present in calculationsdescribing the momentum dependence of the irreducible part [21]. Notice, however,that the used approximation is enough to estimate correctly the critical repulsion ofthe Mott transition Uc  6t, which is in agreement with more exact SCDT calculations[17, 21].Comparing Figs. 2(c) and 3(c), we see that the magnetic eld leads to the appear-ance of weak maxima near the Mott gap. The location of these peaks depends onthe value of B, as demonstrated in Fig. 4. The Mott gap and peaks are retained atmoderate doping. Hence, when the chemical potential is located in the peak region,( = 0) will oscillate with B. The DOS on FL is the second derivative of the Landau. 10. -4-2024. -4-202468. -4-20246810. (a). r. (b). w/t. (c). Fig. 5 Densities of states for U = 0.5t,  = 0.05t,  = 7 (x = 0.95, a), U = 4t,  = 0.3t,  = 5(x = 0.7, b), and U = 8t,  = 0.3t,  = 3 (x = 0.53, c). For all curves  = 1.. thermodynamic potential (T, V, , B) over  (V is the volume). Therefore, the oscil-lation of (0) points to the similar variation of  with B. Since quantum oscillationexperiments measure changes of dierent derivatives of this thermodynamic potentialwith the magnetic eld strength [1], the variation of the peaks with B is the source ofthese oscillations. We relate the peaks to electron states satisfying the Onsager quan-tization condition [1]. Opening the Mott gap leads to the appearance of small Fermisurface pockets in the vicinity of the gap [10, 11]. The consequence of their small. 11. -4-2024. -4-202468. -4-2024681012. (a). r. (b). w/t. (c). Fig. 6 Densities of states for B = 0, U = 0.5t,  = 0.05t (x = 0.94, a), U = 4t,  = 0.3t (x = 0.75,b), and U = 8t,  = 0.3t (x = 0.57, c).. area is anomalously low frequencies of quantum oscillations, which are close to thoseobserved in lightly doped cuprates [9].Figure 5 demonstrates DOSs in the doped case when the chemical potential is closeto the lower edge of the range 0 <  < U. As seen, doping leads to the signicantwidening of gaps near  =  and U  . Besides, for U = 8t, the Mott gap disap-pears. As for half-lling, Landau subbands are well resolved for U = 0.5t and revealthemselves in remnant DOS variations for larger repulsions. These spectral features. 12. 0246. 02468. 02468. 024681012. (a). (b). (c). w/t. r. (d). Fig. 7 Densities of states for U = 0.5t,  = 2.5t,  = 3 (x = 0.22, a), U = 4t,  = 1.5t,  = 3(x = 0.3, b), U = 4t,  = 1.5t,  = 5 (x = 0.27, c), and U = 8t,  = 1.5t,  = 3 (x = 0.22, d). Inall cases  = 1.. are more noticeable from the comparison with the zero-eld DOSs in Fig. 6. As seen,the reabsorption gaps are wider for B = 0.Let us now consider the heavily underdoped case  < 0. As mentioned above, forsuch chemical potentials, the irreducible part K() diers signicantly from that in the0 <  < U range. As a consequence, DOS shapes are entirely dierent. The calculatedresults are shown in Fig. 7. The spectra demonstrate several well-resolved Landausubbands, even in the case of moderate and strong Hubbard repulsions. In the latter. 13. -202468. -20246810. -202468101214. (a). r. (b). w/t. (c). Fig. 8 Densities of states for B = 0, U = 0.5t,  = 2.5t (x = 0.26, a), U = 4t,  = 1.5 (x = 0.35,b), and U = 8t,  = 1.5t (x = 0.27, c).. cases, we see the Mott gap separating low- and high-frequency Landau subbands, intowhich lower and upper Hubbard subbands divide. The number of Landau subbandsforming lower and upper Hubbard subbands is equal to  (in Fig. 7(c), the intensitysuppression separating the second and third Landau subbands merges with the dipnear the reabsorption frequency  = ). These results can be understood fromthe outcomes of Ref. [10]. In that work, we considered the same problem using thesimpler Hubbard-I approximation [24]. Each Landau subband splits by the Hubbardrepulsion into lower and upper Hubbard subbands in this approximation. Due to the. 14. narrowness of Landau subbands, the splitting occurs even for U < Uc. Consequently,the DOS contains the Mott gap separating an equal number of lower and upper Landausubbands. This picture is close to that shown in Fig. 7.Notice that all DOSs in Fig. 7 have dips at the reabsorption frequency  = . Theintensity of peaks forming the upper Hubbard subband decreases with the diminutionof  from zero. The subband disappears as the Fermi level reaches the lower boundaryof the spectrum, and for U = 4t and 8t, the DOS shapes become similar to that shownin Fig. 7(a). This change of shapes is connected with the fact that the local interactionbetween electrons becomes increasingly rare with the decrease in their concentration.For comparison, in Fig. 8, we show DOSs calculated for the same U and  withoutthe magnetic eld. For U = 4t and 8t, the spectra demonstrate two Hubbard subbands.As for B = 0, the intensity of the upper subband decreases with the diminution of. The subband disappears as the Fermi level attains the lower edge of the spectrum,and the DOSs acquire a shape similar to that in Fig. 8(a). All DOSs have dips atfrequencies  = .. 4 Conclusion. In this work, we used the strong coupling diagram technique to investigate the zero-temperature density of states of the correlated electron system on a square latticeimmersed in a perpendicular uniform magnetic eld. In calculating the irreducible part,we considered two lowest-order diagrams from the SCDT series. On the one hand, inzero eld, this approximation allows one to obtain the DOSs close to those calculatedwith an innite series of SCDT ladder diagrams and in some other approaches. On theother hand, the approximation enables to perform the continuation to real frequen-cies analytically without using approximate numerical methods. Such continuationallows us to increase the resolution of obtained DOSs. The considered Hamiltonian hasparticle-hole symmetry. Therefore, we considered only electron concentrations equalto or less than unity. For T = 0, the irreducible part and the related DOS changeabruptly when the chemical potential crosses zero. This fact is connected with thesudden modication of the ground state of the Hubbard atom at this  from empty tosingly occupied state. For  > 0, the Landau subbands in the DOS are well resolvedat small Hubbard repulsions only. For moderate and large U, only slight DOS varia-tions are retained from the merged subbands. All spectra contain gaps at the transferfrequencies of the Hubbard atom  =  and U . The gaps appear due to multipleelectron reabsorptions on these frequencies. In contrast, for nite temperatures, thereabsorptions reveal themselves as spectral intensity suppressions at these frequen-cies. For T = 0, widths of the reabsorption gaps grow as the chemical potential shiftsfrom half-lling. At half-lling, as for B = 0, the Mott gap opens at Uc  6t. Oncethis happens, small peaks appear below and above the gap. The peaks are retained atmoderate doping. Their location depends on the magnetic eld strength. This depen-dence leads to oscillations of the density of electron states on the Fermi level. Theirfrequency is low due to small areas of Fermi surface pockets arising with the Mottgap opening. This result can explain low quantum oscillation frequencies observed inlightly doped cuprates.. 15. The DOSs calculated for  < 0 are entirely dierent. For such chemical potentials,Landau subbands are well resolved not only for small repulsions, but for moderateand large U as well. In later cases, Landau subbands form lower and upper Hubbardsubbands with the Mott gap between them. This gap exists even for U < Uc. Thenumbers of the upper and lower Landau subbands are equal to . The intensity of theupper subbands decreases as the Fermi level approaches the lower edge of the spectrumand nally disappears. This spectral change reects that the local interaction betweenelectrons becomes increasingly rare with decreasing concentration.. References. [1] D. Shoenberg, Magnetic Oscillations in Metals (Cambridge University Press,Cambridge, 1984). [2] L.D. Landau, E.M. Lifshitz, Quantum Mechanics (Pergamon Press, Oxford, 1965). [3] E.Brown,Phys.Rev.133,A1038(1964).https://doi.org/10.1103/PhysRev.133.A1038. [4] D.Langbein,Phys.Rev.180,633(1969).https://doi.org/10.1103/PhysRev.180.633. [5] K. v. Klitzing, G. Dorda, M. Pepper, Phys. Rev. Lett. 45, 494 (1980).https://doi.org/10.1103/PhysRevLett.45.494. [6] D.J. Thouless, M. Kohmoto, M.P. Nightingale, M. den Nijs, Phys. Rev. Lett. 49,405 (1982). https://doi.org/10.1103/PhysRevLett.49.405. [7] D.C. Tsui, H.L. Stormer, A.C. Gossard, Phys. Rev. Lett. 48, 1559 (1982).https://doi.org/10.1103/PhysRevLett.48.1559. [8] R.B.Laughlin,Phys.Rev.Lett.50,1395(1983).https://doi.org/10.1103/PhysRevLett.50.1395. [9] S.E. Sebastian, N. Harrison, G.G. Lonzarich, Rep. Progr. Phys. 75, 102501 (2012).https://doi.org/10.1088/0034-4885/75/10/102501. [10] A.Sherman,Phys.Lett.A379,1912(2015).https://doi.org/10.1016/j.physleta.2015.05.023. [11] A. Sherman, J. Low Temp. Phys. 209, 96 (2022). https://doi.org/10.1007/s10909-022-02800-1. [12] A.A. Markov, G. Rohringer, A.N. Rubtsov, Phys. Rev. B 100, 115102 (2019).https://doi.org/10.1103/PhysRevB.100.115102. 16. [13] J.Vucicevic,R.Zitko,Phys.Rev.B104,205101(2021).https://doi.org/10.1103/PhysRevB.104.205101. [14] M.I.Vladimir,V.A.Moskalenko,Theor.Math.Phys.82,301(1990).https://doi.org/10.1007/BF01029224. [15] W.Metzner,Phys.Rev.B43,8549(1991).https://doi.org/10.1103/PhysRevB.43.8549. [16] S. Pairault, D. Senechal, A.-M.S. Tremblay, Eur. Phys. J. B 16, 85 (2000).https://doi.org/10.1007/s100510070253. [17] A.Sherman,J.Phys.:Condens.Matter30,195601(2018).https://doi.org/10.1088/1361-648X/aaba0e. [18] C.GroberC,R.Eder,W.Hanke,Phys.Rev.B62,4336(2000).https://doi.org/10.1103/PhysRevB.62.4336. [19] R. Peierls, Z. Phys. 80, 763 (1933). [20] G.H.Wannier,Rev.Mod.Phys.34,645(1962).https://doi.org/10.1103/RevModPhys.34.645. [21] A. Sherman, Phys. Scr. 98, 115947 (1923). https://doi.org/10.1088/1402-4896/ad000b. [22] D. Senechal, D. Perez, and D. Ploue, Phys. Rev. B 66, 075129 (2002).https://doi.org/10.1103/PhysRevB.66.075129. [23] J.C. Slater, Phys. Rev. 82, 538 (1951). https://doi.org/10.1103/PhysRev.82.538. [24] J.Hubbard,Proc.Roy.Soc.(London)A281,401(1964).https://doi.org/10.1098/rspa.1964.0190. 17"}